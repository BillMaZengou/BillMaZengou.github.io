<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Graphics Basis: Quaternian</title>
    <link href="/2020/05/09/Graphics-Basis-Quaternian/"/>
    <url>/2020/05/09/Graphics-Basis-Quaternian/</url>
    
    <content type="html"><![CDATA[<p>Another important concept in VR/AR is graphics. Rotation can be expressed using different forms. Rotation matrix is a common way in maths and physics. However, each matrix contains 9 parameters which make it computationally expensive. Also, in computer simulation or graphics, we normally consider rigid body rotation which means that the object does not deform during the rotation. (Additionally, other problems occur using other forms to express rotation).</p><h1 id="quaternian"><a class="markdownIt-Anchor" href="#quaternian"></a> Quaternian</h1><p>Quaternian is a useful way in computer graphics to express rotation. It is a generalisation of the idea of complex numbers. Suppose \(u_0\) is a scalar and \(\mathbf{u} = (u_1, u_2, u_3)\) is a vector. Then the 4-tuple \((u_0, u_1, u_2, u_3)\) represents a quaternian, defined as:<br />\[<br />u = u_0 + \mathbf{u}<br />\]<br />or<br />\[<br />u = u_0 + u_1\mathbf{i} + u_2\mathbf{j} + u_3\mathbf{k}<br />\]<br />where<br />\[<br />\mathbf{i}^2 = \mathbf{j}^2 = \mathbf{k}^2 = \mathbf{i}\mathbf{j}\mathbf{k} = -1<br />\]<br />which is the same as the imaginary number. Also, multiplication between any two unit vector is the same as cross product, for example,<br />\[<br />\mathbf{i}\mathbf{j} = \mathbf{i} \times \mathbf{j} = \mathbf{k}<br />\]</p><p>The conjugate of a quaternian \(u\) is<br />\[<br />u^* = u_0 - \mathbf{u}<br />\]</p><p>A <em>pure quaternian</em> has its scalar part \(u_0 = 0\)</p><hr /><h2 id="polar-representation-of-quaternian"><a class="markdownIt-Anchor" href="#polar-representation-of-quaternian"></a> Polar Representation of Quaternian</h2><p>To understand the geometric interpretation of the quaternian, we need to have a look at its <strong>polar representation</strong>.</p><p>Let \(u\) be a unit quaternian, \(u = u_0 + \mathbf{u}\). Since \(u\) is unit, we have<br />\[<br />|u|^2 = u_0^2 + |\mathbf{u}|^2 = 1 = cos^2(\theta) + sin^2(\theta)<br />\]<br />for some \(-\pi \le \theta \leqslant \pi\).</p><p>Also, we can define a unit vector that<br />\[<br />\mathbf{s} = \frac{\mathbf{u}}{|\mathbf{u}|}<br />\]</p><p>Hence<br />\[<br />u = u_0 + \mathbf{u} = u_0 + |\mathbf{u}|\mathbf{s}<br />\]<br />Also,<br />\[<br />u = cos(\theta) + \mathbf{s} sin(\theta)<br />\]</p><hr /><h2 id="quaternian-rotation"><a class="markdownIt-Anchor" href="#quaternian-rotation"></a> Quaternian Rotation</h2><p>Now we will show how to represent rotation with quaternian. Suppose \(\mathbf{p}\) is a vector or position, then \(p = 0 + \mathbf{p}\). Let \(u\) be any unit quaternian, such that \(u = cos(\theta) + \mathbf{s} sin(\theta)\). Then multiplication \(up u^* \) results in a pure quaternian \(q = 0 + \mathbf{q}\).</p><p><strong>\(\mathbf{q}\) is the rotation of \(\mathbf{p}\) about the axis \(\mathbf{s}\) by an angle of \(2\theta\)</strong>.</p><p>For example, to rotate \(\mathbf{p} = (x, y, z)\) about the \(Z\) axis (i.e. \(\mathbf{s} = (0, 0, 1)\)). Then<br />\[<br />up = -(\mathbf{u} \cdot \mathbf{p}) + \mathbf{u} \times \mathbf{p},<br />\]<br />and<br />\[<br />upu^* = (xcos2\theta - ysin2\theta, xsin2\theta + ycos2\theta, z)<br />\]</p><p>Suppose that there are a <strong>sequence</strong> of rotations, angle \(2\theta_i\), about axis \(\mathbf{s}<em>i\), for \(i = 1, …, n\). Then the result can be achieved by applying the quaternian rotation operator in sequence:<br />\[<br />u_n u</em>{n-1}, …, u_1 p u_1^* , …, u_{n-1}^* u_n^*<br />\]<br />where \(u_i = cos\theta_i + \mathbf{s}_i sin\theta_i\)</p><hr /><h1 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h1><ol><li>Slater, M., Steed, A. and Chrysanthou, Y., 2002. Computer Graphics And Virtual Environments. Wokingham: Addison-Wesley.</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Data Structure &amp; Algorithm Revision: Link List and LRU (TODO)</title>
    <link href="/2020/05/08/Data-Structure-Algorithm-Revision-Link-List-and-LRU/"/>
    <url>/2020/05/08/Data-Structure-Algorithm-Revision-Link-List-and-LRU/</url>
    
    <content type="html"><![CDATA[<p>Here is a new series about Data Structure and Algorithm.</p><h1 id="link-list"><a class="markdownIt-Anchor" href="#link-list"></a> Link List</h1><p>Two fundamental data structures are <strong>Array</strong> and <strong>Link List</strong>.</p><p>If we create an array, RAM will allocate a fixed amount of the space to store the elements. For example, <code>int a[2]</code> can store 3 elements and requires 12 bits of the memory as each <code>int</code> type occupies 4 bits of the memory. If we want to extend the array, RAM has to allocate a new space, and then copy and paste the original data into the new array. After that, new elements can be added. This is time consuming with time complexity \(O(n)\).</p><p>To speed up this process, an alternative data structure can be considered. That is the <strong>Link List</strong>. The simplest link list Amzo1998331</p>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation -&gt; Image Processing6: Edge Detection</title>
    <link href="/2020/05/01/Computer-Vision-Foundation-Image-Processing6-Edge-Detection/"/>
    <url>/2020/05/01/Computer-Vision-Foundation-Image-Processing6-Edge-Detection/</url>
    
    <content type="html"><![CDATA[<p>In this post, another basic image operation is explored - Edge Detection.</p><p>This post is split into four sections:</p><ol><li>The Basic Principles of Edge Detection</li><li>Practice with OpenCV in Python</li><li>Compute Sobel Edge Detection and Canny Edge Detection From Scratch</li></ol><p>Source code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; edge.py (OpenCV)</p><hr><h1 id="The-Basic-Principles-of-Edge-Detection"><a href="#The-Basic-Principles-of-Edge-Detection" class="headerlink" title="The Basic Principles of Edge Detection"></a>The Basic Principles of Edge Detection</h1><p>In an image, it is for us to realise edges by observing it. For a computer, the straight-forward way is to find changes of intensity. The change in intensity may be ascribed to two objects such that one is at foreground and the other is at background, or two objects with different reflection and absorption of light. Nevertheless, the change of intensity can be a great indicator of edges in the image.</p><p>We can easily find the changes by calculating the derivative of the light intensity across the image in both x and y directions. In image processing, as we do not care too much about the actual magnitudes of the derivative, we can use a discrete differentiation operator to approximate the derivatives. The operator for x direction is as<br>\[<br>G_x = \begin{bmatrix} -1 &amp; 0 &amp; +1 \\ -2 &amp; 0 &amp; +2 \\ -1 &amp; 0 &amp; +1 \end{bmatrix},<br>\]<br>which is symmetric in y direction. Imagine if one pixel with value, say, \(50\), and all the surrounding pixels also have pixel value \(50\), then the convolution will return \(0\) as plus and minus can cancel out each other. However, if one side is greater than the other side, then it will return a value. The signs of the return values are <strong>not</strong> important.</p><p>Symmetrically, for y direction, the operator is<br>\[<br>G_y = \begin{bmatrix} -1 &amp; -2 &amp; -1 \\ 0 &amp; 0 &amp; 0 \\ +1 &amp; +2 &amp; +1 \end{bmatrix},<br>\]</p><p>The edges may not lie perfectly in horizontal or vertical direction. Overall, our gradient operator is<br>\[<br>G = \sqrt(G_x^2 + G_y^2)<br>\]<br>Notice that \(G\), as expected, has no sign.</p><p>Based on \(G_x\) and \(G_y\), we can also calculate the direction of the gradient by using<br>\[<br>\theta = atan(\frac{G_y}{G_x})<br>\]</p><p>By convoluting the gradient operator with image intensity, we can find all the pixel position where there is a difference in intensity with its neighbours, no matter how small the difference is. But this operation can also pick up all the noise. Therefore, <em>Sobel Operator</em> is to run a small Gaussian filter first to smooth the image and then apply the gradient operator.</p><p><em>Canny Edge Detector</em> is a modified <em>Sobel Operator</em>. As we see above, <em>Sobel Operator</em> should give as a thick and board edges line as most of the change did not happen in one pixel, but a continuous change across many pixels. <em>Canny Edge Detector</em> is a double threshold method. First, set a minimum threshold, <code>minVal</code>, and a maximum threshold, <code>maxVal</code>. Any pixel with gradient value smaller than the <code>minVal</code> will be ignored, and any pixel with gradient value greater than the <code>maxVal</code> will be accepted. Any pixel with gradient in between will be selected: If they connect with the pixels that are greater than the <code>maxVal</code>, then they will be accepted. Otherwise, they will be ignored.</p><p>By doing so, we can find <em>sharp</em> edges only the trough will likely be ignored.</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><p>In OpenCV, we consider two functions for edge detection, <code>cv2.Sobel</code> and <code>cv2.Canny</code>.<br><em>Note</em> The input image should be a <em>Greyscale</em> image!!</p><pre><code>dst = cv2.Sobel(src, ddepth, dx, dy, dst, ksize, scale, delta, borderType)</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image.<br><strong>dst</strong> -&gt; Destination image.<br><strong>ddepth</strong> -&gt; (compulsory) Output image depth. The supported <strong>src.depth()</strong> and <strong>ddepth</strong> are:<br>  <code>src.depth()</code> = <code>CV_8U</code>, <code>ddepth</code> = <code>-1</code>/<code>CV_16S</code>/<code>CV_32F</code>/<code>CV_64F</code><br>  <code>src.depth()</code> = <code>CV_16U</code>/<code>CV_16S</code>, <code>ddepth</code> = <code>-1</code>/<code>CV_32F</code>/<code>CV_64F</code><br>  <code>src.depth()</code> = <code>CV_32F</code>, <code>ddepth</code> = <code>-1</code>/<code>CV_32F</code>/<code>CV_64F</code><br>  <code>src.depth()</code> = <code>CV_64F</code>, <code>ddepth</code> = <code>-1</code>/<code>CV_64F</code><br><em>Note</em> When <code>ddepth = -1</code>, the destination image will have the same depth as the source; in the case of 8-bit input images it will result in truncated derivatives.<br><strong>xorder</strong> -&gt; (compulsory) Order of the derivative x. Normally, \(0\) or \(1\).<br><strong>yorder</strong> -&gt; (compulsory) Order of the derivative y. Normally, \(0\) or \(1\).<br><strong>ksize</strong> -&gt; (optional) Size of the extended Sobel kernel; it must be 1, 3, 5, or 7. Default value is 3.<br><strong>scale</strong> -&gt; (optional) optional scale factor for the computed derivative values; by default, <em>no scaling</em> is applied<br><strong>delta</strong> -&gt; (optional) optional delta value that is added to the results prior to storing them in <strong>dst</strong>.<br><strong>borderType</strong> -&gt; (optional) pixel extrapolation method</p><pre><code>edges = cv2.Canny(image, threshold1, threshold2, edges, apertureSize, L2gradient)</code></pre><p><strong>image</strong> -&gt; (compulsory) Source image. 8-bit single-channel.<br><strong>edges</strong> -&gt; Output edge map; it has the same size and type as <strong>image</strong>.<br><strong>threshold1</strong> -&gt; (compulsory) first threshold for the hysteresis procedure.<br><strong>threshold2</strong> -&gt; (compulsory) second threshold for the hysteresis procedure.<br><strong>apertureSize</strong> -&gt; (optional) aperture size for the <code>Sobel()</code> operator. Default is \(3\).<br><strong>L2gradient</strong> -&gt; (optional) Default is false. (TODO: discuss in detail later)</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><pre><code>import cv2name = image_nameimg = cv2.imread(&#39;./{}.jpg&#39;.format(name), cv2.IMREAD_UNCHANGED)img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)cv2.imshow(&quot;Grey image&quot;, img)img = cv2.GaussianBlur(img, (3, 3), sigmaX=3, sigmaY=0)  # Uniform distorsion in x and y direcitonsobelx = cv2.Sobel(img,cv2.CV_64F,1,0)sobely = cv2.Sobel(img,cv2.CV_64F,0,1)edges = cv2.Canny(img,30,60)  # the threshold values should change accordingly.cv2.imshow(&quot;X edges&quot;, sobelx)cv2.imshow(&quot;Y edges&quot;, sobely)cv2.imshow(&quot;Canny&quot;, edges)cv2.waitKey(0)cv2.destroyAllWindows()</code></pre><p>Example results are shown below:<br><strong>Greyscale image</strong><br><img src="laptop_gray.jpg" srcset="/img/loading.gif" alt="grey"></p><p><strong>Edges in x direction using Sobel</strong><br><img src="laptop_x.jpg" srcset="/img/loading.gif" alt="x"></p><p><strong>Edges in y direction using Sobel</strong><br><img src="laptop_y.jpg" srcset="/img/loading.gif" alt="y"></p><p>Compare result from Sobel with Canny.<br><strong>Canny Detector</strong><br><img src="laptop_edges.jpg" srcset="/img/loading.gif" alt="edges"></p><hr><h1 id="Compute-Sobel-and-Canny-From-Scratch-TODO"><a href="#Compute-Sobel-and-Canny-From-Scratch-TODO" class="headerlink" title="Compute Sobel and Canny From Scratch (TODO)"></a>Compute Sobel and Canny From Scratch (TODO)</h1><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Photo by Omid Armin on Unsplash</li><li>Docs.opencv.org. 2020. Sobel Derivatives — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/sobel_derivatives/sobel_derivatives.html" target="_blank" rel="noopener">https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/sobel_derivatives/sobel_derivatives.html</a> [Accessed 1 May 2020].</li><li>Opencv-python-tutroals.readthedocs.io. 2020. Image Gradients — Opencv-Python Tutorials 1 Documentation. [online] Available at: <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_gradients/py_gradients.html" target="_blank" rel="noopener">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_gradients/py_gradients.html</a> [Accessed 1 May 2020].</li><li>Opencv-python-tutroals.readthedocs.io. 2020. Canny Edge Detection — Opencv-Python Tutorials 1 Documentation. [online] Available at: <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html" target="_blank" rel="noopener">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html</a> [Accessed 1 May 2020].</li><li>Docs.opencv.org. 2020. Image Filtering — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html?highlight=sobel#void%20Sobel(InputArray%20src,%20OutputArray%20dst,%20int%20ddepth,%20int%20dx,%20int%20dy,%20int%20ksize,%20double%20scale,%20double%20delta,%20int%20borderType)" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html?highlight=sobel#void%20Sobel(InputArray%20src,%20OutputArray%20dst,%20int%20ddepth,%20int%20dx,%20int%20dy,%20int%20ksize,%20double%20scale,%20double%20delta,%20int%20borderType)</a> [Accessed 1 May 2020].</li><li>Docs.opencv.org. 2020. Feature Detection — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=canny" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=canny</a> [Accessed 1 May 2020].</li></ol><h3 id="Two-Additional-materials-that-I-really-recommended"><a href="#Two-Additional-materials-that-I-really-recommended" class="headerlink" title="Two Additional materials that I really recommended"></a>Two Additional materials that I really recommended</h3><p>Finding the Edges (Sobel Operator) - Computerphile -&gt; <a href="https://www.youtube.com/watch?v=uihBwtPIBxM" target="_blank" rel="noopener">https://www.youtube.com/watch?v=uihBwtPIBxM</a><br>Canny Edge Detector - Computerphile -&gt; <a href="https://www.youtube.com/watch?v=sRFM5IEqR2w" target="_blank" rel="noopener">https://www.youtube.com/watch?v=sRFM5IEqR2w</a></p><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation -&gt; Image Processing5: Image Thresholding</title>
    <link href="/2020/04/28/Computer-Vision-Foundation-Image-Processing5-Image-Thresholding/"/>
    <url>/2020/04/28/Computer-Vision-Foundation-Image-Processing5-Image-Thresholding/</url>
    
    <content type="html"><![CDATA[<p>In this post, another basic image operation is explored - Image Thresholding.</p><p>This post is split into four sections:</p><ol><li>The Basic Principles of Image Thresholding</li><li>Practice with OpenCV in Python</li><li>Compute Otsu’s Algorithm and <em>adaptiveThreshold</em> From Scratch</li></ol><p>Source code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; threshold.py (OpenCV)</p><hr><h1 id="The-Basic-Principles-of-Image-Thresholding"><a href="#The-Basic-Principles-of-Image-Thresholding" class="headerlink" title="The Basic Principles of Image Thresholding"></a>The Basic Principles of Image Thresholding</h1><p>Image Thresholding is the simplest segmentation method. This separation is based on the variation of intensity between the object pixels and the background pixels.</p><p>To do that, we compare each pixel value with respect to a set <em>threshold</em>. If one pixel is above the threshold, we can assign a value, otherwise we can assign another value. (e.g. \(255\) if it is above the threshold, \(0\) otherwise)</p><p>However, manually selecting the threshold can be problematic and requires many times of trail-and-error. A useful method is the Otsu Thresholding.</p><p>Otsu’s method was named after its inventor Nobuyuki Otsu. Otsu’s thresholding method involves iterating through all the possible threshold values (\(0\) to \(255\)) and calculating a measure of spread for the pixel levels each side of the threshold. The main purpose is to separate all the pixels into foreground and background, and find the threshold value where the sum of foreground and background spreads is at its minimum. Statistically, the spread is measured by the variance.</p><p>The next step is to calculate the <em>‘Within-Class Variance’</em>. This is simply the sum of the two variances multiplied by their associated weights. Mathematically,<br>\[<br>  \sigma^2 = W_b \sigma^2_b + W_f \sigma^2_f,<br>\]<br>where \(W_b\) and \(W_f\) denote the weights of background and foreground pixels respectively. Besides,<br>\[<br>  W_b + W_f = 1<br>\]<br>as normal.</p><p>Here is an example. The set-up is shown below.<br><img src="otsuOrig.png" srcset="/img/loading.gif" alt="otsu_eg"><br>Then, we can use Otsu’s method and obtain that when the threshold is at \(3\), the <em>‘Within-Class Variance’</em> is at its minimum.<br><img src="otsu_process.png" srcset="/img/loading.gif" alt="otsu_pro"></p><p>The problems of this approach are that the result highly related to the light conditions, and it works only if there are two distinct peaks in the histogram of the image theoretically. If the ambient light does not shine uniformly, this method simply separate the dark side and the bright side. If the histogram of the image has multiple peaks, Otsu’s method still only separate into two because of the theory.</p><p>To overcome those problems, Adaptive Threshold method can be used. Basically, it calculate the threshold for each pixel according to its neighbourhood using the mean of their intensities or the Gaussian distribution of the intensities.</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><p>In OpenCV, we consider two functions for image thresholding, <code>cv2.threshold</code> and <code>cv2.adaptiveThreshold</code>.</p><pre><code>retval, dst = cv2.threshold(src, thresh, maxval, type)</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image. Single-channel, 8-bit or 32-bit floating point.<br><strong>dst</strong> -&gt; Destination image.<br><strong>thresh</strong> -&gt; (compulsory) Threshold value.<br><strong>maxval</strong> -&gt; (compulsory) The value assigned to the pixel after comparing with the threshold value if <strong>THRESH_BINARY</strong> or <strong>THRESH_BINARY_INV</strong> is used.<br><strong>type</strong> -&gt; (compulsory) Thresholding type.</p><p><em>Note</em> The input image should be a <em>Greyscale</em> image!!</p><p>Some basic thresholding types, which are applied in OpenCV, are<br>  cv2.THRESH_BINARY<br>  cv2.THRESH_BINARY_INV<br>  cv2.THRESH_TRUNC<br>  cv2.THRESH_TOZERO<br>  cv2.THRESH_TOZERO_INV<br>The results using different types are shown below.<br><img src="threshold.jpg" srcset="/img/loading.gif" alt="result"></p><p>Next, the details of each type will be presented. Suppose that the histogram of the image and the threshold are as shown.<br><img src="Threshold_Tutorial_Theory_Base_Figure.png" srcset="/img/loading.gif" alt="base"><br>The red is the intensity distribution and the blue line is the threshold.</p><h4 id="THRESH-BINARY"><a href="#THRESH-BINARY" class="headerlink" title="THRESH_BINARY"></a>THRESH_BINARY</h4><p>\[<br>  dst(x, y) = maxval \space \space \mbox{if } src(x, y) &gt; thresh<br>\]<br>\[<br>  dst(x, y) = 0 \space \space \mbox{otherwise}<br>\]<br><img src="Threshold_Tutorial_Theory_Binary.png" srcset="/img/loading.gif" alt="binary"></p><h4 id="THRESH-BINARY-INV"><a href="#THRESH-BINARY-INV" class="headerlink" title="THRESH_BINARY_INV"></a>THRESH_BINARY_INV</h4><p>\[<br>  dst(x, y) = 0 \space \space \mbox{if } src(x, y) &gt; thresh<br>\]<br>\[<br>  dst(x, y) = maxval \space \space \mbox{otherwise}<br>\]<br><img src="Threshold_Tutorial_Theory_Binary_Inverted.png" srcset="/img/loading.gif" alt="binar_inv"></p><h4 id="THRESH-TRUNC"><a href="#THRESH-TRUNC" class="headerlink" title="THRESH_TRUNC"></a>THRESH_TRUNC</h4><p>\[<br>  dst(x, y) = maxval \space \space \mbox{if } src(x, y) &gt; thresh<br>\]<br>\[<br>  dst(x, y) = src(x, y) \space \space \mbox{otherwise}<br>\]<br><img src="Threshold_Tutorial_Theory_Truncate.png" srcset="/img/loading.gif" alt="trunc"></p><h4 id="THRESH-TOZERO"><a href="#THRESH-TOZERO" class="headerlink" title="THRESH_TOZERO"></a>THRESH_TOZERO</h4><p>\[<br>  dst(x, y) = src(x, y) \space \space \mbox{if } src(x, y) &gt; thresh<br>\]<br>\[<br>  dst(x, y) = 0 \space \space \mbox{otherwise}<br>\]<br><img src="Threshold_Tutorial_Theory_Zero.png" srcset="/img/loading.gif" alt="tozero"></p><h4 id="THRESH-TOZERO-INV"><a href="#THRESH-TOZERO-INV" class="headerlink" title="THRESH_TOZERO_INV"></a>THRESH_TOZERO_INV</h4><p>\[<br>  dst(x, y) = 0 \space \space \mbox{if } src(x, y) &gt; thresh<br>\]<br>\[<br>  dst(x, y) = src(x, y) \space \space \mbox{otherwise}<br>\]<br><img src="Threshold_Tutorial_Theory_Zero_Inverted.png" srcset="/img/loading.gif" alt="tozero_inv"></p><p><em>Note</em> To use Otsu thresholding, pass <code>thresh = 0</code> and <code>type = (...threshold type...)+cv2.THRESH_OTSU</code>.</p><pre><code>dst = cv2.adaptiveThreshold(src, maxValue, adaptiveMethod, thresholdType, blockSize, C)</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image. 8-bit single-channel.<br><strong>dst</strong> -&gt; Destination image.<br><strong>maxValue</strong> -&gt; (compulsory) Non-zero value assigned to the pixels for which the condition is satisfied.<br><strong>adaptiveMethod</strong> -&gt; (compulsory) Adaptive thresholding algorithm to use, <strong>ADAPTIVE_THRESH_MEAN_C</strong> or <strong>ADAPTIVE_THRESH_GAUSSIAN_C</strong>.<br><strong>thresholdType</strong> -&gt; (compulsory) Thresholding type that must be either <strong>THRESH_BINARY</strong> or <strong>THRESH_BINARY_INV</strong>.<br><strong>blockSize</strong> -&gt; (compulsory) Size of a pixel neighbourhood that is used to calculate a threshold value for the pixel: 3, 5, and so on.<br><strong>C</strong> -&gt; (compulsory) Constant subtracted from the mean or weighted mean. It is normally <em>positive</em> but may be zero or negative as well.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><pre><code>import cv2name = image_nameimg = cv2.imread(&#39;./{}.jpg&#39;.format(name), cv2.IMREAD_UNCHANGED)img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)cv2.imshow(&quot;Grey image&quot;, img)# Simple image thresholding with your defined value (in this case, 127)_, simple = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)cv2.imshow(&quot;Simple thresholding image&quot;, simple)# Otsu thresholding (retval is the threshold found with Otsu Algorithm)retval, otsu = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)cv2.imshow(&quot;Otsu thresholding image&quot;, otsu)print(retval)# Mean Adaptive Thresholding and Gaussian Adaptive Thresholdingmean = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)gauss = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)cv2.imshow(&quot;Mean Adaptive Thresholding image&quot;, mean)cv2.imshow(&quot;Gaussian Adaptive Thresholding image&quot;, gauss)cv2.waitKey(0)cv2.destroyAllWindows()</code></pre><p>Example results are shown below:<br><strong>Greyscale image</strong><br><img src="laptop_original.jpg" srcset="/img/loading.gif" alt="grey"></p><p><strong>Simple Thresholding with Threshold = 127</strong><br><img src="laptop_simple_threshold.jpg" srcset="/img/loading.gif" alt="simple"></p><p><strong>Otsu Thresholding with Threshold = 48)</strong><br><img src="laptop_otsu.jpg" srcset="/img/loading.gif" alt="otsu"></p><p>Compare result from Otsu thresholding with next two.<br><strong>Mean Adaptive Thresholding</strong><br><img src="laptop_mean_adaptive_threshold.jpg" srcset="/img/loading.gif" alt="mean"></p><p><strong>Gaussian Adaptive Thresholding</strong><br><img src="laptop_gaussian_adaptive_threshold.jpg" srcset="/img/loading.gif" alt="gauss"></p><h3 id="Adaptive-Thresholding-Parameters"><a href="#Adaptive-Thresholding-Parameters" class="headerlink" title="Adaptive Thresholding Parameters"></a>Adaptive Thresholding Parameters</h3><p>Two important parameters in adaptive thresholding are <strong>blockSize</strong> and <strong>C</strong>. Here, some results from mean adaptive thresholding with different parameters are presented for comparison.</p><h4 id="blockSize"><a href="#blockSize" class="headerlink" title="blockSize"></a>blockSize</h4><p><strong>Adaptive Thresholding with blockSize = 3</strong><br><img src="laptop_mean_Kernel_size_3.jpg" srcset="/img/loading.gif" alt="3"></p><p><strong>Adaptive Thresholding with blockSize = 9</strong><br><img src="laptop_mean_Kernel_size_9.jpg" srcset="/img/loading.gif" alt="9"></p><p><strong>Adaptive Thresholding with blockSize = 15</strong><br><img src="laptop_mean_Kernel_size_15.jpg" srcset="/img/loading.gif" alt="15"></p><h4 id="C"><a href="#C" class="headerlink" title="C"></a>C</h4><p><strong>Adaptive Thresholding with C = -20</strong><br><img src="laptop_mean_c_-20.jpg" srcset="/img/loading.gif" alt="minus20"></p><p><strong>Adaptive Thresholding with C = -10</strong><br><img src="laptop_mean_c_-10.jpg" srcset="/img/loading.gif" alt="minus10"></p><p><strong>Adaptive Thresholding with C = 0</strong><br><img src="laptop_mean_c_0.jpg" srcset="/img/loading.gif" alt="0"></p><p><strong>Adaptive Thresholding with C = 10</strong><br><img src="laptop_mean_c_10.jpg" srcset="/img/loading.gif" alt="10"></p><p><strong>Adaptive Thresholding with C = 20</strong><br><img src="laptop_mean_c_20.jpg" srcset="/img/loading.gif" alt="20"></p><hr><h1 id="Compute-Otsu’s-Algorithm-From-Scratch-TODO"><a href="#Compute-Otsu’s-Algorithm-From-Scratch-TODO" class="headerlink" title="Compute Otsu’s Algorithm From Scratch (TODO)"></a>Compute Otsu’s Algorithm From Scratch (TODO)</h1><h2 id="Also-some-optimisations-available-for-Otsu’s-Thresholding-exists-Search-and-implement-it"><a href="#Also-some-optimisations-available-for-Otsu’s-Thresholding-exists-Search-and-implement-it" class="headerlink" title="Also, some optimisations available for Otsu’s Thresholding exists. Search and implement it."></a>Also, some optimisations available for Otsu’s Thresholding exists. Search and implement it.</h2><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Photo by Omid Armin on Unsplash</li><li>Blog.csdn.net. 2020. OTSU算法（大津法—最大类间方差法）原理及实现_人工智能_小武的博客-CSDN博客. [online] Available at: <a href="https://blog.csdn.net/weixin_40647819/article/details/90179953" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40647819/article/details/90179953</a> [Accessed 29 April 2020].</li><li>Blog.csdn.net. 2020. 自适应阈值（Adaptivethreshold）分割原理及实现_人工智能_小武的博客-CSDN博客. [online] Available at: <a href="https://blog.csdn.net/weixin_40647819/article/details/90213858" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40647819/article/details/90213858</a> [Accessed 29 April 2020].</li><li>Docs.opencv.org. 2020. Basic Thresholding Operations — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/doc/tutorials/imgproc/threshold/threshold.html" target="_blank" rel="noopener">https://docs.opencv.org/2.4/doc/tutorials/imgproc/threshold/threshold.html</a> [Accessed 29 April 2020].</li><li>Opencv-python-tutroals.readthedocs.io. 2020. Image Thresholding — Opencv-Python Tutorials 1 Documentation. [online] Available at: <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html#exercises" target="_blank" rel="noopener">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html#exercises</a> [Accessed 29 April 2020].</li><li>Labbookpages.co.uk. 2020. Otsu Thresholding - The Lab Book Pages. [online] Available at: <a href="http://www.labbookpages.co.uk/software/imgProc/otsuThreshold.html" target="_blank" rel="noopener">http://www.labbookpages.co.uk/software/imgProc/otsuThreshold.html</a> [Accessed 29 April 2020].</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation -&gt; Image Processing4: Image Filtering</title>
    <link href="/2020/04/26/Computer-Vision-Foundation-Image-Processing4-Image-Filtering/"/>
    <url>/2020/04/26/Computer-Vision-Foundation-Image-Processing4-Image-Filtering/</url>
    
    <content type="html"><![CDATA[<p>In this post, another basic image operation is explored - Image Filtering.</p><p>This post is split into four sections:</p><ol><li>The Basic Principles of Image Filtering</li><li>Details of <em>blur</em>, <em>boxFilter</em> and <em>GaussianBlur</em> Functions</li><li>Practice with OpenCV in Python</li><li>Build <em>boxFilter</em> and <em>GaussianBlur</em> Functions From Scratch</li></ol><p>Source code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; filter.py (OpenCV)</p><hr><h1 id="The-Basic-Principles-of-Image-Filtering"><a href="#The-Basic-Principles-of-Image-Filtering" class="headerlink" title="The Basic Principles of Image Filtering"></a>The Basic Principles of Image Filtering</h1><p>In image processing, filters are mainly used to suppress either the high frequencies in the image (<em>smoothing the image</em>) or the low frequencies (<em>enhancing or detecting edges</em>) in the image.</p><p>An image can be filtered either in the <strong>frequency</strong> or in the <strong>spatial</strong> domain.</p><p>To operate in the frequency domain, we should first extract frequencies of the image, then multiple by a value. For example, to smooth the image, we multiple \(0\) to the high frequencies, and \(1\) to the low frequencies. Therefore, high frequencies can be filtered out. Final, we need to transform back to spatial domain to get the image. (c.f. Fourier Transform)</p><p>In the spatial domain, convolution is equivalent to multiplication in the frequency domain. (Proof in Appendix) Mathematically,<br>\[<br>g(i, j) = h(i, j) * f(i, j),<br>\]<br>where, \(g(i, j)\) is the resulted value at point \((i, j)\), \(h(i, j)\) is the filter function and \(f(i, j)\) is the input image.</p><p>The methods discussed above are equivalent in maths. But the results of the implementations in computer are different, since we have to <em>approximate</em> the filter function with a discrete and finite <strong>kernel</strong>.</p><p><em>Note</em> A <strong>kernel</strong> is (usually) a smallish matrix of numbers that is used in image convolutions. Differently sized kernels containing different patterns of numbers give rise to different results under convolution. The word <strong>kernel</strong> is also commonly used as a synonym for <strong>structuring element</strong>, which is a similar object used in mathematical morphology. A structuring element differs from a kernel in that it also has a specified <em>origin</em>.</p><p>The discrete convolution can be defined as a <em>‘shift and multiply’</em> operation, where we shift the kernel over the image and multiply its value with the corresponding pixel values of the image. For a square kernel with size \((M \times M)\), we can calculate the output image with the following formula:<br>\[<br>g(i, j) = \sum_{m = -\frac{M}{2}}^{\frac{M}{2}} \sum_{n = -\frac{M}{2}}^{\frac{M}{2}} h(m, n) f(i - m, j - n),<br>\]</p><p>Also, it is intuitive to apply some non-linear filter in spatial domain.</p><p><strong>Linear Filter</strong>: Box Filter; Gaussian Filter; Laplacian Filter.<br><strong>Non-linear Filter</strong>: Median Filter.</p><hr><h1 id="Details-of-Two-Main-Operations"><a href="#Details-of-Two-Main-Operations" class="headerlink" title="Details of Two Main Operations"></a>Details of Two Main Operations</h1><h3 id="Box-Filter"><a href="#Box-Filter" class="headerlink" title="Box Filter"></a>Box Filter</h3><p>Mean filtering is a simple, intuitive and easy to implement method of smoothing images, i.e. reducing the amount of intensity variation between one pixel and the next.</p><p>Mean filter is a special type of box filter. The main purpose of the mean filter is to blur the unessential parts of the image while keeping the essential features. The <em>unessential</em> parts are the details which are smaller than the kernel in size.</p><p>The drawback of the mean filter is that it is not good at filtering out the noise points from the image.</p><h3 id="Gaussian-Filter"><a href="#Gaussian-Filter" class="headerlink" title="Gaussian Filter"></a>Gaussian Filter</h3><p>The kernel matches to a 2D Gaussian distribution with equation:<br>\[<br>h(x, y) = \frac{1}{2 \pi \sigma^2}e^{-\frac{x^2+y^2}{2\sigma^2}},<br>\]</p><p>Sometimes, for the ease of computation, the first constant term can be ignored.</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><h4 id="Box-Filter-1"><a href="#Box-Filter-1" class="headerlink" title="Box Filter"></a>Box Filter</h4><p>In OpenCV, box filter can be applied with two different functions, <code>cv2.blur</code> and <code>cv2.boxFilter</code>. If a <strong>normalised</strong> box filter should be used, then these two functions are equivalent. But if an <strong>unnormalised</strong> box filter should be used, then you should consider <code>cv2.boxFilter</code> only.</p><p>First is <code>cv2.blur</code></p><pre><code>dst = cv2.blur(src, ksize[, dst[, anchor[, borderType]]])</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image. it can have <em>any</em> number of channels, but the depth should be <code>CV_8U</code>, <code>CV_16U</code>, <code>CV_16S</code>, <code>CV_32F</code> or <code>CV_64F</code>.<br><strong>dst</strong> -&gt; Destination image.<br><strong>ksize</strong> -&gt; (compulsory) blurring kernel size.<br><strong>anchor</strong> -&gt; (optional) anchor point. Default value \(Point(-1,-1)\) means that the anchor is at the kernel centre.<br><strong>borderType</strong> -&gt; (optional) border mode used to extrapolate pixels outside of the image.</p><p>Next is <code>cv2.boxFilter</code></p><pre><code>dst = cv2.boxFilter(src, ddepth, ksize[, dst[, anchor[, normalize[, borderType]]]])</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image.<br><strong>dst</strong> -&gt; Destination image.<br><strong>ddepth</strong> -&gt; (compulsory) the output image depth. If the input is \(-1\), then <code>src.depth()</code> is used.<br><strong>ksize</strong> -&gt; (compulsory) blurring kernel size.<br><strong>anchor</strong> -&gt; (optional) anchor point. Default value \(Point(-1,-1)\) means that the anchor is at the kernel centre.<br><strong>normalize</strong> -&gt; (optional) To specify whether the kernel is normalised by its area. Default is <em>True</em><br><strong>borderType</strong> -&gt; (optional) border mode used to extrapolate pixels outside of the image.</p><h4 id="Gaussian-Filter-1"><a href="#Gaussian-Filter-1" class="headerlink" title="Gaussian Filter"></a>Gaussian Filter</h4><pre><code>dst = cv2.GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]])</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image. it can have <em>any</em> number of channels, but the depth should be <code>CV_8U</code>, <code>CV_16U</code>, <code>CV_16S</code>, <code>CV_32F</code> or <code>CV_64F</code>.<br><strong>dst</strong> -&gt; Destination image.<br><strong>ksize</strong> -&gt; (compulsory) Gaussian kernel size. <code>ksize.width</code> and <code>ksize.height</code> can differ but they both must be <em>positive</em> and <em>odd</em>.<br><strong>sigmaX</strong> -&gt; (compulsory) Gaussian kernel standard deviation in X direction.<br><strong>sigmaY</strong> -&gt; (optional) Gaussian kernel standard deviation in Y direction. Default value is \(0\), which means it is the same as the <strong>sigmaX</strong>.<br><strong>borderType</strong> -&gt; (optional) border mode used to extrapolate pixels outside of the image.</p><p><em>Note</em> If <strong>sigmaX</strong> and <strong>sigmaY</strong> are both \(0\), then they will be automatically calculated from <code>ksize.width</code> and <code>ksize.height</code> respectively. However, it is recommended to specify <em>all</em> of <strong>ksize</strong>, <strong>sigmaX</strong> and <strong>sigmaY</strong>.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><pre><code>import cv2name = image_nameimg = cv2.imread(&#39;./{}.jpg&#39;.format(name), cv2.IMREAD_UNCHANGED)cv2.imshow(&quot;Original image&quot;, img)# Box Filter using two different methodsuni_blur_less = cv2.blur(img, (5, 5))uni_blur_more = cv2.boxFilter(img, -1, (9, 9))# Gaussian Filter with different sigmagauss_blur_uniform = cv2.GaussianBlur(img, (9, 9), sigmaX=5, sigmaY=0)  # Uniform distorsion in x and y direcitongauss_blur_x = cv2.GaussianBlur(img, (9, 9), sigmaX=10, sigmaY=1)  # Main distorsion in x direcitongauss_blur_y = cv2.GaussianBlur(img, (9, 9), sigmaX=1, sigmaY=10)  # Main distorsion in y direcitoncv2.imshow(&quot;Blur image with small kernel&quot;, uni_blur_less)cv2.imshow(&quot;Blur image with large kernel&quot;, uni_blur_more)cv2.imshow(&quot;Blur image with gaussian kernel (sigmaX = sigmaY)&quot;, gauss_blur_uniform)cv2.imshow(&quot;Blur image with gaussian kernel (sigmaX &gt; sigmaY)&quot;, gauss_blur_x)cv2.imshow(&quot;Blur image with gaussian kernel (sigmaX &lt; sigmaY)&quot;, gauss_blur_y)cv2.waitKey(0)cv2.destroyAllWindows()</code></pre><p>Example results are shown below:</p><h4 id="Box-Filter-2"><a href="#Box-Filter-2" class="headerlink" title="Box Filter"></a>Box Filter</h4><p><strong>Less Blurring</strong><br><img src="flower_less_blur.jpg" srcset="/img/loading.gif" alt="less"></p><p><strong>More Blurring</strong><br><img src="flower_more_blur.jpg" srcset="/img/loading.gif" alt="more"></p><h4 id="Gaussian-Filter-2"><a href="#Gaussian-Filter-2" class="headerlink" title="Gaussian Filter"></a>Gaussian Filter</h4><p><strong>sigmaX = sigmaY</strong><br><img src="flower_gauss_blur_uniform.jpg" srcset="/img/loading.gif" alt="uni"></p><p><strong>sigmaX &gt; sigmaY</strong><br><img src="flower_gauss_blur_x.jpg" srcset="/img/loading.gif" alt="x"></p><p><strong>sigmaX &lt; sigmaY</strong><br><img src="flower_gauss_blur_y.jpg" srcset="/img/loading.gif" alt="y"></p><hr><h1 id="Build-boxFilter-and-GaussianBlur-Functions-From-Scratch-TODO"><a href="#Build-boxFilter-and-GaussianBlur-Functions-From-Scratch-TODO" class="headerlink" title="Build boxFilter and GaussianBlur Functions From Scratch (TODO)"></a>Build <em>boxFilter</em> and <em>GaussianBlur</em> Functions From Scratch (TODO)</h1><hr><h1 id="Option-Other-Image-Filtering-TODO"><a href="#Option-Other-Image-Filtering-TODO" class="headerlink" title="(Option) Other Image Filtering (TODO)"></a>(Option) Other Image Filtering (TODO)</h1><hr><h1 id="Option-Test-denoising-for-different-filters-TODO"><a href="#Option-Test-denoising-for-different-filters-TODO" class="headerlink" title="(Option) Test denoising for different filters (TODO)"></a>(Option) Test denoising for different filters (TODO)</h1><hr><h1 id="Appendix-Proof-of-The-convolution-theorem-TODO"><a href="#Appendix-Proof-of-The-convolution-theorem-TODO" class="headerlink" title="Appendix - Proof of The convolution theorem (TODO)"></a>Appendix - Proof of The convolution theorem (TODO)</h1><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Photo by Viktor Mogilat on Unsplash.</li><li>Blog.csdn.net. 2020. 均值滤波原理及C++实现_C/C++小武的博客-CSDN博客. [online] Available at: <a href="https://blog.csdn.net/weixin_40647819/article/details/88774522" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40647819/article/details/88774522</a> [Accessed 27 April 2020].</li><li>Homepages.inf.ed.ac.uk. 2020. Digital Filters. [online] Available at: <a href="https://homepages.inf.ed.ac.uk/rbf/HIPR2/filtops.htm" target="_blank" rel="noopener">https://homepages.inf.ed.ac.uk/rbf/HIPR2/filtops.htm</a> [Accessed 27 April 2020].</li><li>Docs.opencv.org. 2020. Image Filtering — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html</a> [Accessed 27 April 2020].</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation -&gt; Image Processing3: Colour Conversion</title>
    <link href="/2020/04/24/Computer-Vision-Foundation-Image-Processing3-Colour-Conversion/"/>
    <url>/2020/04/24/Computer-Vision-Foundation-Image-Processing3-Colour-Conversion/</url>
    
    <content type="html"><![CDATA[<p>In this post, another basic image operation is explored - Colour conversion.</p><p>This post is split into four sections:</p><ol><li>The Basic Principles of Colour Conversion</li><li>Details of <em>RGB2GRAY</em> and <em>RGB2HSV</em> Functions and Their Inverse Functions</li><li>Practice with OpenCV in Python</li><li>Build <em>RGB2GRAY</em> and <em>RGB2HSV</em> Functions and Their Inverse Functions From Scratch</li></ol><p>Source code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; colour.py (OpenCV); gray.py (My <em>RGB2GRAY</em>)</p><hr><h1 id="The-Basic-Principles-of-Colour-Conversion"><a href="#The-Basic-Principles-of-Colour-Conversion" class="headerlink" title="The Basic Principles of Colour Conversion"></a>The Basic Principles of Colour Conversion</h1><h3 id="Greyscale"><a href="#Greyscale" class="headerlink" title="Greyscale"></a>Greyscale</h3><p>A greyscale image is one in which the value of each pixel represents an amount of light. In other words, it carries only intensity information. The contrast ranges from black at the weakest intensity to white at the strongest.</p><p>Although the conversion from RGB to grey image can easily be done, sadly there is no simple explanation. Compare the following images<br><img src="laptop_original.jpg" srcset="/img/loading.gif" alt="laptop"><br><img src="laptop_Mean_Grey.jpg" srcset="/img/loading.gif" alt="laptop_mean"><br><img src="laptop_RGB_to_Grey.jpg" srcset="/img/loading.gif" alt="laptop_grey"><br>The second one was obtained by <strong>averaging RGB channels</strong>. The third one was obtained using <strong>OpenCV</strong> standard, which will be introduced in later sessions.</p><p>The goal of the conversion is to find the balance point between human perception and computational ease.</p><h4 id="Perceptual-Luminance-preserving-Conversion-to-Greyscale"><a href="#Perceptual-Luminance-preserving-Conversion-to-Greyscale" class="headerlink" title="Perceptual Luminance-preserving Conversion to Greyscale"></a>Perceptual Luminance-preserving Conversion to Greyscale</h4><p>The best conversion should provide the greyscale image with the same luminance as the original colour image. Details can conduct (Grayscale, 2020, <a href="https://en.wikipedia.org/wiki/Grayscale" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Grayscale</a>).</p><p>In standard,<br>\[<br>Y_{linear} = 0.2126 R_{linear} + 0.7152 G_{linear} + 0.0722 B_{linear},<br>\]<br>where \(Y_{linear}\) denotes linear luminance, and \(R_{linear}\), \(G_{linear}\) and \(B_{linear}\) represent linear luminance of each colour channel respectively. These three coefficients are the intensity perception of a normal person with the definition of <em>sRGB</em>.<br><strong>Human vision is most sensitive to green, so this has the greatest coefficient value (0.7152), and least sensitive to blue, so this has the smallest coefficient (0.0722)</strong>.</p><p>However, for a typical RGB image, the colour channels do not store linear luminance. The RGB values are gamma-compressed. Details are in (Gamma correction, 2020, <a href="https://en.wikipedia.org/wiki/Gamma_correction" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Gamma_correction</a>). To simplify, psychophysics suggested that human perception of physical quantities are non-linear. Empirically, it follows Stevens’s power law. Hence, if an image is made without power law, then we will feel unnatural since it will not match to our observation by eyes. Gamma compression is a correction to make the image fit to the power law.</p><p>Solving power equations are computationally expensive. Therefore, the <em>RGB2GRAY</em> operation in everyday life is merely an approximation.</p><h3 id="HSV"><a href="#HSV" class="headerlink" title="HSV"></a>HSV</h3><p>Most of the digital displays produce colours by combining red, green, and blue light with various intensities. Red, green and blue are so-called RGB additive primary colours. The resulting mixtures in RGB colour space can reproduce a wide variety of colours called a <strong>gamut</strong>. Gamut, or colour gamut, is a <em>complete subset</em> of colours, shown below.<br><img src="gamut.png" srcset="/img/loading.gif" alt="gamut"><br><em>CIE 1931 xy chromaticity diagram</em> showing the gamut of the <em>sRGB</em> colour space (the triangle). The outer curved boundary is the monochromatic spectrum with wavelengths shown in nanometers labeled in blue. This image is drawn using sRGB, so colours outside the triangle cannot be accurately coloured. The <em>D65 white point</em> is shown in the centre, and the <em>Planckian locus</em> is shown with colour temperatures labeled in kelvins. D65 is not an ideal 6500-kelvin blackbody because it is based on atmospheric filtered daylight.</p><p>The problem is that using RGB to generate colour is not intuitive. For example, as shown in the figure below, changing from one orange to a less saturated orange requires to modified RGB by different amounts.<br><img src="colorchange.png" srcset="/img/loading.gif" alt="colour_change"></p><p>In the mid-1970s Alvy Ray Smith described the <strong>HSV</strong> model for computer display technology to accommodate more intuitive colour mixing models. These models were useful because they were not only more intuitive than raw RGB values, but also the conversions to and from RGB were extremely fast to compute. <strong>They could run in real time on the hardware of the 1970s</strong>.<br><img src="hsv.jpg" srcset="/img/loading.gif" alt="hsv"><br>HSV stands for “hue, saturation, value”. The model can be shown in a cone. To make them simple,</p><p>  Hue: The angle of a circle, which is the cross-section of the HSV cone. The degrees indicate different colours. By convention, Red is at \(0\), green is at \(120\), and blue is at \(240\).</p><p>  Saturation: The radius of a circle, which is the cross-section of the HSV cone. The lengths indicate different colourfulness.</p><p>  Value: The height of the HSV cone. The lengths indicate different brightness.</p><hr><h1 id="Details-of-Two-Main-Operations"><a href="#Details-of-Two-Main-Operations" class="headerlink" title="Details of Two Main Operations"></a>Details of Two Main Operations</h1><h3 id="RGB-lt-–-gt-Grey"><a href="#RGB-lt-–-gt-Grey" class="headerlink" title="RGB &lt;–&gt; Grey"></a>RGB &lt;–&gt; Grey</h3><p>The conversion from RGB to Grey image is<br>\[<br>Y = 0.299 R + 0.587 G + 0.114 B,<br>\]<br>where \(Y\) is the greyscale value, and \(R\), \(G\), \(B\) denotes RGB values for each channel respectively.<br>As mentioned previously, this equation is an approximation of the accurate conversion. The equation works for digital formats following <strong>Rec. 601</strong> (i.e. most digital standard definition formats).</p><p><em>Note</em> The \(Y\) is gamma compressed, thus we do not need to worry about the power law.</p><p>The conversion from Grey to RGB image is<br>\[<br>R = Y; G = Y; B = Y.<br>\]<br>This operation cannot produce a colourful image, but simply create a RGB image form.</p><h3 id="RGB-lt-–-gt-HSV"><a href="#RGB-lt-–-gt-HSV" class="headerlink" title="RGB &lt;–&gt; HSV"></a>RGB &lt;–&gt; HSV</h3><p>In case of 8-bit and 16-bit images, RGB values are converted to the floating-point format and scaled to fit the 0 to 1 range.<br>\[<br>V = \max{(R, G, B)}<br>\]<br>\[<br>S = \frac{V - \min{(R, G, B)}}{V} \space \space \mbox{if } V \neq 0<br>\]<br>\[<br>S = 0 \space \space \mbox{if } V = 0<br>\]<br>\[<br>H = \frac{60(G - B)}{V - \min{(R, G, B)}} \space \space \mbox{if } V = R<br>\]<br>\[<br>H = 120 + \frac{60(B - R)}{V - \min{(R, G, B)}} \space \space \mbox{if } V = G<br>\]<br>\[<br>H = 240 + \frac{60(R - G)}{V - \min{(R, G, B)}} \space \space \mbox{if } V = B<br>\]<br>If \(H &lt; 0\) then \(H = H + 360\). The output \(0 \leqslant V \leqslant 1\), \(0 \leqslant S \leqslant 1\), \(0 \leqslant H \leqslant 360\).</p><p>The final output of the HSV format depends on the image data types. By default, <code>CV_32F</code> is output.<br>  <code>CV_8U</code>: \(V = 255V\), \(S = 255S\), \(H = H/2\)<br>  <code>CV_16U</code>: \(V = -65535V\), \(S = -65535S\), \(H = -H\) (<em>currently not supported</em>)</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><p>In OpenCV, a general function takes care of all colour conversions.</p><pre><code>dst = cv2.cvtColor(src, code[, dst[, dstCn]])</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image. 8-bit unsigned (<code>CV_8U</code>), 16-bit unsigned (<code>CV_16U</code>), or single-precision floating-point (<code>CV_32F</code>).<br><strong>dst</strong> -&gt; Destination image.<br><strong>code</strong> -&gt; (compulsory) colour space conversion code.<br><strong>dstCn</strong> -&gt; (optional) number of channels in the destination image. If it is \(0\), <strong>dstCn</strong> is derived automatically from <strong>src</strong> and <strong>code</strong>.</p><p><em>Note</em> The default colour format in OpenCV is actually <strong>BGR</strong>!!<br>The conventional range for RGB channel values are:<br>  \(0\) to \(255\) for <code>CV_8U</code><br>  \(0\) to \(65535\) for <code>CV_16U</code><br>  \(0\) to \(1\) for <code>CV_32F</code><br>For linear transformation, the range does not matter. But it does for non-linear transformation.</p><p>The <strong>code</strong> that we care about are <code>CV_BGR2GRAY</code>, <code>CV_GRAY2BGR</code>, <code>CV_BGR2HSV</code> and <code>CV_HSV2BGR</code>.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><h4 id="RGB-lt-–-gt-Grey-1"><a href="#RGB-lt-–-gt-Grey-1" class="headerlink" title="RGB &lt;–&gt; Grey"></a>RGB &lt;–&gt; Grey</h4><pre><code>import cv2name = image_nameimg = cv2.imread(&#39;./{}.jpg&#39;.format(name), cv2.IMREAD_UNCHANGED)cv2.imshow(&quot;Original image&quot;, img)bgr2grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)grey2bgr = cv2.cvtColor(bgr2grey, cv2.COLOR_GRAY2BGR)cv2.imshow(&quot;BGR to Grey image&quot;, bgr2grey)cv2.imshow(&quot;Grey to RGB image&quot;, grey2bgr)print(bgr2grey.shape)print(grey2bgr.shape)cv2.waitKey(0)cv2.destroyAllWindows()</code></pre><p>Example results are shown below:<br><strong>The original image</strong> (From <em>Image Processing1</em> post)<br><img src="flower_original.jpg" srcset="/img/loading.gif" alt="small"></p><p><strong>RGB_to_Grey</strong><br><img src="flower_RGB_to_Grey.jpg" srcset="/img/loading.gif" alt="c2g"></p><p><strong>Grey_to_RGB</strong><br><img src="flower_Grey_to_RGB.jpg" srcset="/img/loading.gif" alt="g2c"></p><p><em>Note</em> The <strong>Grey_to_RGB</strong> cannot recover the original image as expected. The information of RGB values has lost. The reason of using this function is to change the dimension of the grey image.</p><p>In our case, <strong>RGB_to_Grey</strong> has dimension \((530 \times 742)\), but <strong>Grey_to_RGB</strong> has dimension \((530 \times 742 \times 3)\).</p><h4 id="RGB-lt-–-gt-HSV-1"><a href="#RGB-lt-–-gt-HSV-1" class="headerlink" title="RGB &lt;–&gt; HSV"></a>RGB &lt;–&gt; HSV</h4><pre><code>import cv2name = image_nameimg = cv2.imread(&#39;./{}.jpg&#39;.format(name), cv2.IMREAD_UNCHANGED)cv2.imshow(&quot;Original image&quot;, img)bgr2hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)hsv2bgr = cv2.cvtColor(bgr2hsv, cv2.COLOR_HSV2BGR)cv2.imshow(&quot;RGB to HSV image&quot;, bgr2hsv)cv2.imshow(&quot;HSV to RGB image&quot;, hsv2bgr)cv2.waitKey(0)cv2.destroyAllWindows()</code></pre><p>Example results are shown below:<br><strong>RGB_to_HSV</strong><br><img src="flower_RGB_to_HSV.jpg" srcset="/img/loading.gif" alt="h2g"></p><p><strong>HSV_to_RGB</strong><br><img src="flower_HSV_to_RGB.jpg" srcset="/img/loading.gif" alt="g2h"></p><p><em>Note</em> The unnatural colour of the <strong>RGB_to_HSV</strong> is ascribed to the fact that we use RGB to display HSV. <strong>HSV_to_RGB</strong> recover the original image as expected.</p><hr><h1 id="Build-RGB2GRAY-and-RGB2HSV-Functions-From-Scratch"><a href="#Build-RGB2GRAY-and-RGB2HSV-Functions-From-Scratch" class="headerlink" title="Build RGB2GRAY and RGB2HSV Functions From Scratch"></a>Build <em>RGB2GRAY</em> and <em>RGB2HSV</em> Functions From Scratch</h1><h3 id="RGB2GRAY"><a href="#RGB2GRAY" class="headerlink" title="RGB2GRAY"></a><em>RGB2GRAY</em></h3><pre><code>import numpy as npdef bgr_to_gray(img):    cols, rows, channels = img.shape    gray = np.zeros((cols, rows))    b = img[:, :, 0]    g = img[:, :, 1]    r = img[:, :, 2]    gray[:, :] = 0.229*r + 0.587*g + 0.114*b    return gray</code></pre><p>The output is shown as below.<br><strong>My function</strong><br><img src="My_flower_RGB_to_Grey.jpg" srcset="/img/loading.gif" alt="my_grey"><br><strong>OpenCV</strong><br><img src="flower_RGB_to_Grey.jpg" srcset="/img/loading.gif" alt="open_grey"></p><p><em>Note</em> Some techniques can be applied to avoid floating point calculations.</p><h3 id="RGB2HSV-TODO"><a href="#RGB2HSV-TODO" class="headerlink" title="RGB2HSV (TODO)"></a><em>RGB2HSV</em> (TODO)</h3><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Photo by Viktor Mogilat on Unsplash.</li><li>Smith, A.R., 1978. Color gamut transform pairs. ACM Siggraph Computer Graphics, 12(3), pp.12-19.</li><li>En.wikipedia.org. 2020. SRGB. [online] Available at: <a href="https://en.wikipedia.org/wiki/SRGB" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/SRGB</a> [Accessed 24 April 2020].</li><li>En.wikipedia.org. 2020. HSL And HSV. [online] Available at: <a href="https://en.wikipedia.org/wiki/HSL_and_HSV#cite_ref-Smith_13-1" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/HSL_and_HSV#cite_ref-Smith_13-1</a> [Accessed 24 April 2020].</li><li>Blog.csdn.net. 2020. 色彩转换系列之RGB格式与HSV格式互转原理及实现_人工智能_小武的博客-CSDN博客. [online] Available at: <a href="https://blog.csdn.net/weixin_40647819/article/details/92660320" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40647819/article/details/92660320</a> [Accessed 24 April 2020].</li><li>Docs.opencv.org. 2020. Miscellaneous Image Transformations — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html</a> [Accessed 24 April 2020].</li><li>Photo by Omid Armin on Unsplash</li><li>Docs.opencv.org. 2020. Opencv: Color Conversions. [online] Available at: <a href="https://docs.opencv.org/3.1.0/de/d25/imgproc_color_conversions.html" target="_blank" rel="noopener">https://docs.opencv.org/3.1.0/de/d25/imgproc_color_conversions.html</a> [Accessed 24 April 2020].</li><li>Tutorialspoint.com. 2020. Grayscale To RGB Conversion - Tutorialspoint. [online] Available at: <a href="https://www.tutorialspoint.com/dip/grayscale_to_rgb_conversion.htm" target="_blank" rel="noopener">https://www.tutorialspoint.com/dip/grayscale_to_rgb_conversion.htm</a> [Accessed 24 April 2020].</li><li>En.wikipedia.org. 2020. Grayscale. [online] Available at: <a href="https://en.wikipedia.org/wiki/Grayscale" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Grayscale</a> [Accessed 24 April 2020].</li><li>En.wikipedia.org. 2020. Gamma Correction. [online] Available at: <a href="https://en.wikipedia.org/wiki/Gamma_correction" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Gamma_correction</a> [Accessed 24 April 2020].</li><li>Stevens, S.S., 1957. On the psychophysical law. Psychological review, 64(3), p.153.</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation -&gt; Image Processing2: Geometric Transformation</title>
    <link href="/2020/04/21/Computer-Vision-Fundation-Image-Processing2-Geometric-Transformation/"/>
    <url>/2020/04/21/Computer-Vision-Fundation-Image-Processing2-Geometric-Transformation/</url>
    
    <content type="html"><![CDATA[<p>In this post, another basic image operation is explored - Transformation.</p><p>This post is split into four sections:</p><ol><li>Mathematical and Computational Principle of Transformations</li><li>Practice with OpenCV in Python</li><li>Build <em>Translation</em> and <em>Rotation</em> Function From Scratch</li><li>(Option) Other Transformation Functions</li></ol><p>Source code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; transform_image.py (OpenCV)</p><hr><h1 id="Principle-of-Transformation"><a href="#Principle-of-Transformation" class="headerlink" title="Principle of Transformation"></a>Principle of Transformation</h1><h3 id="Mathematics"><a href="#Mathematics" class="headerlink" title="Mathematics"></a>Mathematics</h3><p>In general, a transformation of a point \(\mathbf{p}\), \(\begin{pmatrix} v &amp; w \end{pmatrix}\) to another point \(\mathbf{q}\), \(\begin{pmatrix} x &amp; y \end{pmatrix}\) is given by<br>\[\mathbf{q} = \mathbf{R}\mathbf{p} + \mathbf{T}.\]<br>Explicitly,<br>\[<br>\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} \omega_1 &amp; \omega_2 \\ \omega_3 &amp; \omega_4 \end{bmatrix} \begin{bmatrix} v \\ w \end{bmatrix} + \begin{bmatrix} t_1 \\ t_2 \end{bmatrix}.<br>\]<br>where the \(\mathbf{R}\) and \(\mathbf{T}\) denote rotation and translation matrices respectively.</p><p>To make the expression more compact, we can write it as<br>\[<br>\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} \omega_1 &amp; \omega_2 &amp; t_1\\ \omega_3 &amp; \omega_4 &amp; t_2 \end{bmatrix} \begin{bmatrix} v \\ w \\ 1 \end{bmatrix}.<br>\]</p><p>A quick method to determine the transformation matrix is to consider the changes of unit vectors \(\hat{i}\) and \(\hat{j}\).</p><p>For example, if the \(\mathbf{R}\) is an identity matrix and \(\mathbf{T} = \mathbf{0}\) (i.e. no transformation), then the equation is<br>\[<br>\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} \begin{bmatrix} v \\ w \end{bmatrix}.<br>\]<br>Another way to look at this equation is<br>\[<br>\begin{bmatrix} x \\ y \end{bmatrix} = v \begin{bmatrix} 1 \\ 0 \end{bmatrix} + w \begin{bmatrix} 0 \\ 1 \end{bmatrix},<br>\]<br>which means that \(v\) and \(w\) are values to scale up the unit vectors \(\hat{i}\) and \(\hat{j}\). Therefore, any transformation of the vector \(\begin{bmatrix} v \\ w \end{bmatrix}\) can be regards as the transformation of the entire vector space formed by \(\hat{i}\) and \(\hat{j}\).</p><p><img src="rotation.png" srcset="/img/loading.gif" alt="rotation"><br>Take rotation as an example, if \(\hat{i}\) and \(\hat{j}\) are rotated anti-clockwise by an angle \(\theta\), then \(\hat{i}\) becomes \(\begin{bmatrix} cos(\theta) \\ sin(\theta) \end{bmatrix}\) and \(\hat{j}\) becomes \(\begin{bmatrix} -sin(\theta) \\ cos(\theta) \end{bmatrix}\). Thus, the equation is<br>\[<br>\begin{bmatrix} x \\ y \end{bmatrix} = v \begin{bmatrix} cos(\theta) \\ sin(\theta) \end{bmatrix} + w \begin{bmatrix} -sin(\theta) \\ cos(\theta) \end{bmatrix}.<br>\]<br>Therefore, in the general equation, we should use the matrix<br>\[ \begin{bmatrix} \cos(\theta) &amp; -sin(\theta) &amp; 0\\ sin(\theta) &amp; cos(\theta) &amp; 0 \end{bmatrix}. \]<br>Other transformation matrix can be obtained in the same manner.</p><h3 id="Computation"><a href="#Computation" class="headerlink" title="Computation"></a>Computation</h3><p>Except the expression in <strong>Mathematics</strong>, sometimes square matrix can be used.<br>\[<br>\begin{bmatrix} x \\ y \\ 1\end{bmatrix} = \begin{bmatrix} \omega_1 &amp; \omega_2 &amp; t_1\\ \omega_3 &amp; \omega_4 &amp; t_2 \\ 0 &amp; 0 &amp; 1\end{bmatrix} \begin{bmatrix} v \\ w \\ 1\end{bmatrix},<br>\]<br>or if row vectors are used<br>\[<br>\begin{bmatrix} x &amp; y &amp; 1\end{bmatrix} = \begin{bmatrix} v &amp; w &amp; 1\end{bmatrix} \begin{bmatrix} \omega_1 &amp; \omega_3 &amp; 0\\ \omega_2 &amp; \omega_4 &amp; 0 \\ t_1 &amp; t_2 &amp; 1\end{bmatrix}.<br>\]<br>The matrix should be obtained using the same manner discussed in last section.</p><p>One issue to consider when processing images in computer is that the origin of the image does not locate at the centre of the image, but at the top-left corner. It works for translation and resizing. But converting to Cartesian coordinates is necessary before rotation or shearing.<br><img src="coordinate.png" srcset="/img/loading.gif" alt="coordinate"></p><p>Therefore, rotation or shearing requires three steps:</p><ol><li>Convert from image coordinates to Cartesian coordinates</li><li>Perform rotation or shearing</li><li>Convert the results back to image coordinates</li></ol><p>It is not hard to realise that for a \((M \times N)\) matrix, the origin of the image space locates at \(-\frac{N}{2}, \frac{M}{2}\). Thus, the transformation can be calculated from<br>\[<br>\begin{bmatrix} x &amp; y &amp; 1\end{bmatrix} = \begin{bmatrix} v &amp; w &amp; 1 \end{bmatrix} \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 0 \\ -0.5N &amp; 0.5M &amp; 1 \end{bmatrix} \begin{bmatrix} \mathbf{R}^T &amp; \mathbf{0} \\ \mathbf{T}^T &amp; 1 \end{bmatrix} \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 0 \\ 0.5N &amp; 0.5M &amp; 1 \end{bmatrix}.<br>\]</p><h4 id="Aside"><a href="#Aside" class="headerlink" title="Aside"></a>Aside</h4><p>One technical nuance that we need to consider is whether forward or backward mapping should be used after transformation. Intuitively, forward mapping is the procedure to obtain the transformed images. However, couple of problems can occur.<br><img src="forward_mapping.png" srcset="/img/loading.gif" alt="forward"></p><ol><li>It can result pixels outside the image boundary.</li><li>Complex transforms can map several input pixels to the same output pixel.</li><li>The pixel value of the output pixel cannot be obtained simply. As shown in the diagram, the pixel value of the mapped pixel should be interpolated from nearby pixels. Thus, all the mapped pixels have to be computed first.</li></ol><p>Backward mapping, on the other side, find the corresponding point of the mapped pixel on the original image.<br><img src="backward_mapping.png" srcset="/img/loading.gif" alt="backward"><br>Then, the pixel value can be calculated using the nearby pixels on the original image. Therefore, it costs less computationally.</p><p>As we need to interpolate the pixel values, An interpolation mechanism needs selecting. Normally, bilinear interpolation is preferred. OpenCV uses bilinear interpolation as well.</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><h4 id="General-Transformation-Method-Affine-Transformation"><a href="#General-Transformation-Method-Affine-Transformation" class="headerlink" title="General Transformation Method (Affine Transformation)"></a>General Transformation Method (Affine Transformation)</h4><pre><code>dst = cv2.warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]])</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image.<br><strong>dst</strong> -&gt; Destination image.<br><strong>M</strong> -&gt; (compulsory) \((2 \times 3)\) transformation matrix.<br><strong>dsize</strong> -&gt; (compulsory) Destination image size.<br><strong>flages</strong> -&gt; (optional) Combination of interpolation methods (Conduct the last post <em>Resize</em>). <strong>INTER_LINEAR</strong> is used by <em>default</em><br><em>-Addition-</em><br><strong>borderMode</strong> and <strong>borderValue</strong> take care of the border of the destination image. For example, if <strong>borderMode=BORDER_TRANSPARENT</strong>, then the pixels in the destination image corresponding to the “outliers” in the source image are not modified by the function.</p><p><em>Note</em> Unlike what I stated in <strong>Computation</strong>, <strong>M</strong> in this function is a \((2 \times 3)\) matrix as stated in <strong>Mathematics</strong>.</p><h4 id="Rotation-Matrix"><a href="#Rotation-Matrix" class="headerlink" title="Rotation Matrix"></a>Rotation Matrix</h4><p>As mentioned in <strong>Computation</strong>, for rotation and shearing, we need to convert the coordinate system first. In OpenCV, for rotation, this can easily be done using <code>getRotationMatrix2D</code>.</p><pre><code>R = cv2.getRotationMatrix2D(center, angle, scale)</code></pre><p><strong>center</strong> -&gt; (compulsory) Center of the rotation in the source image.<br><strong>angle</strong> -&gt; (compulsory) Rotation angle in <em>degrees</em>. Positive values mean anti-clockwise rotation (the coordinate origin is assumed to be the top-left corner).<br><strong>scale</strong> -&gt; (compulsory) Isotropic scale factor.<br>This will give the rotation matrix \(\mathbf{R}\), which can then be used in <code>warpAffine</code>. As shown in <strong>Computation</strong>, to convert the coordinate system to Cartesian, the <code>center=(cols/2, rows/2)</code>, where <code>cols</code> and <code>rows</code> can be obtained from <code>image.shape</code></p><h4 id="Shearing-Matrix"><a href="#Shearing-Matrix" class="headerlink" title="Shearing Matrix"></a>Shearing Matrix</h4><p>To implement the shearing operation, we have to construct the matrix ourselves with the procedure stated in <strong>Computation</strong>. Notice that truncated and transverse matrix should be constructed to match OpenCV convention. Details in example code.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><pre><code>import numpy as npimport cv2name = image_nameimg = cv2.imread(&#39;./{}.jpg&#39;.format(name), cv2.IMREAD_UNCHANGED)rows, cols, _ = img.shape# TranslationT = np.array(([1, 0, 100], [0, 1, 50]), dtype=np.float32)# RotationR = cv2.getRotationMatrix2D((cols/2, rows/2), 45, 1)# Shearingconvert = np.array(([1, 0, 0], [0, -1, 0], [cols/2, rows/2, 1]), dtype=np.float32)inverse = np.array(([1, 0, 0], [0, -1, 0], [-cols/2, rows/2, 1]), dtype=np.float32)S = np.array(([1, 0, 0], [0.5, 1, 0], [0, 0, 1]), dtype=np.float32)  # unit vector (0, 1) to (0.5, 1), shear to x directionS = inverse@S@convert# To match OpenCV conventionS = np.transpose(S[:, :-1])translated = cv2.warpAffine(img, T, (cols, rows))rotated = cv2.warpAffine(img, R, (cols, rows))sheared = cv2.warpAffine(img, S, (cols, rows))cv2.imshow(&quot;Translated image&quot;, translated)cv2.imshow(&quot;Rotated image&quot;, rotated)cv2.imshow(&quot;Sheared image&quot;, sheared)# Press &#39;s&#39; for saving the imagek = cv2.waitKey(0)if k == 27:         # wait for ESC key to exit    cv2.destroyAllWindows()elif k == ord(&#39;s&#39;): # wait for &#39;s&#39; key to save and exit    cv2.imwrite(&quot;Translated.jpg&quot;,translated)    cv2.imwrite(&quot;Rotated.jpg&quot;,rotated)    cv2.imwrite(&quot;Sheared.jpg&quot;,sheared)    cv2.destroyAllWindows()</code></pre><p>Example results are shown below:<br><strong>The original image</strong> (From last post)<br><img src="flower_Resized_image.jpg" srcset="/img/loading.gif" alt="small"></p><p><strong>Translation</strong><br><img src="flower_Translated.jpg" srcset="/img/loading.gif" alt="t"></p><p><strong>Rotation</strong><br><img src="flower_Rotated.jpg" srcset="/img/loading.gif" alt="r"></p><p><strong>Shearing</strong><br><img src="flower_Sheared.jpg" srcset="/img/loading.gif" alt="s"></p><hr><h1 id="Build-Each-Transformation-Function-From-Scratch-TODO"><a href="#Build-Each-Transformation-Function-From-Scratch-TODO" class="headerlink" title="Build Each Transformation Function From Scratch (TODO)"></a>Build Each Transformation Function From Scratch (TODO)</h1><p>Shearing function has already been built in the last section.</p><hr><h1 id="Option-Other-Transformation-Functions-TODO"><a href="#Option-Other-Transformation-Functions-TODO" class="headerlink" title="(Option) Other Transformation Functions (TODO)"></a>(Option) Other Transformation Functions (TODO)</h1><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Reed, N., 2020. Rotations And Infinitesimal Generators – Nathan Reed’S Coding Blog. [online] Reedbeta.com. Available at: <a href="http://reedbeta.com/blog/rotations-and-infinitesimal-generators/" target="_blank" rel="noopener">http://reedbeta.com/blog/rotations-and-infinitesimal-generators/</a> [Accessed 22 April 2020].</li><li>Lohninger, H., 2020. Java Programming Course - Coordinates. [online] Vias.org. Available at: <a href="http://www.vias.org/javacourse/chap04_10.html" target="_blank" rel="noopener">http://www.vias.org/javacourse/chap04_10.html</a> [Accessed 22 April 2020].</li><li>Blog.csdn.net. 2020. 图像变换——向前映射和向后映射_人工智能_薇洛的打火机-CSDN博客. [online] Available at: <a href="https://blog.csdn.net/glorydream2015/article/details/44873703" target="_blank" rel="noopener">https://blog.csdn.net/glorydream2015/article/details/44873703</a> [Accessed 22 April 2020].</li><li>Engr.case.edu. 2020. [online] Available at: <a href="http://engr.case.edu/merat_francis/eecs490f07/lectures/lecture4.pdf" target="_blank" rel="noopener">http://engr.case.edu/merat_francis/eecs490f07/lectures/lecture4.pdf</a> [Accessed 22 April 2020].</li><li>Docs.opencv.org. 2020. Geometric Image Transformations — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html?highlight=warpaffine" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html?highlight=warpaffine</a> [Accessed 22 April 2020].</li><li>Photo by Viktor Mogilat on Unsplash.</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation -&gt; Image Processing1: Digital Image Interpolation</title>
    <link href="/2020/04/19/Computer-Vision-Fundation-Image-Processing1-Digital-Image-Interpolation/"/>
    <url>/2020/04/19/Computer-Vision-Fundation-Image-Processing1-Digital-Image-Interpolation/</url>
    
    <content type="html"><![CDATA[<p>To fully understand computer vision, learning image processing is inevitable. One basic operation is to resize the images. When enlarging a small image, the resulted image may have jagged pixel edges. Image interpolation is to add a few pixels and manipulate their value in purpose so that the resulted image looks smoother.</p><p>This post is split into four sections:</p><ol><li>Two Basic Image Interpolation Algorithm</li><li>Practice with OpenCV in Python</li><li>Build <em>Resize</em> Function From Scratch</li><li>(Option) Other Interpolation Algorithm</li></ol><p>OpenCV code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; resize_image.py<br>Re-implementation code: <a href="https://github.com/BillMaZengou/cv_basis/my_opencv" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis/my_opencv</a> -&gt; resize.py</p><hr><h1 id="Two-Basic-Image-Interpolation-Algorithm"><a href="#Two-Basic-Image-Interpolation-Algorithm" class="headerlink" title="Two Basic Image Interpolation Algorithm"></a>Two Basic Image Interpolation Algorithm</h1><h3 id="Nearest-neighbour-interpolation"><a href="#Nearest-neighbour-interpolation" class="headerlink" title="Nearest-neighbour interpolation"></a>Nearest-neighbour interpolation</h3><p>When the image size increases, the extra pixels will use the same value as their nearest neighbour.<br><img src="/images/Nearest1.png" srcset="/img/loading.gif" alt="Nearest"><br>This is the simplest method. However, it is not hard to realise that this approach is problematic as it preserves the same resolution as the small image.</p><h3 id="Bilinear-interpolation"><a href="#Bilinear-interpolation" class="headerlink" title="Bilinear interpolation"></a>Bilinear interpolation</h3><p>Bilinear interpolation tries to estimate the new pixel values using information from their neighbours. Normally, linear relations are assumed in both \(x\) and \(y\) directions, namely bilinear relation. Hopefully, it will give a smoother enlarged image than <strong>Nearest-neighbour interpolation</strong>.</p><p>Typical derivation follows the logic that the enlargement should have bilinear relation and it should work to give a smoother image. Here, I decided to follow the opposite route. By assuming the resulted image is continuous and smooth, we can obtain the <strong>Bilinear interpolation</strong> without assuming bilinear relation at the first place. The full derivation will be shown in <em>Appendix</em>. Two routes are essentially equivalent.<br><img src="/images/Bilinear.png" srcset="/img/loading.gif" alt="Bilinear"><br>The equation is<br>\[<br>  f(x, y) = \frac{(x_2 - x)(y_2-y)}{(x_2 - x_1)(y_2-y_1)}f(Q_{11}) + \frac{(x - x_1)(y_2-y)}{(x_2 - x_1)(y_2-y_1)}f(Q_{21}) + \frac{(x_2 - x)(y-y_1)}{(x_2 - x_1)(y_2-y_1)}f(Q_{12}) + \frac{(x - x_1)(y-y_1)}{(x_2 - x_1)(y_2-y_1)}f(Q_{22}).<br>\]<br>Or in matrix form,<br>$$<br>  f(x, y) = \frac{1}{(x_2 - x_1)(y_2-y_1)} \begin{bmatrix} (x_2-x) &amp; (x-x_1) \end{bmatrix} \begin{bmatrix} f(Q_{11}) &amp; f(Q_{12}) \\ f(Q_{21}) &amp; f(Q_{22}) \end{bmatrix} \begin{bmatrix} (y_2-y) \\ (y-y_1) \end{bmatrix}.<br>$$</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><p>Despite the maths looks complicated, the implementation is straightforward using OpenCV and Python.</p><pre><code>dst = cv2.resize(src, dsize[, dst[, fx[, fy[, interpolation]]]])</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image.<br><strong>dst</strong> -&gt; Destination image.<br><strong>dsize</strong> -&gt; (compulsory) Destination image size. If it is \(0\), it is computed as:</p><pre><code>  dsize = Size(round(fx*src.cols), round(fy*src.rows))</code></pre><p><strong>fx</strong> -&gt; (optional) Scale factor along the horizontal axis. When it is \(0\), it is computed as:</p><pre><code>  (double)dsize.width/src.cols</code></pre><p><strong>fy</strong> -&gt; (optional) Scale factor along the vertical axis. When it is \(0\), it is computed as:</p><pre><code>  (double)dsize.height/src.rows</code></pre><p><strong>interpolation</strong> -&gt; (optional) Interpolation method:<br>  <strong>INTER_NEAREST</strong> - a nearest-neighbour interpolation<br>  <strong>INTER_LINEAR</strong> - a bilinear interpolation (used by <em>default</em>)<br>  <em>-Addition-</em><br>  <strong>INTER_AREA</strong> - resampling using pixel area relation. It may be a preferred method for image decimation, as it gives moire’-free results. But when the image is zoomed, it is similar to the <strong>INTER_NEAREST</strong> method.<br>  <strong>INTER_CUBIC</strong> - a bicubic interpolation over 4x4 pixel neighbourhood<br>  <strong>INTER_LANCZOS4</strong> - a Lanczos interpolation over 8x8 pixel neighbourhood</p><p><em>Note</em> that, normally, <strong>INTER_AREA</strong> is used when scaling down the image. Otherwise, <strong>INTER_CUBIC</strong> and <strong>INTER_LINEAR</strong> are good choices. But <strong>INTER_CUBIC</strong> is a bit slow.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><pre><code>import cv2img = cv2.imread(image_name, cv2.IMREAD_UNCHANGED)  # replace with your image_namescale_percent = 5       # percent of original sizewidth = int(img.shape[1] * scale_percent / 100)height = int(img.shape[0] * scale_percent / 100)dim = (width, height)# scale down the imageresized = cv2.resize(img, dim, interpolation = cv2.INTER_LINEAR)fx = 3fy = 3# scale up the resized imageresized1 = cv2.resize(resized, dsize=None, fx=fx, fy=fy, interpolation = cv2.INTER_NEAREST)resized2 = cv2.resize(resized, dsize=None, fx=fx, fy=fy, interpolation = cv2.INTER_LINEAR)# display the resultscv2.imshow(&quot;Resized image&quot;, resized)cv2.imshow(&quot;INTER_NEAREST image&quot;, resized1)cv2.imshow(&quot;INTER_LINEAR image&quot;, resized2)# Press &#39;s&#39; for saving the imagek = cv2.waitKey(0)if k == 27:         # wait for ESC key to exit    cv2.destroyAllWindows()elif k == ord(&#39;s&#39;): # wait for &#39;s&#39; key to save and exit    cv2.imwrite(&quot;Resized_image.jpg&quot;,resized)    cv2.imwrite(&quot;INTER_NEAREST_image.jpg&quot;,resized1)    cv2.imwrite(&quot;INTER_LINEAR_image.jpg&quot;,resized2)    cv2.destroyAllWindows()</code></pre><p>Example results are shown below:<br><strong>Scale down the original image by 95% using Bilinear interpolation</strong><br><img src="/images/Resized_image.jpg" srcset="/img/loading.gif" alt="small"></p><p><strong>Scale up the resized image by 30% using Nearest-neighbour interpolation</strong><br><img src="/images/INTER_NEAREST_image.jpg" srcset="/img/loading.gif" alt="nearest_image"></p><p><strong>Scale up the resized image by 30% using Bilinear interpolation</strong><br><img src="/images/INTER_LINEAR_image.jpg" srcset="/img/loading.gif" alt="bilinear_image"><br>It is clear that the result of the Bilinear interpolation is much smoother than it of the Nearest-neighbour interpolation.</p><hr><h1 id="Build-Resize-Function-From-Scratch"><a href="#Build-Resize-Function-From-Scratch" class="headerlink" title="Build Resize Function From Scratch"></a>Build <em>Resize</em> Function From Scratch</h1><p>In this section, I will implement <em>Resize</em> function without using OpenCV.</p><h3 id="Nearest-neighbour-interpolation-1"><a href="#Nearest-neighbour-interpolation-1" class="headerlink" title="Nearest-neighbour interpolation"></a>Nearest-neighbour interpolation</h3><p>To simply the problem, currently only focus on greyscale images. More details about  greyscale images, check <strong>Colour Conversion</strong> (<a href="https://billmazengou.github.io/2020/04/24/Computer-Vision-Foundation-Image-Processing3-Colour-Conversion/" target="_blank" rel="noopener">https://billmazengou.github.io/2020/04/24/Computer-Vision-Foundation-Image-Processing3-Colour-Conversion/</a>)</p><pre><code>def scale(img, x_fac, y_fac):    x, y = img.shape    x_new = int(x * x_fac)    y_new = int(y * y_fac)    scaled_img = np.zeros((x_new, y_new))    for i in range(x_new):        for j in range(y_new):            scaled_img[i, j] = img[int(i//x_fac), int(j//y_fac)]    return scaled_img</code></pre><p>The results shown as follow.</p><p><strong>manually Created Small Image</strong><br><img src="/images/temp1.jpg" srcset="/img/loading.gif" alt="small"></p><p><strong>After scaling up by 1.5</strong><br><img src="/images/temp2.jpg" srcset="/img/loading.gif" alt="large"></p><p>The image can be easily created using</p><pre><code># Create an imagesize = 50img_t = np.zeros((size, size))for i in range(size):    if i &gt; size/2:        for j in range(size):            if j &gt; size/2:                img_t[i, j] = 255.0    else:        for j in range(size):            if j &lt; size/2:                img_t[i, j] = 255.0</code></pre><p>You can play with the <strong>size</strong></p><p>Test with other images.<br><strong>Original Greyscale Image</strong><br><img src="/images/temp_g.jpg" srcset="/img/loading.gif" alt="g"></p><p><strong>After scaling up by 1.5</strong><br><img src="/images/temp_c.jpg" srcset="/img/loading.gif" alt="c"></p><h3 id="Bilinear-interpolation-TODO"><a href="#Bilinear-interpolation-TODO" class="headerlink" title="Bilinear interpolation (TODO)"></a>Bilinear interpolation (TODO)</h3><hr><h1 id="Option-Other-Interpolation-Algorithm-TODO"><a href="#Option-Other-Interpolation-Algorithm-TODO" class="headerlink" title="(Option) Other Interpolation Algorithm (TODO)"></a>(Option) Other Interpolation Algorithm (TODO)</h1><hr><h1 id="Appendix-Derivation-of-Bilinear-interpolation"><a href="#Appendix-Derivation-of-Bilinear-interpolation" class="headerlink" title="Appendix - Derivation of Bilinear interpolation"></a>Appendix - Derivation of Bilinear interpolation</h1><p><img src="/images/Bilinear.png" srcset="/img/loading.gif" alt="Bilinear"><br>Assumptions: The image is smooth in any directions; \(Q_{11}\), \(Q_{12}\), \(Q_{21}\), \(Q_{22}\), \(R_1\), \(R_2\) are all close enough to point \(p\), \((x, y)\).</p><p>Follow by the assumptions, we have<br>\[<br>f(R_1) = f(x, y) - \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y-y_1);<br>f(R_2) = f(x, y) + \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y_2-y).<br>\]<br>Then,<br>\[<br>f(Q_{11}) = f(R_1) - \frac{\partial f(R_1)}{\partial x}\Bigr\rvert_{y_1} (x-x_1); f(Q_{21}) = f(R_1) + \frac{\partial f(R_1)}{\partial x}\Bigr\rvert_{y_1} (x_2-x);<br>\]<br>\[<br>f(Q_{12}) = f(R_2) - \frac{\partial f(R_2)}{\partial x}\Bigr\rvert_{y_2} (x-x_1); f(Q_{22}) = f(R_2) + \frac{\partial f(R_2)}{\partial x}\Bigr\rvert_{y_2} (x_2-x).<br>\]<br>Substituting \(f(R_1)\) and \(f(R_2)\) into the equations, we find that<br>\[<br>f(Q_{11}) = f(x, y) - \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y-y_1) - \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_1} (x-x_1) + \frac{\partial^2 f(x, y)}{\partial x \partial y}\Bigr\rvert_{y_1} (x-x_1) (y-y_1),<br>\]<br>the last term equals \(0\) as the function is evaluated at \(y=y_1\). Therefore, we have<br>\[<br>f(Q_{11}) = f(x, y) - \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y-y_1) - \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_1} (x-x_1);<br>\]<br>\[<br>f(Q_{21}) = f(x, y) - \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y-y_1) + \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_1} (x_2-x);<br>\]<br>\[<br>f(Q_{12}) = f(x, y) + \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y_2-y) - \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_2} (x-x_1);<br>\]<br>\[<br>f(Q_{22}) = f(x, y) + \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y_2-y) + \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_2} (x_2-x).<br>\]</p><p>By rearranging the equations, we can find that<br>\[<br>\frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y-y_1) = f(x, y) - f(Q_{11}) - \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_1} (x-x_1) = f(x, y) - f(Q_{21}) + \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_1} (x_2-x);<br>\]<br>\[<br>\frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x}(y_2-y) = f(Q_{12}) - f(x, y) +  \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_2} (x-x_1) = f(Q_{22}) - f(x, y) - \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_2} (x_2-x).<br>\]<br>Then,<br>\[<br>\frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_1} = \frac{f(Q_{21}) - f(Q_{11})}{x_2-x_1};<br>\]<br>\[<br>\frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_2} = \frac{f(Q_{22}) - f(Q_{12})}{x_2-x_1}.<br>\]</p><p>Substituting the above equations back, we can fine \(\frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x}\) as<br>\[<br>\frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} = \frac{f(x, y) - f(Q_{11})}{y-y_1} - \frac{x-x_1}{y-y_1}\frac{f(Q_{21}) - f(Q_{11})}{x_2-x_1} = \frac{f(Q_{12}) - f(x, y)}{y_2-y} - \frac{x-x_1}{y_2-y}\frac{f(Q_{22}) - f(Q_{12})}{x_2-x_1}.<br>\]</p><p>Rearrange to get \(f(x, y)\). Finally, we have the <strong>Bilinear relation</strong> that we want.<br>\[<br>f(x, y) = \frac{(x_2 - x)(y_2-y)}{(x_2 - x_1)(y_2-y_1)}f(Q_{11}) + \frac{(x - x_1)(y_2-y)}{(x_2 - x_1)(y_2-y_1)}f(Q_{21}) + \frac{(x_2 - x)(y-y_1)}{(x_2 - x_1)(y_2-y_1)}f(Q_{12}) + \frac{(x - x_1)(y-y_1)}{(x_2 - x_1)(y_2-y_1)}f(Q_{22}).<br>\]</p><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Blog.csdn.net. 2020. Opencv框架与图像插值算法_网络_Weixin_39940512的博客-CSDN博客. [online] Available at: <a href="https://blog.csdn.net/weixin_39940512/article/details/105343418" target="_blank" rel="noopener">https://blog.csdn.net/weixin_39940512/article/details/105343418</a> [Accessed 20 April 2020].</li><li>Angel, A., 2020. Nearest Neighbor Interpolation. [online] Imageeprocessing.com. Available at: <a href="https://www.imageeprocessing.com/2017/11/nearest-neighbor-interpolation.html" target="_blank" rel="noopener">https://www.imageeprocessing.com/2017/11/nearest-neighbor-interpolation.html</a> [Accessed 19 April 2020].</li><li>En.wikipedia.org. 2020. Bilinear Interpolation. [online] Available at: <a href="https://en.wikipedia.org/wiki/Bilinear_interpolation" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Bilinear_interpolation</a> [Accessed 20 April 2020].</li><li>Opencv.org.cn. 2020. Geometric Image Transformations — Opencv 2.3.2 Documentation. [online] Available at: <a href="http://www.opencv.org.cn/opencvdoc/2.3.2/html/modules/imgproc/doc/geometric_transformations.html?highlight=resize#cv.Resize" target="_blank" rel="noopener">http://www.opencv.org.cn/opencvdoc/2.3.2/html/modules/imgproc/doc/geometric_transformations.html?highlight=resize#cv.Resize</a> [Accessed 20 April 2020].</li><li>Photo by Viktor Mogilat on Unsplash.</li><li>Opencv-python-tutroals.readthedocs.io. 2020. Getting Started With Images — Opencv-Python Tutorials 1 Documentation. [online] Available at: <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_image_display/py_image_display.html" target="_blank" rel="noopener">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_image_display/py_image_display.html</a> [Accessed 20 April 2020].</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Quarantine-Day-2|隔离第二天</title>
    <link href="/2020/04/16/Quarantine-Day-2-%E9%9A%94%E7%A6%BB%E7%AC%AC%E4%BA%8C%E5%A4%A9/"/>
    <url>/2020/04/16/Quarantine-Day-2-%E9%9A%94%E7%A6%BB%E7%AC%AC%E4%BA%8C%E5%A4%A9/</url>
    
    <content type="html"><![CDATA[<p>Today is the second day of my quarantine session. Due to the unespected situation and my original career path, I decided to accelerate my job hunting. But when I begin, I am a little comfused and upset. I understand I need sometime to digest a new way of thinking. I might be immature and naive. I almost spent the whole day in looking for information and filling up different files. </p><p>To summarise some points that I obtained today, I need to firm my essential coding skills. I did not learn programming until last year, but it cannot be my excuse. Also, I need to find out how I can use what I know to make profit. This year, I learned about Computer Vision, Graphics, VR/AR. I enjoyed learning new stuff but worried little about what I can do with these cutting-edge technologies. Also, I need to link these back to the fundamental data sturctures and popular libraries. </p><p>Hope today’s work can help me make my mind!</p>]]></content>
    
    
    
    <tags>
      
      <tag>Diary</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Weight Record</title>
    <link href="/2020/04/15/Weight-Record/"/>
    <url>/2020/04/15/Weight-Record/</url>
    
    <content type="html"><![CDATA[<p><em>Goal</em>: 70kg</p><p>One-punch Man Training:</p><ol><li>Push-ups - 100 times</li><li>Sit-ups - 100 times</li><li>Squats - 100 times</li><li>Jogging - 10km</li></ol><hr><h1 id="Date-2020-04-15"><a href="#Date-2020-04-15" class="headerlink" title="Date: 2020/04/15"></a>Date: 2020/04/15</h1><p>Weight: 77kg<br>Activity: N/A</p><h1 id="Date-2020-04-16"><a href="#Date-2020-04-16" class="headerlink" title="Date: 2020/04/16"></a>Date: 2020/04/16</h1><p>Weight: 77kg<br>Activity: 50% of 1 - 3</p><h1 id="Date-2020-04-17"><a href="#Date-2020-04-17" class="headerlink" title="Date: 2020/04/17"></a>Date: 2020/04/17</h1><p>Weight: 77kg<br>Activity: 50% of 1 - 3</p><h1 id="Date-2020-04-18"><a href="#Date-2020-04-18" class="headerlink" title="Date: 2020/04/18"></a>Date: 2020/04/18</h1><p>Weight: 77kg<br>Activity: 50% of 1 - 3</p><h1 id="Date-2020-04-19"><a href="#Date-2020-04-19" class="headerlink" title="Date: 2020/04/19"></a>Date: 2020/04/19</h1><p>Weight: 77kg<br>Activity: N/A</p><h1 id="Date-2020-04-20"><a href="#Date-2020-04-20" class="headerlink" title="Date: 2020/04/20"></a>Date: 2020/04/20</h1><p>Weight: 77kg<br>Activity: 50% of 1 - 3</p><h1 id="Date-2020-04-21"><a href="#Date-2020-04-21" class="headerlink" title="Date: 2020/04/21"></a>Date: 2020/04/21</h1><p>Weight: 77kg<br>Activity: 50% of 1 - 3</p><h1 id="Date-2020-04-22"><a href="#Date-2020-04-22" class="headerlink" title="Date: 2020/04/22"></a>Date: 2020/04/22</h1><p>Weight: 77kg<br>Activity: 50% of 1 - 3</p><h1 id="Date-2020-04-23"><a href="#Date-2020-04-23" class="headerlink" title="Date: 2020/04/23"></a>Date: 2020/04/23</h1><p>Weight: 76kg<br>Activity: N/A</p><h1 id="Date-2020-04-24"><a href="#Date-2020-04-24" class="headerlink" title="Date: 2020/04/24"></a>Date: 2020/04/24</h1><p>Weight: 76kg<br>Activity: 50% of 1 - 3</p><h1 id="Date-2020-04-25"><a href="#Date-2020-04-25" class="headerlink" title="Date: 2020/04/25"></a>Date: 2020/04/25</h1><p>Weight: 76kg<br>Activity: N/A</p><h1 id="Date-2020-04-25-1"><a href="#Date-2020-04-25-1" class="headerlink" title="Date: 2020/04/25"></a>Date: 2020/04/25</h1><p>Weight: 76kg<br>Activity: N/A</p>]]></content>
    
    
    
    <tags>
      
      <tag>Diary</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Quarantine Day 1|隔离第一天</title>
    <link href="/2020/04/15/Quarantine-Day-1-%E9%9A%94%E7%A6%BB%E7%AC%AC%E4%B8%80%E5%A4%A9/"/>
    <url>/2020/04/15/Quarantine-Day-1-%E9%9A%94%E7%A6%BB%E7%AC%AC%E4%B8%80%E5%A4%A9/</url>
    
    <content type="html"><![CDATA[<p>Today is 2020/04/15. I have arrived in Shanghai and begun my quarantine life. As I have two upcoming deadlines, I will dedicate my time on two topics during the quarantine period. One is <strong>volumetric transform</strong>, the other is <strong>using asymmetric vibration to induce sensation of force</strong>. Because of UCL school policy, I am not allowed to show my coursework in detail. I will only show briefly the literature review and my understandings of the second topics. Later, I will probably show you the results of the first topics. Also, I will make a series of topics in <strong>Computer Vision</strong> when I do my revision during and after the quarantine period.</p><p>One thing I found intriguing in the area of VR was haptics. Nowadays, there are many ways to use human hands to interact with virtual objects. They are easier and more intuitive than the normal VR controllers. The cheapest way is the hand tracking using cameras. For example, Oculus has announced that they will include hand tracking in the latest Oculus Quest; Leap Motion has worked in this field for a decade. Despite, using visual information is cheap, it is not always reliable as the hand position, orientation, light and occlusion conditions are constantly varying. Haptics glove is one alternative. Data-driven gloves can precisely track our hand and fingers. Plus, with the available data, force feedback haptics can be built.</p><p>Personally, I believed that force feedback haptic glove is the future of VR interaction. Users cannot only touch, hold and move virtual objects, but also feel the shape, reaction force, texture or even temperature of the virtual objects. Is not a fascinating idea?</p>]]></content>
    
    
    
    <tags>
      
      <tag>Diary, Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Create A Blog|创建博客</title>
    <link href="/2020/04/13/Create-A-Blog-%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/"/>
    <url>/2020/04/13/Create-A-Blog-%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<p>Today is 2020/04/13, I will be on the plane to China in about 10 hours. I heard about how writing blog would help improve IT skills for a long time, but did not know when and how to start. Now I finally made my mind to create a blog. For thhe processes, I conducted “<a href="https://www.bilibili.com/video/BV1Yb411a7ty?from=search&amp;seid=13126380643235482353&quot;" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Yb411a7ty?from=search&amp;seid=13126380643235482353&quot;</a>. CodeSheep gave a great and detailed introduction to the Hexo blog framework. As I succeed, I decided to write it down which may make your creation easier and faster.</p><p>I used MacOs, so if you encounter any difficulies, google it.</p><p>This article is served as a record for myself. All the credits should be given to CodeSheep. (<a href="https://www.codesheep.cn/" target="_blank" rel="noopener">https://www.codesheep.cn/</a>)</p><h2 id="Step-1-Download-Dependencies"><a href="#Step-1-Download-Dependencies" class="headerlink" title="Step 1: Download Dependencies"></a>Step 1: Download Dependencies</h2><p>nodejs: <a href="https://nodejs.org/en/" target="_blank" rel="noopener">https://nodejs.org/en/</a><br>git: <a href="https://git-scm.com/downloads" target="_blank" rel="noopener">https://git-scm.com/downloads</a></p><p>Open Terminal and switch to root user</p><pre><code>sudo su</code></pre><p>put your password in.</p><p>Use</p><pre><code>node -vnpm -vgit -v</code></pre><p>to confirm you successfully download node.js and Git.</p><h2 id="Addition-Step"><a href="#Addition-Step" class="headerlink" title="Addition Step"></a>Addition Step</h2><p>If you are in China, ‘cnpm’ should be faster than ‘npm’.</p><p>Install cnpm and registry to TaoBao.</p><pre><code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code></pre><p>Then, in the following context, replace ‘npm’ with ‘cnpm’</p><hr><h2 id="Step-2-Download-Hexo-Framework"><a href="#Step-2-Download-Hexo-Framework" class="headerlink" title="Step 2: Download Hexo Framework"></a>Step 2: Download Hexo Framework</h2><pre><code>npm install -g hexo-cli</code></pre><p>After dowaload, you can use</p><pre><code>hexo -v</code></pre><p>to confirm.</p><hr><h2 id="Step-3-Create-a-blog"><a href="#Step-3-Create-a-blog" class="headerlink" title="Step 3: Create a blog"></a>Step 3: Create a blog</h2><p>Use</p><pre><code>mkdir blog</code></pre><p>to create a directory for you to write your blog.</p><p>In that directory, use</p><pre><code>sudo hexo init</code></pre><p>to create the blog.</p><p>Congratulations! Your blog has been created. Then you can use</p><pre><code>hexo s</code></pre><p>to start running the hexo server. By default, you should be able to access your blog locally with “<a href="http://localhost:4000/&quot;" target="_blank" rel="noopener">http://localhost:4000/&quot;</a></p><p>Basic hexo instructions can be found in your blog when you first create it. Here are some.</p><pre><code>hexo n &quot;PUT THE NAME OF YOUR POST HERE&quot;  # to creat a new posthexo clean  # to clean the bloghexo g  # to generate the blog with your posthexo s  # to run your blog locallyhexo d  # to deploy your blog</code></pre><p>The blog posts will be in MarkDown format.</p><hr><h2 id="Step-4-Deploy-your-Blog"><a href="#Step-4-Deploy-your-Blog" class="headerlink" title="Step 4: Deploy your Blog"></a>Step 4: Deploy your Blog</h2><p>To run your blog remotely, you have to deploy your blog. Here, we will deploy it on your github.</p><p>Create a new repository on Github. The Repository name has to be “YourID.github.io”!!</p><p>Use</p><pre><code>npm install --save hexo-deployer-git</code></pre><p>to download a plugin needed.</p><p>After download, modify “_config.yml” file, “Deployment” section to</p><pre><code>deploy:  type: git  repo: https://github.com/YourID/YourID.github.io.git  branch: master  # set the default branch.</code></pre><p>The ‘branch’ is not necessary unless you want to use an alternative branch of your git project.</p><p>Use the instuction</p><pre><code>hexo d</code></pre><p>to deploy the blog.</p><p>It may ask you to fill in your github ID and password. Then use “<a href="https://YourID.github.io&quot;" target="_blank" rel="noopener">https://YourID.github.io&quot;</a>, you should be able to access your blog remotely.</p><hr><h2 id="Step-5-Change-the-Theme"><a href="#Step-5-Change-the-Theme" class="headerlink" title="Step 5: Change the Theme"></a>Step 5: Change the Theme</h2><p>First, you can find a hexo theme that you like, just google it.</p><p>Here I will use mine as an example. I used hexo-theme-fluid. More details are in the github “<a href="https://github.com/fluid-dev/hexo-theme-fluid&quot;" target="_blank" rel="noopener">https://github.com/fluid-dev/hexo-theme-fluid&quot;</a>.</p><p>To use the theme, you should first download it or simply use</p><pre><code>&quot;&quot;&quot;Change the link to your selected one, and the &#39;fluid&#39; to the name you like.Better to use the theme name&quot;&quot;&quot;git clone https://github.com/fluid-dev/hexo-theme-fluid themes/fluid</code></pre><p>modify “_config.yml”, “Extensions” section. Change the theme name to your file name. In my case, it was</p><pre><code>theme: fluid</code></pre><p>Clean, generate and deploy it again. Then, the blog becomes the way you see now.</p><p>That is all. I am glad if you also manage to create one. Later, I will put more stuff on this website. May be my studies, my interests or even diaries. Hopefully, they will be helpful for you.</p><p><em>Thank you! Wish you a great day!</em></p>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
