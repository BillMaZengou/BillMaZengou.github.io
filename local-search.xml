<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Natural Language Processing4: Contextual Word Embeddings</title>
    <link href="/2020/07/03/NLP4-Contextual-Word-Embeddings/"/>
    <url>/2020/07/03/NLP4-Contextual-Word-Embeddings/</url>
    
    <content type="html"><![CDATA[<p>In previous posts, we discussed the basic vectorisations of words. <strong>One-Hot</strong> considers each words in the context is orthogonal to each other. <strong>Word2vec</strong> provides us a distributive way to present words as probabilities of showing up given the neighbouring words. <strong>\(n\)-grams</strong> as a character-based method solves some problems about the word-based method.</p><p>For a versatile NLP application, we would like to have a pre-trained word vectors so that we can fine tune for some specific application such as QA or translation. But use a pre-trained model, we will inevitably encounter unknown words which are not in the pre-trained model. Usually, we will let the machine to output <code>&lt;UNK&gt;</code> for unknown words. The solutions to the unknown words problem can be to develop character-based word vectors or partially solve it during the fine-tune stage.</p><p>Another problem is that we just have one representation for a word (i.e.one word vector for one word), but words have different <strong>aspects</strong>, including semantics, syntactic behaviour, and register/connotations.</p><h1 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h1><p>In the old NLP models, those language models are producing context-specific word representations at each position.</p><p>Later approach called <strong>TagLM</strong> in 2017 provides a direction of solution. Do semi-supervised approach where we train NLM on large unlabelled corpus, rather than just word vectors.<br><img src="taglm.png" srcset="/img/loading.gif" alt="taglm"></p><p>The breakout version of word token vectors or contextual word vectors emerged in 2018, called <strong>ELMo: Embeddings from Language<br>Models</strong>. It learns a deep Bi-NLM and uses all its layers in prediction. ELMo ends up performing slightly better than TagLM in the task of CoNLL 2003 Named Entity Recognition. However, results from many tests showed that ELMo gives a better performance for all tasks.</p><p>For ELMo, the two biLSTM NLM layers have differentiated uses or meanings.<br>-&gt; Lower layer is better for lower-level syntax, etc. (Part-of-speech tagging, syntactic dependencies, NER)<br>-&gt; Higher layer is better for higher-level semantics. (Sentiment, Semantic role labelling, question answering, SNLI)</p><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>From 2018 onwards, many models used Transformer architecture to gain better performance.</p><p>The motivation to use the Transformer is that we want parallelisation but RNNs are inherently sequential. Moreover, despite GRUs and LSTMs, RNNs still need attention mechanism to deal with long range dependencies – path length between states grows with sequence otherwise. But if attention gives us access to any state, probably we only need attention.</p><p>The architecture is like<br><img src="transformer.png" srcset="/img/loading.gif" alt="transformer"></p><p>There are two types of attention function. The one used in the Transformer is so-called <strong>Dot-Product Attention</strong>. Mathematically,<br>\[<br>  A(q, K, V) = \sum_i \frac{exp(q \dot k_i)}{\sum_j exp(q \dot k_j)} v_i<br>\]<br>where \(q\) is a query, and \((k_i-v_i)\) denotes the key-value pairs.</p><p>For multiple queries, \(Q\), and with normalisation, we have<br>\[<br>  A(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V<br>\]</p><p>Another improvement is to use multi-head attention such that<br>\[<br>  MultiHead(Q, K, V) = Concat(head_1, …, head_h)W^O<br>\]<br>where<br>\[<br>  head_i = A(QW_i^Q, KW_i^K, VW_i^V)<br>\]</p><h1 id="BERT-TODO"><a href="#BERT-TODO" class="headerlink" title="BERT (TODO)"></a>BERT (TODO)</h1><p>With the improvement made by Transformer, a new model was proposed by Google AI. That is <strong>BERT</strong>, which stands for <strong>Bidirectional Encoder Representations from Transformers</strong>. It is a pre-training of deep bidirectional Transformers for language understanding.</p><!-- However, until now, we have not discussed a way to contextually embed the words. --><!-- Source code: https://github.com/BillMaZengou/nlp_basis -> WordVector.ipynb --><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>. The learning path is based on <strong>Stanford University CS224n: Natural Language Processing with Deep Learning</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Bilibili.com. 2020. [online] Available at: <a href="https://www.bilibili.com/video/BV1s4411N7fC?p=13" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1s4411N7fC?p=13</a> [Accessed 3 July 2020].</li><li>Nlp.seas.harvard.edu. 2020. The Annotated Transformer. [online] Available at: <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a> [Accessed 3 July 2020].</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation 3: Haar-like Feature</title>
    <link href="/2020/07/02/Computer-Vision-Foundation3-Haar-Like-Feature/"/>
    <url>/2020/07/02/Computer-Vision-Foundation3-Haar-Like-Feature/</url>
    
    <content type="html"><![CDATA[<p>In last post, we discussed an algorithm to do the face detection using LBP descriptor. However, it is not the only way do it. Another popular way is to use <strong>Haar-like Features</strong>.</p><p>This post is split into three sections:</p><ol><li>The Basic Principles of Haar</li><li>Practice with OpenCV in Python</li><li>Compute Haar From Scratch</li></ol><p>Source code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; <a href="http://haar.py" target="_blank" rel="noopener">haar.py</a> (OpenCV)</p><hr /><h1 id="the-basic-principles-of-haar"><a class="markdownIt-Anchor" href="#the-basic-principles-of-haar"></a> The Basic Principles of Haar</h1><p>All human faces share some similar properties. These regularities may be matched using <strong>Haar Features</strong>.</p><hr /><h1 id="practice-with-opencv-in-python"><a class="markdownIt-Anchor" href="#practice-with-opencv-in-python"></a> Practice with OpenCV in Python</h1><h3 id="documentation"><a class="markdownIt-Anchor" href="#documentation"></a> Documentation</h3><p>In OpenCV, we can use exactly the same code as the last post. However, we need to replace the <strong>xml</strong> file of LBP to it of the Haar.</p><p>In the corresponding environment, use</p><pre class="highlight"><code class="">pip show opencv-python</code></pre><p>to find the <strong>location</strong> of the library.</p><p>Then enter the location, follow with</p><pre class="highlight"><code class="">cd cv2 &amp;&amp; ls data</code></pre><p>to find <code>haarcascade_SOMETHING.xml</code> files which are pre-trained for different purposes.</p><h3 id="implementation"><a class="markdownIt-Anchor" href="#implementation"></a> Implementation</h3><pre class="highlight"><code class="">import cv2import numpy as nphaar_front_face_xml = 'haarcascade_frontalface_default.xml'  # Your .xml file directoryhaar_eye_xml = 'haarcascade_eye.xml'  # Your .xml file directorydef StaticDetect(filename):    face_cascade = cv2.CascadeClassifier(haar_front_face_xml)    img = cv2.imread(filename)    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    faces = face_cascade.detectMultiScale(gray_img, 1.3, 5)    for (x, y, w, h) in faces:        img = cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)    cv2.namedWindow('Face Detected！', 0)    cv2.imshow('Face Detected！', img)    cv2.waitKey(0)    cv2.destroyAllWindows()  def DynamicDetect():      face_cascade = cv2.CascadeClassifier(haar_front_face_xml)      eye_cascade = cv2.CascadeClassifier(haar_eye_xml)      # Turn on the camera      camera = cv2.VideoCapture(0)      cv2.namedWindow('Dynamic')      while True:          # Read a frame          ret, frame = camera.read()          if ret:              gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)              faces = face_cascade.detectMultiScale(gray_img, 1.3, 5)              for (x, y, w, h) in faces:                  cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)                  roi_gray = gray_img[y:y + h, x:x + w]                  eyes = eye_cascade.detectMultiScale(roi_gray, 1.03, 5, 0, (40, 40))                  for (ex, ey, ew, eh) in eyes:                      cv2.rectangle(frame, (ex + x, ey + y), (x + ex + ew, y + ey + eh), (0, 255, 0), 2)              cv2.imshow('Dynamic', frame)              if cv2.waitKey(100) &amp; 0xff == ord('q'):                  break      camera.release()      cv2.destroyAllWindows()if __name__ == '__main__':    filename = &quot;...&quot;  # Your image directory    StaticDetect(filename)  # Image faces/eyes detection    DynamicDetect()  # Video faces/eyes detection</code></pre><p><strong>Face Recognition</strong><br /><img src="Solvay_Conferences.png" srcset="/img/loading.gif" alt="Solvay_Conferences" /></p><p>The problem about this approach is similar as the LBP. It has two hyperparameters which are hard to choose. Therefore human intervenes are necessary.</p><p>Another issue is that Haar-like features have a greater chance to fail if the object is rotated or tilted.</p><hr /><h1 id="compute-haar-from-scratch-todo"><a class="markdownIt-Anchor" href="#compute-haar-from-scratch-todo"></a> Compute Haar From Scratch (TODO)</h1><hr /><h1 id="acknowledgement"><a class="markdownIt-Anchor" href="#acknowledgement"></a> Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr /><h1 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h1><ol><li><a href="http://En.wikipedia.org" target="_blank" rel="noopener">En.wikipedia.org</a>. 2020. Viola–Jones Object Detection Framework. [online] Available at: <a href="https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Viola–Jones_object_detection_framework</a> [Accessed 2 July 2020].</li></ol><hr />]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Natural Language Processing3: Subword Models</title>
    <link href="/2020/06/30/NLP3-Subword-Models/"/>
    <url>/2020/06/30/NLP3-Subword-Models/</url>
    
    <content type="html"><![CDATA[<p>In <em>NLP1</em> and <em>NLP2</em>, we introduced the concept of the word vectors and a method of generating them with the distributional semantics, called <strong>Word2Vec</strong>. However, this approach has an inherent problem. A word vector can only capture the semantic information for each word. Hence, if we run the model on some words on the internet like “Gooooooood”, the computer may not recognise this word correctly as “Good”.</p><p>There is another approach called <strong>\(n\)-grams</strong>, which was firstly proposed in 1986.</p><p>This post is split into four sections:</p><ol><li>The Basic Principles of \(n\)-grams</li><li>Practice in Python<!-- 3. Compute \\(n\\)-grams From Scratch --></li></ol><!-- Source code: https://github.com/BillMaZengou/nlp_basis -> WordVector.ipynb --><hr><h1 id="The-Basic-Principles-of-n-grams"><a href="#The-Basic-Principles-of-n-grams" class="headerlink" title="The Basic Principles of \(n\)-grams"></a>The Basic Principles of \(n\)-grams</h1><p>In Linguistics, they have a special branch called Phonetics, which studies the sound of the language. Phonology posits a small set or sets of distinctive, categorical units: <strong>phonemes</strong> or distinctive features. However, it is hard or even impossible to extract information from phonemes using the neural networking.</p><p>Another branch called Morphology. We have <strong>morphemes</strong> as the smallest semantic unit. For example, “unfortunately” can be separated as<br>\[<br>  [un+[[fortun(e)]+ate]+ly<br>\]<br>However, morphology cannot be successfully applied in deep learning.</p><p>A practical approach is to use <strong>\(n\)-grams</strong> as <strong>Character-Level Models</strong>.</p><p>Another linguistic issue in general is that different languages have different rules. For example, Chinese does not have segmentation; Arabic may concentrate a few words into a single word. Also, when translating people’s names to another language, instead of interpreting the meaning of the name, we actually want transliteration according to the destination language. These problems will gain benefits from the Character-Level Models. Purely character-level NMT models gradually gained promising results around 2016.</p><p>One obvious problem about these models is that they are slow. Thus, a trade-off method is called <strong>Sub-word models</strong>.</p><p>There are two trends of Sub-word models. One is to use the same architecture as for word-level model, but use smaller units, called <em>word pieces</em>. Another is to use hybrid architectures. Main model has <em>words</em>; something else for <em>characters</em>.</p><p><strong>A Word Segmentation Algorithm</strong></p><ol><li>Start with a unigram vocabulary of all (Unicode) characters in data.</li><li>Use the most frequent \(n\)-gram pairs to generate a new \(n\)-gram.</li><li>Have a target vocabulary size and stop when you reach it.</li><li></li></ol><p>An example is shown as below.<br><strong>Step 1</strong><br><img src="step1.png" srcset="/img/loading.gif" alt="step1"></p><p><strong>Step 2</strong><br><img src="step2.png" srcset="/img/loading.gif" alt="step2"></p><p><strong>Step 3</strong><br><img src="step3.png" srcset="/img/loading.gif" alt="step3"></p><p><strong>Step 4</strong><br><img src="step4.png" srcset="/img/loading.gif" alt="step4"></p><hr><h1 id="Practice-in-Python-TODO"><a href="#Practice-in-Python-TODO" class="headerlink" title="Practice in Python (TODO)"></a>Practice in Python (TODO)</h1><!-- ---# Compute word2vec From Scratch (TODO) --><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>. The learning path is based on <strong>Stanford University CS224n: Natural Language Processing with Deep Learning</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Bilibili.com. 2020. [online] Available at: <a href="https://www.bilibili.com/video/BV1s4411N7fC?p=12" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1s4411N7fC?p=12</a> [Accessed 30 June 2020].</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Natural Language Processing2: Word Vectors -&gt; Algorithm and Analysis</title>
    <link href="/2020/06/27/NLP2-Word-Vectors-Algorithm-and-Analysis/"/>
    <url>/2020/06/27/NLP2-Word-Vectors-Algorithm-and-Analysis/</url>
    
    <content type="html"><![CDATA[<p>In <em>NLP1</em>, we discussed how to convert a word into a vector with distributional semantics. Also, we demonstrated using pre-trained results to find the synonyms. However, they are more details to consider. Before diving into the neural networks and build a custom system, we present some issues and Stanford’s solution.</p><p>Source code: <a href="https://github.com/BillMaZengou/nlp_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/nlp_basis</a> -&gt; WordVector.ipynb</p><hr><h1 id="Issues-of-word-vectors"><a href="#Issues-of-word-vectors" class="headerlink" title="Issues of word vectors"></a>Issues of word vectors</h1><h2 id="Visualisation"><a href="#Visualisation" class="headerlink" title="Visualisation"></a>Visualisation</h2><p>Usually, the results of NLP lie on a high dimensional space. (The pre-train result that we used is <code>glove.6B.100d.txt</code>, which is from <a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">https://nlp.stanford.edu/projects/glove/</a>) Therefore, accurately visualising the result is impossible. A plausible way is to project the word vectors onto a 2D surface.</p><pre><code>def display_pca_scatterplot(model, words=None, sample=0):    import numpy as np    import matplotlib.pyplot as plt    from sklearn.decomposition import PCA    if words == None:        if sample &gt; 0:            words = np.random.choice(list(model.vocab.keys()), sample)        else:            words = [ word for word in model.vocab ]    word_vectors = np.array([model[w] for w in words])    twodim = PCA().fit_transform(word_vectors)[:, :2]    plt.figure(figsize=(6,6))    plt.scatter(twodim[:,0], twodim[:,1], edgecolors=&#39;k&#39;, c=&#39;r&#39;)    for word, (x,y) in zip(words, twodim):        plt.text(x+0.05, y+0.05, word)  # put labels</code></pre><p><strong>Projection of The Results</strong><br><img src="result.png" srcset="/img/loading.gif" alt="project"></p><p>As you can see, the food and drinks are on the left top corner; countries are on the right top corner; animals are on the left bottom corner; anything related to studies are on the right bottom corner. However, since we forced them to show up on the 2D plane, many distant features may look closer than they suppose to be.</p><p>If you inspect the left top corner carefully, tea seems closer to food rather than drinks. If we compute the similarities between the tea and the coffee as well as the tea and pizza, we can see clearly that the <em>tea</em> is a drink.</p><p><strong>Real Similarities</strong><br><img src="similarity.png" srcset="/img/loading.gif" alt="similarity"></p><h2 id="Optimisation-Gradient-Descent"><a href="#Optimisation-Gradient-Descent" class="headerlink" title="Optimisation: Gradient Descent"></a>Optimisation: Gradient Descent</h2><p>In the last post, we proposed the cost function, \(J(\theta)\) for Word2vec. Gradient descent is an algorithm to minimise the cost function.</p><p>The key idea is to use the negative gradient of \(J(\theta)\) as a direction to take a small step. Iterate the process until we reach the minimum of the cost function. Mathematically,<br>\[<br>  \theta^{new} = \theta^{old} - \alpha \nabla_{\theta} J(\theta)<br>\]<br>where \(\alpha\) is the step size or learning rate. The choice of \(\alpha\) is a trade-off. If \(\alpha\) is small, it may take too many iteration to converge. However, if it is too large, \(\theta\) cannot even converge because of overshooting.</p><p>Nevertheless, \(\nabla_{\theta} J(\theta)\) is computationally expensive as the training text sample can contain millions of words.</p><p>As the last post mentioned, people normally use <strong>Stochastic Gradient Descent</strong>. Instead of consider all the words in the text, we consider only a portion of the text around the centre word. This portion is called a window. It may work badly for one window of the words. However, as we iterate through all words, it does converge.</p><p><em>Note</em> The window is normally one of the power numbers of \(2\), like \(32\) or \(64\). They can match the structure of GPU relatively easier.</p><p>With the stochastic gradient, as we only compute the gradient of some certain word vectors, \(\nabla_{\theta} J(\theta)\) is large sparse matrix. Therefore, if we use sparse matrix structure instead of dense matrix, we are able to save space of memories.</p><h2 id="Probability"><a href="#Probability" class="headerlink" title="Probability"></a>Probability</h2><p>in the last post, we mentioned to use softmax function to turn the dot product (cosine similarity) of two vectors into a probability. However, to calculate the probability with softmax function, the computational expense is high. Hence, in standard word2vec, the implementation of the skip-gram model uses <strong>negative sampling</strong>.</p><p>For negative sampling, we need to train binary logistic regressions for a <em>true pair</em> (centre word and word in its context window) versus several <em>noise pairs</em> (the centre word paired with a random word)</p><p>The new cost function is<br>\[<br>  J(\theta) = \frac{1}{T} \sum_{t=1}^{T} J_t(\theta)<br>\]<br>\[<br>  J_t(\theta) = -log(\sigma(\mathbf{u_o^T}\mathbf{v_c})) - \sum_{k=1}^{K}log(\sigma(\mathbf{u_k^T}\mathbf{v_c}))<br>\]<br>where we take (K\) negative samples and<br>\[<br>  \sigma(x) = \frac{1}{1+e^{-x}}<br>\]</p><p>For this cost function, we need to maximise probability that real outside word appears but minimise probability that random words appear around centre word.<br>\[<br>  P(w) = \frac{U(w)^{\frac{3}{4}}}{Z}<br>\]<br>where \(Z\) is for the normalisation and \(U(w)\) is a unigram distribution so that \(P(w_1, w_2) = P(w_1|w_2)P(w_2) \) turns to \(P_{uni}(w_1, w_2) = P(w_1)P(w_2)\). The \(\frac{3}{4}\) is a hyperparameter.</p><h2 id="Co-occurrence-Vector-TODO"><a href="#Co-occurrence-Vector-TODO" class="headerlink" title="Co-occurrence Vector (TODO)"></a>Co-occurrence Vector (TODO)</h2><p>An alternative approach. It produce a large matrix, but can be reduce the dimension using SVD. More about SVD, we will discuss in later post.</p><p>It may compose a vector space such that semantic similarity can be found be inspecting the linear relation between words. E.g.) The vector difference between <em>Swim</em> and <em>Swimmer</em> can be close to the difference between <em>Teach</em> and <em>Teacher</em></p><h2 id="Hybrid-of-Two-Approach-TODO"><a href="#Hybrid-of-Two-Approach-TODO" class="headerlink" title="Hybrid of Two Approach (TODO)"></a>Hybrid of Two Approach (TODO)</h2><p>Instead of using the co-occurrence probability, use the ratio of the co-occurrence probability can improve the result.</p><h2 id="Question-About-the-Symmetry"><a href="#Question-About-the-Symmetry" class="headerlink" title="Question About the Symmetry"></a>Question About the Symmetry</h2><p>Recall from the last post, we can do word composition with word vectors. For example,<br>\[<br>  w_{King} - w_{Man} + w_{Woman} = w_{Queen}<br>\]</p><p>However, it seems the rule is not simply a vector addition.<br><img src="asymmetry.png" srcset="/img/loading.gif" alt="asymmetry"></p><p>It is clear that<br>\[<br>  w_{Unscrupulous} - w_{Man} + w_{Woman} = w_{Dishonest}<br>\]<br>and<br>\[<br>  w_{Dishonest} - w_{Woman} + w_{Man} = w_{Inept}<br>\]<br>rather than returning to \(w_{Unscrupulous}\).</p><p>Currently, we have two hypotheses. One is that occurs because of the bias of the training sample and the default word2vec algorithm does not have debias approach. The other is that the way to calculate the similarity does not simply use the vector addition or something similar due to the multi-dimension.</p><p><em>Note</em> the hypotheses need verification.</p><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>. The learning path is based on <strong>Stanford University CS224n: Natural Language Processing with Deep Learning</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Bilibili.com. 2020. [online] Available at: <a href="https://www.bilibili.com/video/BV1s4411N7fC?t=4725" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1s4411N7fC?t=4725</a> [Accessed 25 June 2020].</li><li>En.wikipedia.org. 2020. Language Model. [online] Available at: <a href="https://en.wikipedia.org/wiki/Language_model" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Language_model</a> [Accessed 27 June 2020].</li><li>Myndbook.com. 2020. 1.1. Word2vec | Myndbook. [online] Available at: <a href="https://myndbook.com/view/4900" target="_blank" rel="noopener">https://myndbook.com/view/4900</a> [Accessed 27 June 2020].</li><li>Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S. and Dean, J., 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation 2: LBP Face Recognition</title>
    <link href="/2020/06/25/Computer-Vision-Foundation2-LBP-Face-Recognition/"/>
    <url>/2020/06/25/Computer-Vision-Foundation2-LBP-Face-Recognition/</url>
    
    <content type="html"><![CDATA[<p>Local binary patterns (LBP) is a type of visual descriptor used for classification in computer vision. LBP was first described in 1994. It has since been found to be a powerful feature for texture classification; it has further been determined that when LBP is combined with the Histogram of oriented gradients (HOG) descriptor, it improves the detection performance considerably on some datasets. A comparison of several improvements of the original LBP in the field of background subtraction was made in 2015 by Silva et al. A full survey of the different versions of LBP can be found in Bouwmans et al.</p><p><em>Note</em> Descriptors are the first step to find out the connection between pixels contained in a digital image and what humans recall after having observed an image or a group of images after some minutes. They can contain general information like colour, shape, textures and motion. Also, it can be trained to process information in some specific domain, like face recognition.</p><p><em>Note</em> The histogram of oriented gradients (HOG) is a feature descriptor used in computer vision and image processing for the purpose of object detection. The technique counts <strong>occurrences</strong> of gradient orientation in localised portions of an image. It is computed on a dense grid of uniformly spaced cells and uses overlapping local contrast normalisation for improved accuracy. The essential thought behind the histogram of oriented gradients descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions. The image is divided into small connected regions called cells, and for the pixels within each cell, a histogram of gradient directions is compiled. The descriptor is the concatenation of these histograms. For improved accuracy, the local histograms can be contrast-normalised by calculating a measure of the intensity across a larger region of the image, called a block, and then using this value to normalise all cells within the block. This normalisation results in better invariance to changes in illumination and shadowing.</p><p>This post is split into three sections:</p><ol><li>The Basic Principles of LBP</li><li>Practice with OpenCV in Python</li><li>Compute LBP From Scratch</li></ol><p>Source code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; lbp.py (OpenCV)</p><hr><h1 id="The-Basic-Principles-of-LBP"><a href="#The-Basic-Principles-of-LBP" class="headerlink" title="The Basic Principles of LBP"></a>The Basic Principles of LBP</h1><p>The basic idea for developing the LBP operator was that two-dimensional surface textures can be described by two complementary measures: local spatial patterns and grey scale contrast.</p><p>The LBP operator originally used \(3 \cross 3\) neighbourhood of each pixel and consider the result as a binary number. (\(0\) or \(1\))</p><p>The LBP operator was extended to use neighbourhood of different sizes (Ojala et al. 2002). Using a circular neighbourhood and bilinearly interpolating values at non-integer pixel coordinates allow any radius and number of pixels in the neighbourhood.</p><p>The basic idea of Local Binary Patterns is to summarise the local structure in an image by comparing each pixel with its neighbourhood. Take a pixel as centre and threshold its neighbours against. If the intensity of the centre pixel is greater-equal its neighbour, then denote it with \(1\) and \(0\) if not. You’ll end up with a binary number for each pixel, just like <code>11001111</code>. So with \(8\) surrounding pixels you’ll end up with \(2^8\) possible combinations, called <em>Local Binary Patterns</em> or sometimes referred to as <em>LBP</em> codes. The first LBP operator described in literature actually used a fixed \(3 \cross 3\) neighbourhood just like this:<br><img src="lbp.png" srcset="/img/loading.gif" alt="lbp"></p><p>Mathematically, LBP is given by<br>\[<br>  LBP(x_c, y_c) = \sum_{p=0}^{P-1} 2^p s(i_p - i_c)<br>\]<br>where \((x_c, y_c)\) is the centre pixel with the intensity, \(i_c\), \(i_p\) denotes the intensity of the \(p\)-th neighbour. \(s\) is the sign function which is defined as<br>\[<br>  s(x) = 1<br>\]<br>if \(x &gt; 0\), otherwise it equals \(0\).</p><p>Soon after the operator was published it was noted that a fixed neighbourhood fails to encode details differing in scale. So the operator was extended. The idea is to align <strong>an arbitrary number of neighbours</strong> on a circle with <strong>a variable radius</strong>.</p><p>For a given point \((x_c,y_c)\), the position of the neighbour \((x_p,y_p)\), \(p \in P\) can be calculated by<br>\[<br>  x_p = x_c + R cos(\frac{2 \pi p}{P})<br>\]<br>and<br>\[<br>  y_p = y_c + R sin(\frac{2 \pi p}{P})<br>\]<br>where \(R\) is the radius of the circle and \(P\) is the number of sample points.</p><p>The operator is an extension to the original LBP codes, so it’s sometimes called <em>Extended LBP</em> (also referred to as <em>Circular LBP</em>). If a points coordinate on the circle does not correspond to image coordinates, the point gets interpolated. There are many interpolation schemes. The OpenCV implementation does a <strong>bilinear interpolation</strong>.</p><p>Now we have the spatial information. Next is to make a face recognition model. The representation proposed by Ahonen et al. is to divide the LBP image into \(m\) local regions and extract a histogram from each. The spatially enhanced feature vector is then obtained by concatenating the local histograms (<strong>not merging them</strong>). These histograms are called <em>Local Binary Patterns Histograms</em>.</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><p>In OpenCV, we can use <code>cv2.CascadeClassifier.detectMultiScale</code> to implement LBP.</p><pre><code>obj = cv2.CascadeClassifier.detectMultiScale(img, scaleFactor=1.1, minNeighbors=3, minSize=(0,0), maxSize=(0,0))</code></pre><p><strong>img</strong> -&gt; (compulsory) Matrix of the type <code>CV_8U</code> containing an image where objects are detected.<br><strong>obj</strong> -&gt; Vector of rectangles where each rectangle contains the detected object.<br><strong>scaleFactor</strong> -&gt; (optional) Parameter specifying how much the image size is reduced at each image scale.<br><strong>minNeighbors</strong> -&gt; (optional) Parameter specifying how many neighbours each candidate rectangle should have to retain it.<br><strong>minSize</strong> -&gt; (optional) Minimum possible object size. Objects smaller than that are ignored.<br><strong>maxSize</strong> -&gt; (optional) Maximum possible object size. Objects larger than that are ignored.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><pre><code>import globimport cv2# Face Detection (needs to download &quot;lbpcascade_frontalface_improved.xml&quot;)face_detect = cv2.CascadeClassifier(&quot;lbpcascade_frontalface_improved.xml&quot;)img_list = glob.glob(&#39;./faces/*.jpg&#39;)img = cv2.imread(img_list[0], cv2.IMREAD_UNCHANGED)gray = cv2.cvtColor(img, code=cv2.COLOR_BGR2GRAY)face_zone = face_detect.detectMultiScale(gray, scaleFactor = 2, minNeighbors = 2) # maxSize = (55,55)print(face_zone)# For Drawing the Imagefor x, y, w, h in face_zone:    cv2.rectangle(img, pt1 = (x, y), pt2 = (x+w, y+h), color = [0,0,255], thickness=2)    cv2.circle(img, center = (x + w//2, y + h//2), radius = w//2, color = [0,255,0], thickness = 2)cv2.namedWindow(&quot;Face&quot;, 0)cv2.imshow(&quot;Face&quot;, img)cv2.waitKey(0)cv2.destroyAllWindows()</code></pre><hr><h1 id="Compute-LBP-From-Scratch-TODO"><a href="#Compute-LBP-From-Scratch-TODO" class="headerlink" title="Compute LBP From Scratch (TODO)"></a>Compute LBP From Scratch (TODO)</h1><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>En.wikipedia.org. 2020. Local Binary Patterns. [online] Available at: <a href="https://en.wikipedia.org/wiki/Local_binary_patterns#:~:text=Local%20binary%20patterns%20(LBP)%20is,was%20first%20described%20in%201994." target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Local_binary_patterns#:~:text=Local%20binary%20patterns%20(LBP)%20is,was%20first%20described%20in%201994.</a> [Accessed 25 June 2020].</li><li>En.wikipedia.org. 2020. Visual Descriptor. [online] Available at: <a href="https://en.wikipedia.org/wiki/Visual_descriptor" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Visual_descriptor</a> [Accessed 25 June 2020].</li><li>En.wikipedia.org. 2020. Histogram Of Oriented Gradients. [online] Available at: <a href="https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients</a> [Accessed 26 June 2020].</li><li>PietikÃ¤inen, M., 2010. Local Binary Patterns. Scholarpedia, 5(3), p.9775.</li><li>Docs.opencv.org. 2020. Face Recognition With Opencv — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html#local-binary-patterns-histograms" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html#local-binary-patterns-histograms</a> [Accessed 29 June 2020].</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Natural Language Processing1: Word To Vectors</title>
    <link href="/2020/06/23/NLP1-Word-to-Vectors/"/>
    <url>/2020/06/23/NLP1-Word-to-Vectors/</url>
    
    <content type="html"><![CDATA[<p>In this series of posts, we will tackle on another direction of AI - Natural Language Processing. Human language is a type of natural language which differs from normal programming language.</p><p>This post is split into four sections:</p><ol><li>The Basic Principles of word2vec</li><li>Practice in Python</li><li>Compute word2vec From Scratch</li><li>Appendix - Mathematical Derivation</li></ol><p>Source code: <a href="https://github.com/BillMaZengou/nlp_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/nlp_basis</a> -&gt; WordVector.ipynb</p><hr><h1 id="The-Basic-Principles-of-word2vec"><a href="#The-Basic-Principles-of-word2vec" class="headerlink" title="The Basic Principles of word2vec"></a>The Basic Principles of word2vec</h1><p>To perform NLP, we need to first find a way to vectorise words so that we can put them into our applications.</p><h3 id="One-hot-Vector"><a href="#One-hot-Vector" class="headerlink" title="One-hot Vector"></a>One-hot Vector</h3><p>The simplest method is the <strong>one-hot vector</strong>. Each word can be considered as a unit vector. For example,<br>\[<br>  w^{a} =<br>  \begin{bmatrix}<br>  1\\<br>  0\\<br>  0\\<br>  .\\<br>  .\\<br>  .\\<br>  0<br>  \end{bmatrix},<br>  w^{at} =<br>  \begin{bmatrix}<br>  0\\<br>  1\\<br>  0\\<br>  .\\<br>  .\\<br>  .\\<br>  0<br>  \end{bmatrix},<br>  …<br>  w^{zebra} =<br>  \begin{bmatrix}<br>  0\\<br>  0\\<br>  0\\<br>  .\\<br>  .\\<br>  .\\<br>  1<br>  \end{bmatrix}<br>\]<br>However, in NLP, we would normally like to analysis words in the context. Therefore, we need the correlations between each word. For instance, when we search “house”, we expect to find anything related to houses like “home” or “apartment”. On the contrast, we also expect to find a low correlation between words like “house” and “cat”. Those are impossible to do with one-hot vectors because all the unit vectors are orthogonal. I.e.)<br>\[<br>  (w^{house})^T w^{home} = (w^{house})^T w^{cat} = \mathbf{0}<br>\]  </p><p>Previously, researchers tried to use WordNet’s list of synonyms to get similarity of the word vectors.</p><p><em>Note</em> WordNet is like a large dictionary, which contains information like synonyms, hypernyms, etc.</p><p>The problem is that in WordNet, the synonyms are put together without a clear relationship between each word. Take “good” as an example, “proficient” is considered as a synonym of “good”. They represent the same meaning in some certain circumstances. However, they are not interchangeable in general. Therefore, it is well-known to fail badly to use WordNet to get the similarities of words.</p><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>A better approach is to encode similarity into the word vectors. The key idea is to use so-called <strong>distributional semantics</strong>. A word’s meaning is given by the words that frequently appear close-by.</p><p>Word vectors, which are sometimes referred as word embeddings or word representations, are a distributed representation.</p><p>To obtain the distributed representation of word vectors, <strong>Word2Vec</strong> is a frequently used technique.</p><p><strong>The Algorithm</strong></p><ol><li>Get a large corpus of text.</li><li>Every word in a fixed vocabulary is represented by a vector.</li><li>Go through each position \(t\) in the text, which has a centre word \(c\) and context (“outside”) words \(o\).</li><li>Use the similarity of the word vectors for \(c\) and \(o\) to calculate the probability of \(o\) given \(c\) (i.e. \(P(o|c)\)) or vice versa.</li><li>Adjust the word vectors to maximise the probability.<br><img src="word2vec.png" srcset="/img/loading.gif" alt="word"></li></ol><p><em>Note</em> <strong>Corpus</strong> is used in NLP to represent a bulk</p><p>The mathematical derivation can be found in Appendix.</p><p>Recall from \(\theta\) that, for each word, we have two vectors. We can average both at the end to obtain a better result of the optimisation.</p><p>In this post, we introduced a basic model of Word2Vec. It is called <strong>Skip-grams</strong> or SG for short. There is an alternative choice of the model called <strong>Continuous Bag of Words (CBOW)</strong></p><p>For the optimisation, we can use <strong>Stochastic Gradient Descent</strong> which is faster than the normal gradient descent. We may discuss in detail in later posts.</p><hr><h1 id="Practice-in-Python"><a href="#Practice-in-Python" class="headerlink" title="Practice in Python"></a>Practice in Python</h1><pre><code>import numpy as npimport matplotlib.pyplot as pltplt.style.use(&quot;ggplot&quot;)from sklearn.manifold import TSNEfrom sklearn.decomposition import PCAfrom gensim.test.utils import datapath, get_tmpfilefrom gensim.models import KeyedVectorsfrom gensim.scripts.glove2word2vec import glove2word2vecglove_file = datapath(&#39;/Users/billma/Desktop/Interests/nlp_basis/data/glove.6B.100d.txt&#39;)word2vec_glove_file = get_tmpfile(&quot;glove.6B.100d.txt&quot;)dimension = glove2word2vec(glove_file, word2vec_glove_file)print(dimension)model = KeyedVectors.load_word2vec_format(word2vec_glove_file)leader = model.most_similar(&#39;obama&#39;)print(&#39;obama is related to: &#39;)for i in leader:    print(i)anti_fruit = model.most_similar(negative=&#39;banana&#39;)for i in anti_fruit:    print(i)result = model.most_similar(positive=[&#39;woman&#39;,&#39;king&#39;], negative=[&#39;man&#39;])print(&quot;{}: {:.4f}&quot;.format(*result[0]))</code></pre><p>Example results are shown below:<br><strong>Similar Words To Obama</strong><br><img src="obama.png" srcset="/img/loading.gif" alt="obama"></p><p><strong>Dissimilar Words To Banana</strong><br><img src="banana.png" srcset="/img/loading.gif" alt="banana"></p><p><strong>Word Composition</strong><br>\[<br>  w<em>{King} - w</em>{Man} + w<em>{Woman} = w</em>{Queen}<br>\]<br><img src="queen.png" srcset="/img/loading.gif" alt="queen"></p><hr><h1 id="Compute-word2vec-From-Scratch-TODO"><a href="#Compute-word2vec-From-Scratch-TODO" class="headerlink" title="Compute word2vec From Scratch (TODO)"></a>Compute word2vec From Scratch (TODO)</h1><hr><h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><p>For each position \(t = 1,…,T\), predict context words within a window of fixed size \(m\), given centre word \(w<em>j\), we have a likelihood function such that<br>\[<br>  L(\theta) = \prod</em>{t=1}^T \prod<em>{-m \leq j \leq m, j \neq 0} P(w</em>{t+j}|w_t;\theta)<br>\]</p><p>The objective function, or so-called loss or cost function, \(J(\theta)\) is the normalised negative log likelihood,<br>\[<br>  J(\theta) = -\frac{1}{T}log(L(\theta)) = -\frac{1}{T} \sum<em>{t=1}^T \sum</em>{-m \leq j \leq m, j \neq 0} log(P(w_{t+j}|w_t;\theta) )<br>\]</p><p>Thus, our goal is to minimise the objective function so that the predictive accuracy can be maximised.</p><p>The probability of the words are calculated using <strong>softmax</strong> function, which is commonly used in classification. Therefore,<br>\[<br>  P(o|c) = \frac{exp(u<em>{o}^{T} v_c)}{\sum</em>{w}exp(u_{w}^{T} v_c)}<br>\]<br>where \(v_c\) and \(u_w\) are the word vectors. \(w\) is the context words.</p><p>In the objective function, \(\theta\) can represent all model parameters in one long vector. In our case with \(d\)-dimensional vectors and \(V\)-many words, we have a vector with \(2dV\) elements.<br>\[<br>  \theta =<br>  \begin{bmatrix}<br>  v<em>{a}\\<br>  .\\<br>  .\\<br>  .\\<br>  v</em>{zebra}\\<br>  u<em>{a}\\<br>  .\\<br>  .\\<br>  .\\<br>  u</em>{zebra}\\<br>  \end{bmatrix}<br>\]<br><em>Note</em> every word has two vectors.</p><p>To optimise the objective function, we need to take derivative for both \(v_c\) and \(u_w\).</p><h3 id="Optimise-with-respect-to-v-c"><a href="#Optimise-with-respect-to-v-c" class="headerlink" title="Optimise with respect to \(v_c\)"></a>Optimise with respect to \(v_c\)</h3><p>\[<br>  \frac{\partial}{\partial v<em>c} log(\frac{exp(u</em>{o}^{T} v<em>c)}{\sum</em>{w}exp(u<em>{w}^{T} v_c)})<br>\]<br>\[<br> =&gt;  \frac{\partial}{\partial v_c} u</em>{o}^{T} v<em>c - \frac{\partial}{\partial v_c} log(\sum</em>{w}exp(u<em>{w}^{T} v_c))<br>\]<br>\[<br> =&gt; u</em>{o}^{T} - \sum<em>x \frac{exp(u</em>{x}^T v<em>c)}{\sum</em>{w}exp(u<em>{w}^{T} v_c)} u_x<br>\]<br>\[<br> =&gt; u</em>{o}^{T} - \sum_x P(x|c) u_x<br>\]</p><h3 id="Optimise-with-respect-to-u-w"><a href="#Optimise-with-respect-to-u-w" class="headerlink" title="Optimise with respect to \(u_w\)"></a>Optimise with respect to \(u_w\)</h3><p>\[<br>  \frac{\partial}{\partial u<em>o} log(\frac{exp(u</em>{o}^{T} v<em>c)}{\sum</em>{w}exp(u<em>{w}^{T} v_c)})<br>\]<br>\[<br> =&gt;  \frac{\partial}{\partial u_o} u</em>{o}^{T} v<em>c - \frac{\partial}{\partial u_o} log(\sum</em>{w}exp(u<em>{w}^{T} v_c))<br>\]<br>\[<br> =&gt; v_c - \frac{exp(u</em>{o}^T v<em>c)}{\sum</em>{w}exp(u_{w}^{T} v_c)} v_c<br>\]<br>\[<br> =&gt; v_c - P(o|c) v_c<br>\]</p><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>. The learning path is based on <strong>Stanford University CS224n: Natural Language Processing with Deep Learning</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Zh.gluon.ai. 2020. 10.1. 词嵌入（Word2vec） — 《动手学深度学习》 文档. [online] Available at: <a href="http://zh.gluon.ai/chapter_natural-language-processing/word2vec.html" target="_blank" rel="noopener">http://zh.gluon.ai/chapter_natural-language-processing/word2vec.html</a> [Accessed 25 June 2020].</li><li>Bilibili.com. 2020. [online] Available at: <a href="https://www.bilibili.com/video/BV1s4411N7fC?t=4725" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1s4411N7fC?t=4725</a> [Accessed 25 June 2020].</li><li>En.wikipedia.org. 2020. Natural Language Processing. [online] Available at: <a href="https://en.wikipedia.org/wiki/Natural_language_processing" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Natural_language_processing</a> [Accessed 25 June 2020].</li><li>Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S. and Dean, J., 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation 1: Harris Corner Detector</title>
    <link href="/2020/06/22/Computer-Vision-Foundation1-Harris-Corner-Detector/"/>
    <url>/2020/06/22/Computer-Vision-Foundation1-Harris-Corner-Detector/</url>
    
    <content type="html"><![CDATA[<p>In this series of posts, with the foundation of image processing, we will look at some key ideas in Computer Vision. One important progress in CV is the corner detector. Recall from  <strong>Computer Vision Foundation -&gt; Image Processing6: Edge Detection</strong>, previously we were able to find edges. However, interest-point detector is more critical. The interest-point can be used as an indicator to find the same object or position in two different images.</p><p>This post is split into three sections:</p><ol><li>The Basic Principles of Harris Corner Detector</li><li>Practice with OpenCV in Python</li><li>Compute Harris Corner Detector From Scratch</li></ol><p>Source code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; harris.py (OpenCV)</p><hr><h1 id="The-Basic-Principles-of-Harris-Corner-Detector"><a href="#The-Basic-Principles-of-Harris-Corner-Detector" class="headerlink" title="The Basic Principles of Harris Corner Detector"></a>The Basic Principles of Harris Corner Detector</h1><p>Originally, when people wanted to do some operations across multiple images, they would do patch matching, which meant that they needed to compare a portion of the image with anywhere else and compute the similarity. As shown in the picture below, computer needs to compare each patch and calculates the correlation.<br><img src="patch_detection.png" srcset="/img/loading.gif" alt="patch"></p><p>However, this approach is not robust. Consider the example below, it is hard to tell which section of the sky is the same as our target as the sky looks identical.<br><img src="bad_patch.png" srcset="/img/loading.gif" alt="bad"></p><p>This should give us an insight that using patch detection through the whole image is not a reliable way to find the same position between multiple images. Moreover, only the distinct features should be considered.</p><p>The interest points are the local features that associated with a significant change of an image property or several properties simultaneously (e.g. intensity, colour, texture).</p><p><strong>Six properties of good features</strong></p><ol><li>Local: features are local, robust to occlusion and clutter. (i.e. no prior segmentation required)</li><li>Accurate: precise localisation.</li><li>Invariant (or covariant)</li><li>Robust: noise, blur, compression, etc. do not have a big impact on the feature.</li><li>Distinctive: individual features can be matched to a large database of objects.</li><li>Efficient: close to real-time performance.</li></ol><p>The properties 3. and 4. make the detector repeatable across multiple images.</p><p>Therefore, a natural selection of our interest points is the corners. Corners are locations where variations of intensity function \(f(x, y)\) in both \(x\) and \(y\) directions are high. (i.e. the partial derivatives of \(f(x, y)\) with respect to \(x\) and \(y\) are large)</p><p>On the contrast, for an edge, the partial derivative is large in only a certain direction; for a flat surface, the partial derivatives are small in both directions.</p><p>Harris corner detector uses these properties. Consider a small window (a kernel) on each pixel, and compute the partial derivatives by moving the kernel.<br><img src="harris.png" srcset="/img/loading.gif" alt="harris"></p><p><strong>The Algorithm</strong><br><img src="step.png" srcset="/img/loading.gif" alt="steps"></p><ol><li>For each pixel in the input image, the corner operator is applied to obtain a <em>cornerness</em> measure for this pixel.</li><li><em>Threshold cornerness map</em> to eliminate weak corners.</li><li>Apply <em>non-maximal suppression</em> to eliminate points whose cornerness measure is not larger than the cornerness values of all points within a certain distance.</li></ol><p>Mathematically, the change of intensity for the shift \([u, v]\) can be expressed as<br>\[<br>  E(u, v) = \sum_{x, y} w(x, y) [I(x+u, y+v) - I(x, y)]^2<br>\]<br>where \(w(x, y)\) is the window function and \(I(x, y)\) is the intensity function. The element in the window function can all be \(1\) or Gaussian distributed.</p><p>To simplify the equation and ease our computational costs, we can use first-order Taylor expansion on the square term.<br>\[<br>  I(x+u, y+v) \approx I(x, y) +<br>  \begin{bmatrix}<br>  \frac{\partial I}{\partial x} &amp; \frac{\partial I}{\partial y}<br>  \end{bmatrix}<br>  \begin{bmatrix}<br>  u\\<br>  v<br>  \end{bmatrix}<br>\]<br>Thus, the square term can be considered as<br>\[<br>  (\begin{bmatrix}<br>  \frac{\partial I}{\partial x} &amp; \frac{\partial I}{\partial y}<br>  \end{bmatrix}<br>  \begin{bmatrix}<br>  u\\<br>  v<br>  \end{bmatrix})^2<br>\]<br>using a matrix identity: \(\mathbf{u}^2 = \mathbf{u}^{T}\mathbf{u}\), it becomes<br>\[<br>  \begin{bmatrix}<br>  u &amp; v<br>  \end{bmatrix}<br>  \begin{bmatrix}<br>  \frac{\partial I}{\partial x} \\ \frac{\partial I}{\partial y}<br>  \end{bmatrix}<br>  \begin{bmatrix}<br>  \frac{\partial I}{\partial x} &amp; \frac{\partial I}{\partial y}<br>  \end{bmatrix}<br>  \begin{bmatrix}<br>  u\\<br>  v<br>  \end{bmatrix}<br>\]<br>Therefore,<br>\[<br>  E(u, v) \approx<br>  \begin{bmatrix}<br>  u &amp; v<br>  \end{bmatrix}<br>  \mathbf{M}<br>  \begin{bmatrix}<br>  u\\<br>  v<br>  \end{bmatrix}<br>\]<br>where<br>\[<br>  \mathbf{M} = \sum_{x, y} w(x, y)<br>  \begin{bmatrix}<br>    \frac{\partial^2 I}{\partial x^2} &amp; \frac{\partial^2 I}{\partial x \partial y} \\<br>    \frac{\partial^2 I}{\partial x \partial y} &amp; \frac{\partial^2 I}{\partial y^2}<br>  \end{bmatrix}<br>\]</p><p>Diagonalising the matrix \(\mathbf{M}\) gives us<br>\[<br>  \mathbf{M} = \mathbf{P}^{-1}<br>  \begin{bmatrix}<br>    \lambda_1 &amp; 0 \\<br>    0 &amp; \lambda_2<br>  \end{bmatrix}<br>  \mathbf{P}<br>\]<br>where \(\lambda_1\) and \(\lambda_2\) are the eigenvalues of \(\mathbf{M}\)</p><p>Then \(\lambda_1\) and \(\lambda_2\) can be used to compute a score, \(R\) such that<br>\[<br>  R = det(\mathbf{M}) - k(trace(\mathbf{M}))^2 = \lambda_1 \lambda_2 - k (\lambda_1+\lambda_2)^2<br>\]<br>where \(k\) is a tunable sensitivity parameter. Empirically, it is around \(0.04-0.06\).</p><p>Different \(R\) values can categorise a feature into flat surfaces, edges and corners.<br><img src="r.png" srcset="/img/loading.gif" alt="cate"></p><p>Normally, it follows this pattern.<br><img src="harrisClass.png" srcset="/img/loading.gif" alt="class"></p><p>The properties of Harris Corner Detector were studied.<br><img src="properties.png" srcset="/img/loading.gif" alt="property"><br>It is clear that Harris is invariant under the rotation but <strong>not</strong> invariant after scaling up.</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><p>In OpenCV, we can use <code>cv2.cornerHarris</code> to implement a Harris corner detector.</p><pre><code>dst = cv2.cornerHarris(src, blockSize, ksize, k, borderType=BORDER_DEFAULT)</code></pre><p><strong>src</strong> -&gt; (compulsory) Input single-channel 8-bit or floating-point image.<br><strong>dst</strong> -&gt; Image to store the Harris detector responses.<br><strong>blockSize</strong> -&gt; (compulsory) Neighbourhood size.<br><strong>ksize</strong> -&gt; (compulsory) Aperture parameter for the Sobel operator. For more information about the Sobel operator, check <strong>Computer Vision Foundation -&gt; Image Processing6: Edge Detection</strong>.<br><strong>k</strong> -&gt; (compulsory) Harris detector free parameter.<br><strong>borderType</strong> -&gt; (optional) Pixel extrapolation method. <em>BORDER_WRAP</em> is not supported. The default option is <em>BORDER_DEFAULT</em>.</p><p><em>Note</em> In OpenCV, <code>borderType</code> has<br><code>cv2.BORDER_CONSTANT</code> -&gt;    <code>iiiiii|abcdefgh|iiiiiii</code> with a specific <code>i</code><br><code>cv2.BORDER_REPLICATE</code> -&gt;   <code>aaaaaa|abcdefgh|hhhhhhh</code><br><code>cv2.BORDER_REFLECT</code> -&gt;     <code>fedcba|abcdefgh|hgfedcb</code><br><code>cv2.BORDER_WRAP</code> -&gt;        <code>cdefgh|abcdefgh|abcdefg</code><br><code>cv2.BORDER_REFLECT_101</code> -&gt; <code>gfedcb|abcdefgh|gfedcba</code><br><code>cv2.BORDER_TRANSPARENT</code> -&gt; <code>uvwxyz|abcdefgh|ijklmno</code></p><p><code>cv2.BORDER_DEFAULT</code> is as the same as <code>cv2.BORDER_REFLECT_101</code>.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><pre><code>import cv2# detector parametersblock_size = 3sobel_size = 3k = 0.06name = image_nameimg = cv2.imread(&#39;./{}.jpg&#39;.format(name), cv2.IMREAD_UNCHANGED)img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# modify the data type setting to 32-bit floating pointimg = np.float32(img)# detect the corners with appropriate values as input parameterscorners_img = cv2.cornerHarris(img, block_size, sobel_size, k)#result is dilated for marking the corners, not importantdst = cv2.dilate(dst, None)# Threshold for an optimal value, marking the corners in Greenimage[corners_img&gt;0.01*dst.max()] = [0,255,0]cv2.imshow(&#39;Harris Corner Detector&#39;,image)cv2.waitKey(0)cv2.destroyAllWindows()</code></pre><p>Example results are shown below:<br><strong>Original image</strong><br><img src="train.jpg" srcset="/img/loading.gif" alt="train"></p><p><strong>Interest Points</strong><br><img src="train_interest_points.jpg" srcset="/img/loading.gif" alt="corner"></p><p><em>Note</em> More details about <code>cv2.dilate</code>, please conduct <em><a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html" target="_blank" rel="noopener">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html</a></em></p><hr><h1 id="Compute-Harris-Corner-Detector-From-Scratch-TODO"><a href="#Compute-Harris-Corner-Detector-From-Scratch-TODO" class="headerlink" title="Compute Harris Corner Detector From Scratch (TODO)"></a>Compute Harris Corner Detector From Scratch (TODO)</h1><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Cse.psu.edu. 2020. [online] Available at: <a href="http://www.cse.psu.edu/~rtc12/CSE486/lecture06.pdf" target="_blank" rel="noopener">http://www.cse.psu.edu/~rtc12/CSE486/lecture06.pdf</a> [Accessed 23 June 2020].</li><li><ol start="2020"><li>[online] Available at: <a href="https://zhuanlan.zhihu.com/p/83064609" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/83064609</a> [Accessed 23 June 2020].</li></ol></li><li>Docs.opencv.org. 2020. Opencv: Harris Corner Detection. [online] Available at: <a href="https://docs.opencv.org/3.4/dc/d0d/tutorial_py_features_harris.html" target="_blank" rel="noopener">https://docs.opencv.org/3.4/dc/d0d/tutorial_py_features_harris.html</a> [Accessed 23 June 2020].</li><li>Opencv-python-tutroals.readthedocs.io. 2020. Harris Corner Detection — Opencv-Python Tutorials 1 Documentation. [online] Available at: <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html" target="_blank" rel="noopener">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html</a> [Accessed 23 June 2020].</li><li>Schmid, C., Mohr, R. and Bauckhage, C., 2000. Evaluation of interest point detectors. International Journal of computer vision, 37(2), pp.151-172.</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CV Foundation (Competition) -&gt; Street View House Numbers1: Data Understanding</title>
    <link href="/2020/05/18/CV-Foundation-Competition-Street-View-House-Numbers1-Data-Understanding/"/>
    <url>/2020/05/18/CV-Foundation-Competition-Street-View-House-Numbers1-Data-Understanding/</url>
    
    <content type="html"><![CDATA[<p>Competition is a fast way to practice and learn new knowledge. Datawhale and Tianchi holds a SVHN competition to help everyone learn CV. Details in <a href="https://tianchi.aliyun.com/competition/entrance/531795/introduction" target="_blank" rel="noopener">https://tianchi.aliyun.com/competition/entrance/531795/introduction</a>.</p><p>I appreciate Datawhale invited me to be a TA for this competition. Nevertheless, to improve my ability, I will participate the competition along with all the students.</p><hr><h1 id="Data-Information"><a href="#Data-Information" class="headerlink" title="Data Information"></a>Data Information</h1><p>The first task is to understand the SVHN data. The competition use a portion of the original SVHN data (the original dataset can be found in <a href="http://ufldl.stanford.edu/housenumbers/" target="_blank" rel="noopener">http://ufldl.stanford.edu/housenumbers/</a>). It is constituted by 30 thousand images for training data, 10 thousand images for testing data.</p><hr><h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>The score is simply evaluated using<br>\[<br>  Score=\frac{Correct identified image}{Test sample size}<br>\]</p><hr><h1 id="Data-json-File"><a href="#Data-json-File" class="headerlink" title="Data .json File"></a>Data .json File</h1><p>The .json file provides some descriptions of the images and can be loaded with the following code.</p><pre><code>import jsontrain_json = json.load(open(&#39;../input/train.json&#39;))</code></pre><p>The descriptions are like (say, for <code>train_json[&#39;000008.png&#39;]</code>)</p><pre><code>{&#39;height&#39;: [24, 24, 24], &#39;label&#39;: [1, 2, 8], &#39;left&#39;: [19, 29, 38], &#39;top&#39;: [4, 4, 5], &#39;width&#39;: [14, 13, 17]}</code></pre><p>as file <code>000008.png</code> contains three digits.</p><p><strong>height</strong>, <strong>left</strong>, <strong>top</strong> and <strong>width</strong> give the location of each digit and <strong>label</strong> provides the ground-truth value of the data, as shown below.<br><img src="coordinate.png" srcset="/img/loading.gif" alt="digit"></p><p>To show the images</p><pre><code>def parse_json(d):   arr = np.array([       d[&#39;top&#39;], d[&#39;height&#39;], d[&#39;left&#39;],  d[&#39;width&#39;], d[&#39;label&#39;]   ])   arr = arr.astype(int)   return arrimg = cv2.imread(&#39;../input/train/000011.png&#39;)arr = parse_json(train_json[&#39;000011.png&#39;])# The original imageplt.figure(figsize=(10, 10))plt.subplot(1, arr.shape[1]+1, 1)plt.imshow(img)plt.xticks([]); plt.yticks([])# Each digitfor idx in range(arr.shape[1]):   plt.subplot(1, arr.shape[1]+1, idx+2)   plt.imshow(img[ arr[0, idx] : arr[0, idx]+arr[1, idx] , arr[2, idx] : arr[2, idx]+arr[3, idx] ])   plt.title(arr[4, idx])   plt.xticks([]); plt.yticks([])plt.waitforbuttonpress()</code></pre><p>Here we show the <code>000011.png</code><br><img src="63.png" srcset="/img/loading.gif" alt="result"></p><hr><h1 id="Potential-Solutions"><a href="#Potential-Solutions" class="headerlink" title="Potential Solutions"></a>Potential Solutions</h1><p>Essentially, this is a classification problem. Given an image of a number, use machine learning to find what the digits are. However, the difficulty is that the length of the numbers is not the same. Some potential solutions are considered.</p><p>###</p><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Graphics Basis: Quaternian</title>
    <link href="/2020/05/09/Graphics-Basis-Quaternian/"/>
    <url>/2020/05/09/Graphics-Basis-Quaternian/</url>
    
    <content type="html"><![CDATA[<p>Another important concept in VR/AR is graphics. Rotation can be expressed using different forms. Rotation matrix is a common way in maths and physics. However, each matrix contains 9 parameters which make it computationally expensive. Also, in computer simulation or graphics, we normally consider rigid body rotation which means that the object does not deform during the rotation. (Additionally, other problems occur using other forms to express rotation).</p><h1 id="Quaternian"><a href="#Quaternian" class="headerlink" title="Quaternian"></a>Quaternian</h1><p>Quaternian is a useful way in computer graphics to express rotation. It is a generalisation of the idea of complex numbers. Suppose \(u_0\) is a scalar and \(\mathbf{u} = (u_1, u_2, u_3)\) is a vector. Then the 4-tuple \((u_0, u_1, u_2, u_3)\) represents a quaternian, defined as:<br>\[<br>  u = u_0 + \mathbf{u}<br>\]<br>or<br>\[<br>  u = u_0 + u_1\mathbf{i} + u_2\mathbf{j} + u_3\mathbf{k}<br>\]<br>where<br>\[<br>  \mathbf{i}^2 = \mathbf{j}^2 = \mathbf{k}^2 = \mathbf{i}\mathbf{j}\mathbf{k} = -1<br>\]<br>which is the same as the imaginary number. Also, multiplication between any two unit vector is the same as cross product, for example,<br>\[<br>  \mathbf{i}\mathbf{j} = \mathbf{i} \times \mathbf{j} = \mathbf{k}<br>\]</p><p>The conjugate of a quaternian \(u\) is<br>\[<br>  u^* = u_0 - \mathbf{u}<br>\]</p><p>A <em>pure quaternian</em> has its scalar part \(u_0 = 0\)</p><hr><h2 id="Polar-Representation-of-Quaternian"><a href="#Polar-Representation-of-Quaternian" class="headerlink" title="Polar Representation of Quaternian"></a>Polar Representation of Quaternian</h2><p>To understand the geometric interpretation of the quaternian, we need to have a look at its <strong>polar representation</strong>.</p><p>Let \(u\) be a unit quaternian, \(u = u_0 + \mathbf{u}\). Since \(u\) is unit, we have<br>\[<br>  |u|^2 = u_0^2 + |\mathbf{u}|^2 = 1 = cos^2(\theta) + sin^2(\theta)<br>\]<br>for some \(-\pi \le \theta \leqslant \pi\).</p><p>Also, we can define a unit vector that<br>\[<br>  \mathbf{s} = \frac{\mathbf{u}}{|\mathbf{u}|}<br>\]</p><p>Hence<br>\[<br>  u = u_0 + \mathbf{u} = u_0 + |\mathbf{u}|\mathbf{s}<br>\]<br>Also,<br>\[<br>  u = cos(\theta) + \mathbf{s} sin(\theta)<br>\]</p><hr><h2 id="Quaternian-Rotation"><a href="#Quaternian-Rotation" class="headerlink" title="Quaternian Rotation"></a>Quaternian Rotation</h2><p>Now we will show how to represent rotation with quaternian. Suppose \(\mathbf{p}\) is a vector or position, then \(p = 0 + \mathbf{p}\). Let \(u\) be any unit quaternian, such that \(u = cos(\theta) + \mathbf{s} sin(\theta)\). Then multiplication \(up u^* \) results in a pure quaternian \(q = 0 + \mathbf{q}\).</p><p><strong>\(\mathbf{q}\) is the rotation of \(\mathbf{p}\) about the axis \(\mathbf{s}\) by an angle of \(2\theta\)</strong>.</p><p>For example, to rotate \(\mathbf{p} = (x, y, z)\) about the \(Z\) axis (i.e. \(\mathbf{s} = (0, 0, 1)\)). Then<br>\[<br>  up = -(\mathbf{u} \cdot \mathbf{p}) + \mathbf{u} \times \mathbf{p},<br>\]<br>and<br>\[<br>  upu^* = (xcos2\theta - ysin2\theta, xsin2\theta + ycos2\theta, z)<br>\]</p><p>Suppose that there are a <strong>sequence</strong> of rotations, angle \(2\theta_i\), about axis \(\mathbf{s}_i\), for \(i = 1, …, n\). Then the result can be achieved by applying the quaternian rotation operator in sequence:<br>\[<br>  u_n u_{n-1} … u_1 p u_1^* … u_{n-1}^* u_n^*<br>\]<br>where \(u_i = cos\theta_i + \mathbf{s}_i sin\theta_i\)</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Slater, M., Steed, A. and Chrysanthou, Y., 2002. Computer Graphics And Virtual Environments. Wokingham: Addison-Wesley.</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation -&gt; Image Processing6: Edge Detection</title>
    <link href="/2020/05/01/Computer-Vision-Foundation-Image-Processing6-Edge-Detection/"/>
    <url>/2020/05/01/Computer-Vision-Foundation-Image-Processing6-Edge-Detection/</url>
    
    <content type="html"><![CDATA[<p>In this post, another basic image operation is explored - Edge Detection.</p><p>This post is split into four sections:</p><ol><li>The Basic Principles of Edge Detection</li><li>Practice with OpenCV in Python</li><li>Compute Sobel Edge Detection and Canny Edge Detection From Scratch</li></ol><p>Source code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; edge.py (OpenCV)</p><hr><h1 id="The-Basic-Principles-of-Edge-Detection"><a href="#The-Basic-Principles-of-Edge-Detection" class="headerlink" title="The Basic Principles of Edge Detection"></a>The Basic Principles of Edge Detection</h1><p>In an image, it is for us to realise edges by observing it. For a computer, the straight-forward way is to find changes of intensity. The change in intensity may be ascribed to two objects such that one is at foreground and the other is at background, or two objects with different reflection and absorption of light. Nevertheless, the change of intensity can be a great indicator of edges in the image.</p><p>We can easily find the changes by calculating the derivative of the light intensity across the image in both x and y directions. In image processing, as we do not care too much about the actual magnitudes of the derivative, we can use a discrete differentiation operator to approximate the derivatives. The operator for x direction is as<br>\[<br>G_x = \begin{bmatrix} -1 &amp; 0 &amp; +1 \\ -2 &amp; 0 &amp; +2 \\ -1 &amp; 0 &amp; +1 \end{bmatrix},<br>\]<br>which is symmetric in y direction. Imagine if one pixel with value, say, \(50\), and all the surrounding pixels also have pixel value \(50\), then the convolution will return \(0\) as plus and minus can cancel out each other. However, if one side is greater than the other side, then it will return a value. The signs of the return values are <strong>not</strong> important.</p><p>Symmetrically, for y direction, the operator is<br>\[<br>G_y = \begin{bmatrix} -1 &amp; -2 &amp; -1 \\ 0 &amp; 0 &amp; 0 \\ +1 &amp; +2 &amp; +1 \end{bmatrix},<br>\]</p><p>The edges may not lie perfectly in horizontal or vertical direction. Overall, our gradient operator is<br>\[<br>G = \sqrt(G_x^2 + G_y^2)<br>\]<br>Notice that \(G\), as expected, has no sign.</p><p>Based on \(G_x\) and \(G_y\), we can also calculate the direction of the gradient by using<br>\[<br>\theta = atan(\frac{G_y}{G_x})<br>\]</p><p>By convoluting the gradient operator with image intensity, we can find all the pixel position where there is a difference in intensity with its neighbours, no matter how small the difference is. But this operation can also pick up all the noise. Therefore, <em>Sobel Operator</em> is to run a small Gaussian filter first to smooth the image and then apply the gradient operator.</p><p><em>Canny Edge Detector</em> is a modified <em>Sobel Operator</em>. As we see above, <em>Sobel Operator</em> should give as a thick and board edges line as most of the change did not happen in one pixel, but a continuous change across many pixels. <em>Canny Edge Detector</em> is a double threshold method. First, set a minimum threshold, <code>minVal</code>, and a maximum threshold, <code>maxVal</code>. Any pixel with gradient value smaller than the <code>minVal</code> will be ignored, and any pixel with gradient value greater than the <code>maxVal</code> will be accepted. Any pixel with gradient in between will be selected: If they connect with the pixels that are greater than the <code>maxVal</code>, then they will be accepted. Otherwise, they will be ignored.</p><p>By doing so, we can find <em>sharp</em> edges only the trough will likely be ignored.</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><p>In OpenCV, we consider two functions for edge detection, <code>cv2.Sobel</code> and <code>cv2.Canny</code>.<br><em>Note</em> The input image should be a <em>Greyscale</em> image!!</p><pre><code>dst = cv2.Sobel(src, ddepth, dx, dy, dst, ksize, scale, delta, borderType)</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image.<br><strong>dst</strong> -&gt; Destination image.<br><strong>ddepth</strong> -&gt; (compulsory) Output image depth. The supported <strong>src.depth()</strong> and <strong>ddepth</strong> are:<br>  <code>src.depth()</code> = <code>CV_8U</code>, <code>ddepth</code> = <code>-1</code>/<code>CV_16S</code>/<code>CV_32F</code>/<code>CV_64F</code><br>  <code>src.depth()</code> = <code>CV_16U</code>/<code>CV_16S</code>, <code>ddepth</code> = <code>-1</code>/<code>CV_32F</code>/<code>CV_64F</code><br>  <code>src.depth()</code> = <code>CV_32F</code>, <code>ddepth</code> = <code>-1</code>/<code>CV_32F</code>/<code>CV_64F</code><br>  <code>src.depth()</code> = <code>CV_64F</code>, <code>ddepth</code> = <code>-1</code>/<code>CV_64F</code><br><em>Note</em> When <code>ddepth = -1</code>, the destination image will have the same depth as the source; in the case of 8-bit input images it will result in truncated derivatives.<br><strong>xorder</strong> -&gt; (compulsory) Order of the derivative x. Normally, \(0\) or \(1\).<br><strong>yorder</strong> -&gt; (compulsory) Order of the derivative y. Normally, \(0\) or \(1\).<br><strong>ksize</strong> -&gt; (optional) Size of the extended Sobel kernel; it must be 1, 3, 5, or 7. Default value is 3.<br><strong>scale</strong> -&gt; (optional) optional scale factor for the computed derivative values; by default, <em>no scaling</em> is applied<br><strong>delta</strong> -&gt; (optional) optional delta value that is added to the results prior to storing them in <strong>dst</strong>.<br><strong>borderType</strong> -&gt; (optional) pixel extrapolation method</p><pre><code>edges = cv2.Canny(image, threshold1, threshold2, edges, apertureSize, L2gradient)</code></pre><p><strong>image</strong> -&gt; (compulsory) Source image. 8-bit single-channel.<br><strong>edges</strong> -&gt; Output edge map; it has the same size and type as <strong>image</strong>.<br><strong>threshold1</strong> -&gt; (compulsory) first threshold for the hysteresis procedure.<br><strong>threshold2</strong> -&gt; (compulsory) second threshold for the hysteresis procedure.<br><strong>apertureSize</strong> -&gt; (optional) aperture size for the <code>Sobel()</code> operator. Default is \(3\).<br><strong>L2gradient</strong> -&gt; (optional) Default is false. (TODO: discuss in detail later)</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><pre><code>import cv2name = image_nameimg = cv2.imread(&#39;./{}.jpg&#39;.format(name), cv2.IMREAD_UNCHANGED)img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)cv2.imshow(&quot;Grey image&quot;, img)img = cv2.GaussianBlur(img, (3, 3), sigmaX=3, sigmaY=0)  # Uniform distorsion in x and y direcitonsobelx = cv2.Sobel(img,cv2.CV_64F,1,0)sobely = cv2.Sobel(img,cv2.CV_64F,0,1)edges = cv2.Canny(img,30,60)  # the threshold values should change accordingly.cv2.imshow(&quot;X edges&quot;, sobelx)cv2.imshow(&quot;Y edges&quot;, sobely)cv2.imshow(&quot;Canny&quot;, edges)cv2.waitKey(0)cv2.destroyAllWindows()</code></pre><p>Example results are shown below:<br><strong>Greyscale image</strong><br><img src="laptop_gray.jpg" srcset="/img/loading.gif" alt="grey"></p><p><strong>Edges in x direction using Sobel</strong><br><img src="laptop_x.jpg" srcset="/img/loading.gif" alt="x"></p><p><strong>Edges in y direction using Sobel</strong><br><img src="laptop_y.jpg" srcset="/img/loading.gif" alt="y"></p><p>Compare result from Sobel with Canny.<br><strong>Canny Detector</strong><br><img src="laptop_edges.jpg" srcset="/img/loading.gif" alt="edges"></p><hr><h1 id="Compute-Sobel-and-Canny-From-Scratch-TODO"><a href="#Compute-Sobel-and-Canny-From-Scratch-TODO" class="headerlink" title="Compute Sobel and Canny From Scratch (TODO)"></a>Compute Sobel and Canny From Scratch (TODO)</h1><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Photo by Omid Armin on Unsplash</li><li>Docs.opencv.org. 2020. Sobel Derivatives — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/sobel_derivatives/sobel_derivatives.html" target="_blank" rel="noopener">https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/sobel_derivatives/sobel_derivatives.html</a> [Accessed 1 May 2020].</li><li>Opencv-python-tutroals.readthedocs.io. 2020. Image Gradients — Opencv-Python Tutorials 1 Documentation. [online] Available at: <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_gradients/py_gradients.html" target="_blank" rel="noopener">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_gradients/py_gradients.html</a> [Accessed 1 May 2020].</li><li>Opencv-python-tutroals.readthedocs.io. 2020. Canny Edge Detection — Opencv-Python Tutorials 1 Documentation. [online] Available at: <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html" target="_blank" rel="noopener">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html</a> [Accessed 1 May 2020].</li><li>Docs.opencv.org. 2020. Image Filtering — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html?highlight=sobel#void%20Sobel(InputArray%20src,%20OutputArray%20dst,%20int%20ddepth,%20int%20dx,%20int%20dy,%20int%20ksize,%20double%20scale,%20double%20delta,%20int%20borderType)" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html?highlight=sobel#void%20Sobel(InputArray%20src,%20OutputArray%20dst,%20int%20ddepth,%20int%20dx,%20int%20dy,%20int%20ksize,%20double%20scale,%20double%20delta,%20int%20borderType)</a> [Accessed 1 May 2020].</li><li>Docs.opencv.org. 2020. Feature Detection — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=canny" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=canny</a> [Accessed 1 May 2020].</li></ol><h3 id="Two-Additional-materials-that-I-really-recommended"><a href="#Two-Additional-materials-that-I-really-recommended" class="headerlink" title="Two Additional materials that I really recommended"></a>Two Additional materials that I really recommended</h3><p>Finding the Edges (Sobel Operator) - Computerphile -&gt; <a href="https://www.youtube.com/watch?v=uihBwtPIBxM" target="_blank" rel="noopener">https://www.youtube.com/watch?v=uihBwtPIBxM</a><br>Canny Edge Detector - Computerphile -&gt; <a href="https://www.youtube.com/watch?v=sRFM5IEqR2w" target="_blank" rel="noopener">https://www.youtube.com/watch?v=sRFM5IEqR2w</a></p><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation -&gt; Image Processing5: Image Thresholding</title>
    <link href="/2020/04/28/Computer-Vision-Foundation-Image-Processing5-Image-Thresholding/"/>
    <url>/2020/04/28/Computer-Vision-Foundation-Image-Processing5-Image-Thresholding/</url>
    
    <content type="html"><![CDATA[<p>In this post, another basic image operation is explored - Image Thresholding.</p><p>This post is split into four sections:</p><ol><li>The Basic Principles of Image Thresholding</li><li>Practice with OpenCV in Python</li><li>Compute Otsu’s Algorithm and <em>adaptiveThreshold</em> From Scratch</li></ol><p>Source code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; threshold.py (OpenCV)</p><hr><h1 id="The-Basic-Principles-of-Image-Thresholding"><a href="#The-Basic-Principles-of-Image-Thresholding" class="headerlink" title="The Basic Principles of Image Thresholding"></a>The Basic Principles of Image Thresholding</h1><p>Image Thresholding is the simplest segmentation method. This separation is based on the variation of intensity between the object pixels and the background pixels.</p><p>To do that, we compare each pixel value with respect to a set <em>threshold</em>. If one pixel is above the threshold, we can assign a value, otherwise we can assign another value. (e.g. \(255\) if it is above the threshold, \(0\) otherwise)</p><p>However, manually selecting the threshold can be problematic and requires many times of trail-and-error. A useful method is the Otsu Thresholding.</p><p>Otsu’s method was named after its inventor Nobuyuki Otsu. Otsu’s thresholding method involves iterating through all the possible threshold values (\(0\) to \(255\)) and calculating a measure of spread for the pixel levels each side of the threshold. The main purpose is to separate all the pixels into foreground and background, and find the threshold value where the sum of foreground and background spreads is at its minimum. Statistically, the spread is measured by the variance.</p><p>The next step is to calculate the <em>‘Within-Class Variance’</em>. This is simply the sum of the two variances multiplied by their associated weights. Mathematically,<br>\[<br>  \sigma^2 = W_b \sigma^2_b + W_f \sigma^2_f,<br>\]<br>where \(W_b\) and \(W_f\) denote the weights of background and foreground pixels respectively. Besides,<br>\[<br>  W_b + W_f = 1<br>\]<br>as normal.</p><p>Here is an example. The set-up is shown below.<br><img src="otsuOrig.png" srcset="/img/loading.gif" alt="otsu_eg"><br>Then, we can use Otsu’s method and obtain that when the threshold is at \(3\), the <em>‘Within-Class Variance’</em> is at its minimum.<br><img src="otsu_process.png" srcset="/img/loading.gif" alt="otsu_pro"></p><p>The problems of this approach are that the result highly related to the light conditions, and it works only if there are two distinct peaks in the histogram of the image theoretically. If the ambient light does not shine uniformly, this method simply separate the dark side and the bright side. If the histogram of the image has multiple peaks, Otsu’s method still only separate into two because of the theory.</p><p>To overcome those problems, Adaptive Threshold method can be used. Basically, it calculate the threshold for each pixel according to its neighbourhood using the mean of their intensities or the Gaussian distribution of the intensities.</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><p>In OpenCV, we consider two functions for image thresholding, <code>cv2.threshold</code> and <code>cv2.adaptiveThreshold</code>.</p><pre><code>retval, dst = cv2.threshold(src, thresh, maxval, type)</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image. Single-channel, 8-bit or 32-bit floating point.<br><strong>dst</strong> -&gt; Destination image.<br><strong>thresh</strong> -&gt; (compulsory) Threshold value.<br><strong>maxval</strong> -&gt; (compulsory) The value assigned to the pixel after comparing with the threshold value if <strong>THRESH_BINARY</strong> or <strong>THRESH_BINARY_INV</strong> is used.<br><strong>type</strong> -&gt; (compulsory) Thresholding type.</p><p><em>Note</em> The input image should be a <em>Greyscale</em> image!!</p><p>Some basic thresholding types, which are applied in OpenCV, are<br>  cv2.THRESH_BINARY<br>  cv2.THRESH_BINARY_INV<br>  cv2.THRESH_TRUNC<br>  cv2.THRESH_TOZERO<br>  cv2.THRESH_TOZERO_INV<br>The results using different types are shown below.<br><img src="threshold.jpg" srcset="/img/loading.gif" alt="result"></p><p>Next, the details of each type will be presented. Suppose that the histogram of the image and the threshold are as shown.<br><img src="Threshold_Tutorial_Theory_Base_Figure.png" srcset="/img/loading.gif" alt="base"><br>The red is the intensity distribution and the blue line is the threshold.</p><h4 id="THRESH-BINARY"><a href="#THRESH-BINARY" class="headerlink" title="THRESH_BINARY"></a>THRESH_BINARY</h4><p>\[<br>  dst(x, y) = maxval \space \space \mbox{if } src(x, y) &gt; thresh<br>\]<br>\[<br>  dst(x, y) = 0 \space \space \mbox{otherwise}<br>\]<br><img src="Threshold_Tutorial_Theory_Binary.png" srcset="/img/loading.gif" alt="binary"></p><h4 id="THRESH-BINARY-INV"><a href="#THRESH-BINARY-INV" class="headerlink" title="THRESH_BINARY_INV"></a>THRESH_BINARY_INV</h4><p>\[<br>  dst(x, y) = 0 \space \space \mbox{if } src(x, y) &gt; thresh<br>\]<br>\[<br>  dst(x, y) = maxval \space \space \mbox{otherwise}<br>\]<br><img src="Threshold_Tutorial_Theory_Binary_Inverted.png" srcset="/img/loading.gif" alt="binar_inv"></p><h4 id="THRESH-TRUNC"><a href="#THRESH-TRUNC" class="headerlink" title="THRESH_TRUNC"></a>THRESH_TRUNC</h4><p>\[<br>  dst(x, y) = maxval \space \space \mbox{if } src(x, y) &gt; thresh<br>\]<br>\[<br>  dst(x, y) = src(x, y) \space \space \mbox{otherwise}<br>\]<br><img src="Threshold_Tutorial_Theory_Truncate.png" srcset="/img/loading.gif" alt="trunc"></p><h4 id="THRESH-TOZERO"><a href="#THRESH-TOZERO" class="headerlink" title="THRESH_TOZERO"></a>THRESH_TOZERO</h4><p>\[<br>  dst(x, y) = src(x, y) \space \space \mbox{if } src(x, y) &gt; thresh<br>\]<br>\[<br>  dst(x, y) = 0 \space \space \mbox{otherwise}<br>\]<br><img src="Threshold_Tutorial_Theory_Zero.png" srcset="/img/loading.gif" alt="tozero"></p><h4 id="THRESH-TOZERO-INV"><a href="#THRESH-TOZERO-INV" class="headerlink" title="THRESH_TOZERO_INV"></a>THRESH_TOZERO_INV</h4><p>\[<br>  dst(x, y) = 0 \space \space \mbox{if } src(x, y) &gt; thresh<br>\]<br>\[<br>  dst(x, y) = src(x, y) \space \space \mbox{otherwise}<br>\]<br><img src="Threshold_Tutorial_Theory_Zero_Inverted.png" srcset="/img/loading.gif" alt="tozero_inv"></p><p><em>Note</em> To use Otsu thresholding, pass <code>thresh = 0</code> and <code>type = (...threshold type...)+cv2.THRESH_OTSU</code>.</p><pre><code>dst = cv2.adaptiveThreshold(src, maxValue, adaptiveMethod, thresholdType, blockSize, C)</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image. 8-bit single-channel.<br><strong>dst</strong> -&gt; Destination image.<br><strong>maxValue</strong> -&gt; (compulsory) Non-zero value assigned to the pixels for which the condition is satisfied.<br><strong>adaptiveMethod</strong> -&gt; (compulsory) Adaptive thresholding algorithm to use, <strong>ADAPTIVE_THRESH_MEAN_C</strong> or <strong>ADAPTIVE_THRESH_GAUSSIAN_C</strong>.<br><strong>thresholdType</strong> -&gt; (compulsory) Thresholding type that must be either <strong>THRESH_BINARY</strong> or <strong>THRESH_BINARY_INV</strong>.<br><strong>blockSize</strong> -&gt; (compulsory) Size of a pixel neighbourhood that is used to calculate a threshold value for the pixel: 3, 5, and so on.<br><strong>C</strong> -&gt; (compulsory) Constant subtracted from the mean or weighted mean. It is normally <em>positive</em> but may be zero or negative as well.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><pre><code>import cv2name = image_nameimg = cv2.imread(&#39;./{}.jpg&#39;.format(name), cv2.IMREAD_UNCHANGED)img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)cv2.imshow(&quot;Grey image&quot;, img)# Simple image thresholding with your defined value (in this case, 127)_, simple = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)cv2.imshow(&quot;Simple thresholding image&quot;, simple)# Otsu thresholding (retval is the threshold found with Otsu Algorithm)retval, otsu = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)cv2.imshow(&quot;Otsu thresholding image&quot;, otsu)print(retval)# Mean Adaptive Thresholding and Gaussian Adaptive Thresholdingmean = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)gauss = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)cv2.imshow(&quot;Mean Adaptive Thresholding image&quot;, mean)cv2.imshow(&quot;Gaussian Adaptive Thresholding image&quot;, gauss)cv2.waitKey(0)cv2.destroyAllWindows()</code></pre><p>Example results are shown below:<br><strong>Greyscale image</strong><br><img src="laptop_original.jpg" srcset="/img/loading.gif" alt="grey"></p><p><strong>Simple Thresholding with Threshold = 127</strong><br><img src="laptop_simple_threshold.jpg" srcset="/img/loading.gif" alt="simple"></p><p><strong>Otsu Thresholding with Threshold = 48)</strong><br><img src="laptop_otsu.jpg" srcset="/img/loading.gif" alt="otsu"></p><p>Compare result from Otsu thresholding with next two.<br><strong>Mean Adaptive Thresholding</strong><br><img src="laptop_mean_adaptive_threshold.jpg" srcset="/img/loading.gif" alt="mean"></p><p><strong>Gaussian Adaptive Thresholding</strong><br><img src="laptop_gaussian_adaptive_threshold.jpg" srcset="/img/loading.gif" alt="gauss"></p><h3 id="Adaptive-Thresholding-Parameters"><a href="#Adaptive-Thresholding-Parameters" class="headerlink" title="Adaptive Thresholding Parameters"></a>Adaptive Thresholding Parameters</h3><p>Two important parameters in adaptive thresholding are <strong>blockSize</strong> and <strong>C</strong>. Here, some results from mean adaptive thresholding with different parameters are presented for comparison.</p><h4 id="blockSize"><a href="#blockSize" class="headerlink" title="blockSize"></a>blockSize</h4><p><strong>Adaptive Thresholding with blockSize = 3</strong><br><img src="laptop_mean_Kernel_size_3.jpg" srcset="/img/loading.gif" alt="3"></p><p><strong>Adaptive Thresholding with blockSize = 9</strong><br><img src="laptop_mean_Kernel_size_9.jpg" srcset="/img/loading.gif" alt="9"></p><p><strong>Adaptive Thresholding with blockSize = 15</strong><br><img src="laptop_mean_Kernel_size_15.jpg" srcset="/img/loading.gif" alt="15"></p><h4 id="C"><a href="#C" class="headerlink" title="C"></a>C</h4><p><strong>Adaptive Thresholding with C = -20</strong><br><img src="laptop_mean_c_-20.jpg" srcset="/img/loading.gif" alt="minus20"></p><p><strong>Adaptive Thresholding with C = -10</strong><br><img src="laptop_mean_c_-10.jpg" srcset="/img/loading.gif" alt="minus10"></p><p><strong>Adaptive Thresholding with C = 0</strong><br><img src="laptop_mean_c_0.jpg" srcset="/img/loading.gif" alt="0"></p><p><strong>Adaptive Thresholding with C = 10</strong><br><img src="laptop_mean_c_10.jpg" srcset="/img/loading.gif" alt="10"></p><p><strong>Adaptive Thresholding with C = 20</strong><br><img src="laptop_mean_c_20.jpg" srcset="/img/loading.gif" alt="20"></p><hr><h1 id="Compute-Otsu’s-Algorithm-From-Scratch-TODO"><a href="#Compute-Otsu’s-Algorithm-From-Scratch-TODO" class="headerlink" title="Compute Otsu’s Algorithm From Scratch (TODO)"></a>Compute Otsu’s Algorithm From Scratch (TODO)</h1><h2 id="Also-some-optimisations-available-for-Otsu’s-Thresholding-exists-Search-and-implement-it"><a href="#Also-some-optimisations-available-for-Otsu’s-Thresholding-exists-Search-and-implement-it" class="headerlink" title="Also, some optimisations available for Otsu’s Thresholding exists. Search and implement it."></a>Also, some optimisations available for Otsu’s Thresholding exists. Search and implement it.</h2><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Photo by Omid Armin on Unsplash</li><li>Blog.csdn.net. 2020. OTSU算法（大津法—最大类间方差法）原理及实现_人工智能_小武的博客-CSDN博客. [online] Available at: <a href="https://blog.csdn.net/weixin_40647819/article/details/90179953" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40647819/article/details/90179953</a> [Accessed 29 April 2020].</li><li>Blog.csdn.net. 2020. 自适应阈值（Adaptivethreshold）分割原理及实现_人工智能_小武的博客-CSDN博客. [online] Available at: <a href="https://blog.csdn.net/weixin_40647819/article/details/90213858" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40647819/article/details/90213858</a> [Accessed 29 April 2020].</li><li>Docs.opencv.org. 2020. Basic Thresholding Operations — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/doc/tutorials/imgproc/threshold/threshold.html" target="_blank" rel="noopener">https://docs.opencv.org/2.4/doc/tutorials/imgproc/threshold/threshold.html</a> [Accessed 29 April 2020].</li><li>Opencv-python-tutroals.readthedocs.io. 2020. Image Thresholding — Opencv-Python Tutorials 1 Documentation. [online] Available at: <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html#exercises" target="_blank" rel="noopener">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html#exercises</a> [Accessed 29 April 2020].</li><li>Labbookpages.co.uk. 2020. Otsu Thresholding - The Lab Book Pages. [online] Available at: <a href="http://www.labbookpages.co.uk/software/imgProc/otsuThreshold.html" target="_blank" rel="noopener">http://www.labbookpages.co.uk/software/imgProc/otsuThreshold.html</a> [Accessed 29 April 2020].</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation -&gt; Image Processing4: Image Filtering</title>
    <link href="/2020/04/26/Computer-Vision-Foundation-Image-Processing4-Image-Filtering/"/>
    <url>/2020/04/26/Computer-Vision-Foundation-Image-Processing4-Image-Filtering/</url>
    
    <content type="html"><![CDATA[<p>In this post, another basic image operation is explored - Image Filtering.</p><p>This post is split into four sections:</p><ol><li>The Basic Principles of Image Filtering</li><li>Details of <em>blur</em>, <em>boxFilter</em> and <em>GaussianBlur</em> Functions</li><li>Practice with OpenCV in Python</li><li>Build <em>boxFilter</em> and <em>GaussianBlur</em> Functions From Scratch</li></ol><p>Source code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; filter.py (OpenCV)</p><hr><h1 id="The-Basic-Principles-of-Image-Filtering"><a href="#The-Basic-Principles-of-Image-Filtering" class="headerlink" title="The Basic Principles of Image Filtering"></a>The Basic Principles of Image Filtering</h1><p>In image processing, filters are mainly used to suppress either the high frequencies in the image (<em>smoothing the image</em>) or the low frequencies (<em>enhancing or detecting edges</em>) in the image.</p><p>An image can be filtered either in the <strong>frequency</strong> or in the <strong>spatial</strong> domain.</p><p>To operate in the frequency domain, we should first extract frequencies of the image, then multiple by a value. For example, to smooth the image, we multiple \(0\) to the high frequencies, and \(1\) to the low frequencies. Therefore, high frequencies can be filtered out. Final, we need to transform back to spatial domain to get the image. (c.f. Fourier Transform)</p><p>In the spatial domain, convolution is equivalent to multiplication in the frequency domain. (Proof in Appendix) Mathematically,<br>\[<br>g(i, j) = h(i, j) * f(i, j),<br>\]<br>where, \(g(i, j)\) is the resulted value at point \((i, j)\), \(h(i, j)\) is the filter function and \(f(i, j)\) is the input image.</p><p>The methods discussed above are equivalent in maths. But the results of the implementations in computer are different, since we have to <em>approximate</em> the filter function with a discrete and finite <strong>kernel</strong>.</p><p><em>Note</em> A <strong>kernel</strong> is (usually) a smallish matrix of numbers that is used in image convolutions. Differently sized kernels containing different patterns of numbers give rise to different results under convolution. The word <strong>kernel</strong> is also commonly used as a synonym for <strong>structuring element</strong>, which is a similar object used in mathematical morphology. A structuring element differs from a kernel in that it also has a specified <em>origin</em>.</p><p>The discrete convolution can be defined as a <em>‘shift and multiply’</em> operation, where we shift the kernel over the image and multiply its value with the corresponding pixel values of the image. For a square kernel with size \((M \times M)\), we can calculate the output image with the following formula:<br>\[<br>g(i, j) = \sum_{m = -\frac{M}{2}}^{\frac{M}{2}} \sum_{n = -\frac{M}{2}}^{\frac{M}{2}} h(m, n) f(i - m, j - n),<br>\]</p><p>Also, it is intuitive to apply some non-linear filter in spatial domain.</p><p><strong>Linear Filter</strong>: Box Filter; Gaussian Filter; Laplacian Filter.<br><strong>Non-linear Filter</strong>: Median Filter.</p><hr><h1 id="Details-of-Two-Main-Operations"><a href="#Details-of-Two-Main-Operations" class="headerlink" title="Details of Two Main Operations"></a>Details of Two Main Operations</h1><h3 id="Box-Filter"><a href="#Box-Filter" class="headerlink" title="Box Filter"></a>Box Filter</h3><p>Mean filtering is a simple, intuitive and easy to implement method of smoothing images, i.e. reducing the amount of intensity variation between one pixel and the next.</p><p>Mean filter is a special type of box filter. The main purpose of the mean filter is to blur the unessential parts of the image while keeping the essential features. The <em>unessential</em> parts are the details which are smaller than the kernel in size.</p><p>The drawback of the mean filter is that it is not good at filtering out the noise points from the image.</p><h3 id="Gaussian-Filter"><a href="#Gaussian-Filter" class="headerlink" title="Gaussian Filter"></a>Gaussian Filter</h3><p>The kernel matches to a 2D Gaussian distribution with equation:<br>\[<br>h(x, y) = \frac{1}{2 \pi \sigma^2}e^{-\frac{x^2+y^2}{2\sigma^2}},<br>\]</p><p>Sometimes, for the ease of computation, the first constant term can be ignored.</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><h4 id="Box-Filter-1"><a href="#Box-Filter-1" class="headerlink" title="Box Filter"></a>Box Filter</h4><p>In OpenCV, box filter can be applied with two different functions, <code>cv2.blur</code> and <code>cv2.boxFilter</code>. If a <strong>normalised</strong> box filter should be used, then these two functions are equivalent. But if an <strong>unnormalised</strong> box filter should be used, then you should consider <code>cv2.boxFilter</code> only.</p><p>First is <code>cv2.blur</code></p><pre><code>dst = cv2.blur(src, ksize[, dst[, anchor[, borderType]]])</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image. it can have <em>any</em> number of channels, but the depth should be <code>CV_8U</code>, <code>CV_16U</code>, <code>CV_16S</code>, <code>CV_32F</code> or <code>CV_64F</code>.<br><strong>dst</strong> -&gt; Destination image.<br><strong>ksize</strong> -&gt; (compulsory) blurring kernel size.<br><strong>anchor</strong> -&gt; (optional) anchor point. Default value \(Point(-1,-1)\) means that the anchor is at the kernel centre.<br><strong>borderType</strong> -&gt; (optional) border mode used to extrapolate pixels outside of the image.</p><p>Next is <code>cv2.boxFilter</code></p><pre><code>dst = cv2.boxFilter(src, ddepth, ksize[, dst[, anchor[, normalize[, borderType]]]])</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image.<br><strong>dst</strong> -&gt; Destination image.<br><strong>ddepth</strong> -&gt; (compulsory) the output image depth. If the input is \(-1\), then <code>src.depth()</code> is used.<br><strong>ksize</strong> -&gt; (compulsory) blurring kernel size.<br><strong>anchor</strong> -&gt; (optional) anchor point. Default value \(Point(-1,-1)\) means that the anchor is at the kernel centre.<br><strong>normalize</strong> -&gt; (optional) To specify whether the kernel is normalised by its area. Default is <em>True</em><br><strong>borderType</strong> -&gt; (optional) border mode used to extrapolate pixels outside of the image.</p><h4 id="Gaussian-Filter-1"><a href="#Gaussian-Filter-1" class="headerlink" title="Gaussian Filter"></a>Gaussian Filter</h4><pre><code>dst = cv2.GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]])</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image. it can have <em>any</em> number of channels, but the depth should be <code>CV_8U</code>, <code>CV_16U</code>, <code>CV_16S</code>, <code>CV_32F</code> or <code>CV_64F</code>.<br><strong>dst</strong> -&gt; Destination image.<br><strong>ksize</strong> -&gt; (compulsory) Gaussian kernel size. <code>ksize.width</code> and <code>ksize.height</code> can differ but they both must be <em>positive</em> and <em>odd</em>.<br><strong>sigmaX</strong> -&gt; (compulsory) Gaussian kernel standard deviation in X direction.<br><strong>sigmaY</strong> -&gt; (optional) Gaussian kernel standard deviation in Y direction. Default value is \(0\), which means it is the same as the <strong>sigmaX</strong>.<br><strong>borderType</strong> -&gt; (optional) border mode used to extrapolate pixels outside of the image.</p><p><em>Note</em> If <strong>sigmaX</strong> and <strong>sigmaY</strong> are both \(0\), then they will be automatically calculated from <code>ksize.width</code> and <code>ksize.height</code> respectively. However, it is recommended to specify <em>all</em> of <strong>ksize</strong>, <strong>sigmaX</strong> and <strong>sigmaY</strong>.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><pre><code>import cv2name = image_nameimg = cv2.imread(&#39;./{}.jpg&#39;.format(name), cv2.IMREAD_UNCHANGED)cv2.imshow(&quot;Original image&quot;, img)# Box Filter using two different methodsuni_blur_less = cv2.blur(img, (5, 5))uni_blur_more = cv2.boxFilter(img, -1, (9, 9))# Gaussian Filter with different sigmagauss_blur_uniform = cv2.GaussianBlur(img, (9, 9), sigmaX=5, sigmaY=0)  # Uniform distorsion in x and y direcitongauss_blur_x = cv2.GaussianBlur(img, (9, 9), sigmaX=10, sigmaY=1)  # Main distorsion in x direcitongauss_blur_y = cv2.GaussianBlur(img, (9, 9), sigmaX=1, sigmaY=10)  # Main distorsion in y direcitoncv2.imshow(&quot;Blur image with small kernel&quot;, uni_blur_less)cv2.imshow(&quot;Blur image with large kernel&quot;, uni_blur_more)cv2.imshow(&quot;Blur image with gaussian kernel (sigmaX = sigmaY)&quot;, gauss_blur_uniform)cv2.imshow(&quot;Blur image with gaussian kernel (sigmaX &gt; sigmaY)&quot;, gauss_blur_x)cv2.imshow(&quot;Blur image with gaussian kernel (sigmaX &lt; sigmaY)&quot;, gauss_blur_y)cv2.waitKey(0)cv2.destroyAllWindows()</code></pre><p>Example results are shown below:</p><h4 id="Box-Filter-2"><a href="#Box-Filter-2" class="headerlink" title="Box Filter"></a>Box Filter</h4><p><strong>Less Blurring</strong><br><img src="flower_less_blur.jpg" srcset="/img/loading.gif" alt="less"></p><p><strong>More Blurring</strong><br><img src="flower_more_blur.jpg" srcset="/img/loading.gif" alt="more"></p><h4 id="Gaussian-Filter-2"><a href="#Gaussian-Filter-2" class="headerlink" title="Gaussian Filter"></a>Gaussian Filter</h4><p><strong>sigmaX = sigmaY</strong><br><img src="flower_gauss_blur_uniform.jpg" srcset="/img/loading.gif" alt="uni"></p><p><strong>sigmaX &gt; sigmaY</strong><br><img src="flower_gauss_blur_x.jpg" srcset="/img/loading.gif" alt="x"></p><p><strong>sigmaX &lt; sigmaY</strong><br><img src="flower_gauss_blur_y.jpg" srcset="/img/loading.gif" alt="y"></p><hr><h1 id="Build-boxFilter-and-GaussianBlur-Functions-From-Scratch-TODO"><a href="#Build-boxFilter-and-GaussianBlur-Functions-From-Scratch-TODO" class="headerlink" title="Build boxFilter and GaussianBlur Functions From Scratch (TODO)"></a>Build <em>boxFilter</em> and <em>GaussianBlur</em> Functions From Scratch (TODO)</h1><hr><h1 id="Option-Other-Image-Filtering-TODO"><a href="#Option-Other-Image-Filtering-TODO" class="headerlink" title="(Option) Other Image Filtering (TODO)"></a>(Option) Other Image Filtering (TODO)</h1><hr><h1 id="Option-Test-denoising-for-different-filters-TODO"><a href="#Option-Test-denoising-for-different-filters-TODO" class="headerlink" title="(Option) Test denoising for different filters (TODO)"></a>(Option) Test denoising for different filters (TODO)</h1><hr><h1 id="Appendix-Proof-of-The-convolution-theorem-TODO"><a href="#Appendix-Proof-of-The-convolution-theorem-TODO" class="headerlink" title="Appendix - Proof of The convolution theorem (TODO)"></a>Appendix - Proof of The convolution theorem (TODO)</h1><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Photo by Viktor Mogilat on Unsplash.</li><li>Blog.csdn.net. 2020. 均值滤波原理及C++实现_C/C++小武的博客-CSDN博客. [online] Available at: <a href="https://blog.csdn.net/weixin_40647819/article/details/88774522" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40647819/article/details/88774522</a> [Accessed 27 April 2020].</li><li>Homepages.inf.ed.ac.uk. 2020. Digital Filters. [online] Available at: <a href="https://homepages.inf.ed.ac.uk/rbf/HIPR2/filtops.htm" target="_blank" rel="noopener">https://homepages.inf.ed.ac.uk/rbf/HIPR2/filtops.htm</a> [Accessed 27 April 2020].</li><li>Docs.opencv.org. 2020. Image Filtering — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html</a> [Accessed 27 April 2020].</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation -&gt; Image Processing3: Colour Conversion</title>
    <link href="/2020/04/24/Computer-Vision-Foundation-Image-Processing3-Colour-Conversion/"/>
    <url>/2020/04/24/Computer-Vision-Foundation-Image-Processing3-Colour-Conversion/</url>
    
    <content type="html"><![CDATA[<p>In this post, another basic image operation is explored - Colour conversion.</p><p>This post is split into four sections:</p><ol><li>The Basic Principles of Colour Conversion</li><li>Details of <em>RGB2GRAY</em> and <em>RGB2HSV</em> Functions and Their Inverse Functions</li><li>Practice with OpenCV in Python</li><li>Build <em>RGB2GRAY</em> and <em>RGB2HSV</em> Functions and Their Inverse Functions From Scratch</li></ol><p>OpenCV code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; colour.py; gray.py (My <em>RGB2GRAY</em>, display using OpenCV)<br>Re-implementation code: <a href="https://github.com/BillMaZengou/cv_basis/my_opencv" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis/my_opencv</a> -&gt; grey.py (My <em>RGB2GRAY</em>)</p><hr><h1 id="The-Basic-Principles-of-Colour-Conversion"><a href="#The-Basic-Principles-of-Colour-Conversion" class="headerlink" title="The Basic Principles of Colour Conversion"></a>The Basic Principles of Colour Conversion</h1><h3 id="Greyscale"><a href="#Greyscale" class="headerlink" title="Greyscale"></a>Greyscale</h3><p>A greyscale image is one in which the value of each pixel represents an amount of light. In other words, it carries only intensity information. The contrast ranges from black at the weakest intensity to white at the strongest.</p><p>Although the conversion from RGB to grey image can easily be done, sadly there is no simple explanation. Compare the following images<br><img src="laptop_original.jpg" srcset="/img/loading.gif" alt="laptop"><br><img src="laptop_Mean_Grey.jpg" srcset="/img/loading.gif" alt="laptop_mean"><br><img src="laptop_RGB_to_Grey.jpg" srcset="/img/loading.gif" alt="laptop_grey"><br>The second one was obtained by <strong>averaging RGB channels</strong>. The third one was obtained using <strong>OpenCV</strong> standard, which will be introduced in later sessions.</p><p>The goal of the conversion is to find the balance point between human perception and computational ease.</p><h4 id="Perceptual-Luminance-preserving-Conversion-to-Greyscale"><a href="#Perceptual-Luminance-preserving-Conversion-to-Greyscale" class="headerlink" title="Perceptual Luminance-preserving Conversion to Greyscale"></a>Perceptual Luminance-preserving Conversion to Greyscale</h4><p>The best conversion should provide the greyscale image with the same luminance as the original colour image. Details can conduct (Grayscale, 2020, <a href="https://en.wikipedia.org/wiki/Grayscale" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Grayscale</a>).</p><p>In standard,<br>\[<br>Y_{linear} = 0.2126 R_{linear} + 0.7152 G_{linear} + 0.0722 B_{linear},<br>\]<br>where \(Y_{linear}\) denotes linear luminance, and \(R_{linear}\), \(G_{linear}\) and \(B_{linear}\) represent linear luminance of each colour channel respectively. These three coefficients are the intensity perception of a normal person with the definition of <em>sRGB</em>.<br><strong>Human vision is most sensitive to green, so this has the greatest coefficient value (0.7152), and least sensitive to blue, so this has the smallest coefficient (0.0722)</strong>.</p><p>However, for a typical RGB image, the colour channels do not store linear luminance. The RGB values are gamma-compressed. Details are in (Gamma correction, 2020, <a href="https://en.wikipedia.org/wiki/Gamma_correction" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Gamma_correction</a>). To simplify, psychophysics suggested that human perception of physical quantities are non-linear. Empirically, it follows Stevens’s power law. Hence, if an image is made without power law, then we will feel unnatural since it will not match to our observation by eyes. Gamma compression is a correction to make the image fit to the power law.</p><p>Solving power equations are computationally expensive. Therefore, the <em>RGB2GRAY</em> operation in everyday life is merely an approximation.</p><h3 id="HSV"><a href="#HSV" class="headerlink" title="HSV"></a>HSV</h3><p>Most of the digital displays produce colours by combining red, green, and blue light with various intensities. Red, green and blue are so-called RGB additive primary colours. The resulting mixtures in RGB colour space can reproduce a wide variety of colours called a <strong>gamut</strong>. Gamut, or colour gamut, is a <em>complete subset</em> of colours, shown below.<br><img src="gamut.png" srcset="/img/loading.gif" alt="gamut"><br><em>CIE 1931 xy chromaticity diagram</em> showing the gamut of the <em>sRGB</em> colour space (the triangle). The outer curved boundary is the monochromatic spectrum with wavelengths shown in nanometers labeled in blue. This image is drawn using sRGB, so colours outside the triangle cannot be accurately coloured. The <em>D65 white point</em> is shown in the centre, and the <em>Planckian locus</em> is shown with colour temperatures labeled in kelvins. D65 is not an ideal 6500-kelvin blackbody because it is based on atmospheric filtered daylight.</p><p>The problem is that using RGB to generate colour is not intuitive. For example, as shown in the figure below, changing from one orange to a less saturated orange requires to modified RGB by different amounts.<br><img src="colorchange.png" srcset="/img/loading.gif" alt="colour_change"></p><p>In the mid-1970s Alvy Ray Smith described the <strong>HSV</strong> model for computer display technology to accommodate more intuitive colour mixing models. These models were useful because they were not only more intuitive than raw RGB values, but also the conversions to and from RGB were extremely fast to compute. <strong>They could run in real time on the hardware of the 1970s</strong>.<br><img src="hsv.jpg" srcset="/img/loading.gif" alt="hsv"><br>HSV stands for “hue, saturation, value”. The model can be shown in a cone. To make them simple,</p><p>  Hue: The angle of a circle, which is the cross-section of the HSV cone. The degrees indicate different colours. By convention, Red is at \(0\), green is at \(120\), and blue is at \(240\).</p><p>  Saturation: The radius of a circle, which is the cross-section of the HSV cone. The lengths indicate different colourfulness.</p><p>  Value: The height of the HSV cone. The lengths indicate different brightness.</p><hr><h1 id="Details-of-Two-Main-Operations"><a href="#Details-of-Two-Main-Operations" class="headerlink" title="Details of Two Main Operations"></a>Details of Two Main Operations</h1><h3 id="RGB-lt-–-gt-Grey"><a href="#RGB-lt-–-gt-Grey" class="headerlink" title="RGB &lt;–&gt; Grey"></a>RGB &lt;–&gt; Grey</h3><p>The conversion from RGB to Grey image is<br>\[<br>Y = 0.299 R + 0.587 G + 0.114 B,<br>\]<br>where \(Y\) is the greyscale value, and \(R\), \(G\), \(B\) denotes RGB values for each channel respectively.<br>As mentioned previously, this equation is an approximation of the accurate conversion. The equation works for digital formats following <strong>Rec. 601</strong> (i.e. most digital standard definition formats).</p><p><em>Note</em> The \(Y\) is gamma compressed, thus we do not need to worry about the power law.</p><p>The conversion from Grey to RGB image is<br>\[<br>R = Y; G = Y; B = Y.<br>\]<br>This operation cannot produce a colourful image, but simply create a RGB image form.</p><h3 id="RGB-lt-–-gt-HSV"><a href="#RGB-lt-–-gt-HSV" class="headerlink" title="RGB &lt;–&gt; HSV"></a>RGB &lt;–&gt; HSV</h3><p>In case of 8-bit and 16-bit images, RGB values are converted to the floating-point format and scaled to fit the 0 to 1 range.<br>\[<br>V = \max{(R, G, B)}<br>\]<br>\[<br>S = \frac{V - \min{(R, G, B)}}{V} \space \space \mbox{if } V \neq 0<br>\]<br>\[<br>S = 0 \space \space \mbox{if } V = 0<br>\]<br>\[<br>H = \frac{60(G - B)}{V - \min{(R, G, B)}} \space \space \mbox{if } V = R<br>\]<br>\[<br>H = 120 + \frac{60(B - R)}{V - \min{(R, G, B)}} \space \space \mbox{if } V = G<br>\]<br>\[<br>H = 240 + \frac{60(R - G)}{V - \min{(R, G, B)}} \space \space \mbox{if } V = B<br>\]<br>If \(H &lt; 0\) then \(H = H + 360\). The output \(0 \leqslant V \leqslant 1\), \(0 \leqslant S \leqslant 1\), \(0 \leqslant H \leqslant 360\).</p><p>The final output of the HSV format depends on the image data types. By default, <code>CV_32F</code> is output.<br>  <code>CV_8U</code>: \(V = 255V\), \(S = 255S\), \(H = H/2\)<br>  <code>CV_16U</code>: \(V = -65535V\), \(S = -65535S\), \(H = -H\) (<em>currently not supported</em>)</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><p>In OpenCV, a general function takes care of all colour conversions.</p><pre><code>dst = cv2.cvtColor(src, code[, dst[, dstCn]])</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image. 8-bit unsigned (<code>CV_8U</code>), 16-bit unsigned (<code>CV_16U</code>), or single-precision floating-point (<code>CV_32F</code>).<br><strong>dst</strong> -&gt; Destination image.<br><strong>code</strong> -&gt; (compulsory) colour space conversion code.<br><strong>dstCn</strong> -&gt; (optional) number of channels in the destination image. If it is \(0\), <strong>dstCn</strong> is derived automatically from <strong>src</strong> and <strong>code</strong>.</p><p><em>Note</em> The default colour format in OpenCV is actually <strong>BGR</strong>!!<br>The conventional range for RGB channel values are:<br>  \(0\) to \(255\) for <code>CV_8U</code><br>  \(0\) to \(65535\) for <code>CV_16U</code><br>  \(0\) to \(1\) for <code>CV_32F</code><br>For linear transformation, the range does not matter. But it does for non-linear transformation.</p><p>The <strong>code</strong> that we care about are <code>CV_BGR2GRAY</code>, <code>CV_GRAY2BGR</code>, <code>CV_BGR2HSV</code> and <code>CV_HSV2BGR</code>.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><h4 id="RGB-lt-–-gt-Grey-1"><a href="#RGB-lt-–-gt-Grey-1" class="headerlink" title="RGB &lt;–&gt; Grey"></a>RGB &lt;–&gt; Grey</h4><pre><code>import cv2name = image_nameimg = cv2.imread(&#39;./{}.jpg&#39;.format(name), cv2.IMREAD_UNCHANGED)cv2.imshow(&quot;Original image&quot;, img)bgr2grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)grey2bgr = cv2.cvtColor(bgr2grey, cv2.COLOR_GRAY2BGR)cv2.imshow(&quot;BGR to Grey image&quot;, bgr2grey)cv2.imshow(&quot;Grey to RGB image&quot;, grey2bgr)print(bgr2grey.shape)print(grey2bgr.shape)cv2.waitKey(0)cv2.destroyAllWindows()</code></pre><p>Example results are shown below:<br><strong>The original image</strong> (From <em>Image Processing1</em> post)<br><img src="flower_original.jpg" srcset="/img/loading.gif" alt="small"></p><p><strong>RGB_to_Grey</strong><br><img src="flower_RGB_to_Grey.jpg" srcset="/img/loading.gif" alt="c2g"></p><p><strong>Grey_to_RGB</strong><br><img src="flower_Grey_to_RGB.jpg" srcset="/img/loading.gif" alt="g2c"></p><p><em>Note</em> The <strong>Grey_to_RGB</strong> cannot recover the original image as expected. The information of RGB values has lost. The reason of using this function is to change the dimension of the grey image.</p><p>In our case, <strong>RGB_to_Grey</strong> has dimension \((530 \times 742)\), but <strong>Grey_to_RGB</strong> has dimension \((530 \times 742 \times 3)\).</p><h4 id="RGB-lt-–-gt-HSV-1"><a href="#RGB-lt-–-gt-HSV-1" class="headerlink" title="RGB &lt;–&gt; HSV"></a>RGB &lt;–&gt; HSV</h4><pre><code>import cv2name = image_nameimg = cv2.imread(&#39;./{}.jpg&#39;.format(name), cv2.IMREAD_UNCHANGED)cv2.imshow(&quot;Original image&quot;, img)bgr2hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)hsv2bgr = cv2.cvtColor(bgr2hsv, cv2.COLOR_HSV2BGR)cv2.imshow(&quot;RGB to HSV image&quot;, bgr2hsv)cv2.imshow(&quot;HSV to RGB image&quot;, hsv2bgr)cv2.waitKey(0)cv2.destroyAllWindows()</code></pre><p>Example results are shown below:<br><strong>RGB_to_HSV</strong><br><img src="flower_RGB_to_HSV.jpg" srcset="/img/loading.gif" alt="h2g"></p><p><strong>HSV_to_RGB</strong><br><img src="flower_HSV_to_RGB.jpg" srcset="/img/loading.gif" alt="g2h"></p><p><em>Note</em> The unnatural colour of the <strong>RGB_to_HSV</strong> is ascribed to the fact that we use RGB to display HSV. <strong>HSV_to_RGB</strong> recover the original image as expected.</p><hr><h1 id="Build-RGB2GRAY-and-RGB2HSV-Functions-From-Scratch"><a href="#Build-RGB2GRAY-and-RGB2HSV-Functions-From-Scratch" class="headerlink" title="Build RGB2GRAY and RGB2HSV Functions From Scratch"></a>Build <em>RGB2GRAY</em> and <em>RGB2HSV</em> Functions From Scratch</h1><h3 id="RGB2GRAY"><a href="#RGB2GRAY" class="headerlink" title="RGB2GRAY"></a><em>RGB2GRAY</em></h3><pre><code>import numpy as npdef bgr_to_gray(img):    cols, rows, channels = img.shape    gray = np.zeros((cols, rows))    b = img[:, :, 0]    g = img[:, :, 1]    r = img[:, :, 2]    gray[:, :] = 0.229*r + 0.587*g + 0.114*b    return gray</code></pre><p>The output is shown as below.<br><strong>My function</strong><br><img src="My_flower_RGB_to_Grey.jpg" srcset="/img/loading.gif" alt="my_grey"><br><strong>OpenCV</strong><br><img src="flower_RGB_to_Grey.jpg" srcset="/img/loading.gif" alt="open_grey"></p><p>More example:<br><strong>Color Image</strong><br><img src="temp1.jpg" srcset="/img/loading.gif" alt="pixel"><br><strong>Greyscale Image</strong><br><img src="tempG.jpg" srcset="/img/loading.gif" alt="pixel_grey"></p><p><em>Note</em> Some techniques can be applied to avoid floating point calculations.</p><h3 id="GRAY2RGB"><a href="#GRAY2RGB" class="headerlink" title="GRAY2RGB"></a><em>GRAY2RGB</em></h3><pre><code>def grey_to_rgb(img):    if len(img.shape) != 2:        print(&quot;Input should be a greyscale image&quot;)    else:        dst = np.zeros((img.shape[0], img.shape[1], 3))        dst[:, :, 0] = img[:, :]        dst[:, :, 1] = img[:, :]        dst[:, :, 2] = img[:, :]        return dst</code></pre><h3 id="RGB2HSV-TODO"><a href="#RGB2HSV-TODO" class="headerlink" title="RGB2HSV(TODO)"></a><em>RGB2HSV</em>(TODO)</h3><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Photo by Viktor Mogilat on Unsplash.</li><li>Smith, A.R., 1978. Color gamut transform pairs. ACM Siggraph Computer Graphics, 12(3), pp.12-19.</li><li>En.wikipedia.org. 2020. SRGB. [online] Available at: <a href="https://en.wikipedia.org/wiki/SRGB" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/SRGB</a> [Accessed 24 April 2020].</li><li>En.wikipedia.org. 2020. HSL And HSV. [online] Available at: <a href="https://en.wikipedia.org/wiki/HSL_and_HSV#cite_ref-Smith_13-1" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/HSL_and_HSV#cite_ref-Smith_13-1</a> [Accessed 24 April 2020].</li><li>Blog.csdn.net. 2020. 色彩转换系列之RGB格式与HSV格式互转原理及实现_人工智能_小武的博客-CSDN博客. [online] Available at: <a href="https://blog.csdn.net/weixin_40647819/article/details/92660320" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40647819/article/details/92660320</a> [Accessed 24 April 2020].</li><li>Docs.opencv.org. 2020. Miscellaneous Image Transformations — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html</a> [Accessed 24 April 2020].</li><li>Photo by Omid Armin on Unsplash</li><li>Docs.opencv.org. 2020. Opencv: Color Conversions. [online] Available at: <a href="https://docs.opencv.org/3.1.0/de/d25/imgproc_color_conversions.html" target="_blank" rel="noopener">https://docs.opencv.org/3.1.0/de/d25/imgproc_color_conversions.html</a> [Accessed 24 April 2020].</li><li>Tutorialspoint.com. 2020. Grayscale To RGB Conversion - Tutorialspoint. [online] Available at: <a href="https://www.tutorialspoint.com/dip/grayscale_to_rgb_conversion.htm" target="_blank" rel="noopener">https://www.tutorialspoint.com/dip/grayscale_to_rgb_conversion.htm</a> [Accessed 24 April 2020].</li><li>En.wikipedia.org. 2020. Grayscale. [online] Available at: <a href="https://en.wikipedia.org/wiki/Grayscale" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Grayscale</a> [Accessed 24 April 2020].</li><li>En.wikipedia.org. 2020. Gamma Correction. [online] Available at: <a href="https://en.wikipedia.org/wiki/Gamma_correction" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Gamma_correction</a> [Accessed 24 April 2020].</li><li>Stevens, S.S., 1957. On the psychophysical law. Psychological review, 64(3), p.153.</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation -&gt; Image Processing2: Geometric Transformation</title>
    <link href="/2020/04/21/Computer-Vision-Fundation-Image-Processing2-Geometric-Transformation/"/>
    <url>/2020/04/21/Computer-Vision-Fundation-Image-Processing2-Geometric-Transformation/</url>
    
    <content type="html"><![CDATA[<p>In this post, another basic image operation is explored - Transformation.</p><p>This post is split into four sections:</p><ol><li>Mathematical and Computational Principle of Transformations</li><li>Practice with OpenCV in Python</li><li>Build <em>Translation</em> and <em>Rotation</em> Function From Scratch</li><li>(Option) Other Transformation Functions</li></ol><p>OpenCV code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; transform_image.py<br>Re-implementation code: <a href="https://github.com/BillMaZengou/cv_basis/my_opencv" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis/my_opencv</a> -&gt; transform.py</p><hr><h1 id="Principle-of-Transformation"><a href="#Principle-of-Transformation" class="headerlink" title="Principle of Transformation"></a>Principle of Transformation</h1><h3 id="Mathematics"><a href="#Mathematics" class="headerlink" title="Mathematics"></a>Mathematics</h3><p>In general, a transformation of a point \(\mathbf{p}\), \(\begin{pmatrix} v &amp; w \end{pmatrix}\) to another point \(\mathbf{q}\), \(\begin{pmatrix} x &amp; y \end{pmatrix}\) is given by<br>\[\mathbf{q} = \mathbf{R}\mathbf{p} + \mathbf{T}.\]<br>Explicitly,<br>\[<br>\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} \omega_1 &amp; \omega_2 \\ \omega_3 &amp; \omega_4 \end{bmatrix} \begin{bmatrix} v \\ w \end{bmatrix} + \begin{bmatrix} t_1 \\ t_2 \end{bmatrix}.<br>\]<br>where the \(\mathbf{R}\) and \(\mathbf{T}\) denote rotation and translation matrices respectively.</p><p>To make the expression more compact, we can write it as<br>\[<br>\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} \omega_1 &amp; \omega_2 &amp; t_1\\ \omega_3 &amp; \omega_4 &amp; t_2 \end{bmatrix} \begin{bmatrix} v \\ w \\ 1 \end{bmatrix}.<br>\]</p><p>A quick method to determine the transformation matrix is to consider the changes of unit vectors \(\hat{i}\) and \(\hat{j}\).</p><p>For example, if the \(\mathbf{R}\) is an identity matrix and \(\mathbf{T} = \mathbf{0}\) (i.e. no transformation), then the equation is<br>\[<br>\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} \begin{bmatrix} v \\ w \end{bmatrix}.<br>\]<br>Another way to look at this equation is<br>\[<br>\begin{bmatrix} x \\ y \end{bmatrix} = v \begin{bmatrix} 1 \\ 0 \end{bmatrix} + w \begin{bmatrix} 0 \\ 1 \end{bmatrix},<br>\]<br>which means that \(v\) and \(w\) are values to scale up the unit vectors \(\hat{i}\) and \(\hat{j}\). Therefore, any transformation of the vector \(\begin{bmatrix} v \\ w \end{bmatrix}\) can be regards as the transformation of the entire vector space formed by \(\hat{i}\) and \(\hat{j}\).</p><p><img src="rotation.png" srcset="/img/loading.gif" alt="rotation"><br>Take rotation as an example, if \(\hat{i}\) and \(\hat{j}\) are rotated anti-clockwise by an angle \(\theta\), then \(\hat{i}\) becomes \(\begin{bmatrix} cos(\theta) \\ sin(\theta) \end{bmatrix}\) and \(\hat{j}\) becomes \(\begin{bmatrix} -sin(\theta) \\ cos(\theta) \end{bmatrix}\). Thus, the equation is<br>\[<br>\begin{bmatrix} x \\ y \end{bmatrix} = v \begin{bmatrix} cos(\theta) \\ sin(\theta) \end{bmatrix} + w \begin{bmatrix} -sin(\theta) \\ cos(\theta) \end{bmatrix}.<br>\]<br>Therefore, in the general equation, we should use the matrix<br>\[ \begin{bmatrix} \cos(\theta) &amp; -sin(\theta) &amp; 0\\ sin(\theta) &amp; cos(\theta) &amp; 0 \end{bmatrix}. \]<br>Other transformation matrix can be obtained in the same manner.</p><h3 id="Computation"><a href="#Computation" class="headerlink" title="Computation"></a>Computation</h3><p>Except the expression in <strong>Mathematics</strong>, sometimes square matrix can be used.<br>\[<br>\begin{bmatrix} x \\ y \\ 1\end{bmatrix} = \begin{bmatrix} \omega_1 &amp; \omega_2 &amp; t_1\\ \omega_3 &amp; \omega_4 &amp; t_2 \\ 0 &amp; 0 &amp; 1\end{bmatrix} \begin{bmatrix} v \\ w \\ 1\end{bmatrix},<br>\]<br>or if row vectors are used<br>\[<br>\begin{bmatrix} x &amp; y &amp; 1\end{bmatrix} = \begin{bmatrix} v &amp; w &amp; 1\end{bmatrix} \begin{bmatrix} \omega_1 &amp; \omega_3 &amp; 0\\ \omega_2 &amp; \omega_4 &amp; 0 \\ t_1 &amp; t_2 &amp; 1\end{bmatrix}.<br>\]<br>The matrix should be obtained using the same manner discussed in last section.</p><p>One issue to consider when processing images in computer is that the origin of the image does not locate at the centre of the image, but at the top-left corner. It works for translation and resizing. But converting to Cartesian coordinates is necessary before rotation or shearing.<br><img src="coordinate.png" srcset="/img/loading.gif" alt="coordinate"></p><p>Therefore, rotation or shearing requires three steps:</p><ol><li>Convert from image coordinates to Cartesian coordinates</li><li>Perform rotation or shearing</li><li>Convert the results back to image coordinates</li></ol><p>It is not hard to realise that for a \((M \times N)\) matrix, the origin of the image space locates at \(-\frac{N}{2}, \frac{M}{2}\). Thus, the transformation can be calculated from<br>\[<br>\begin{bmatrix} x &amp; y &amp; 1\end{bmatrix} = \begin{bmatrix} v &amp; w &amp; 1 \end{bmatrix} \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 0 \\ -0.5N &amp; 0.5M &amp; 1 \end{bmatrix} \begin{bmatrix} \mathbf{R}^T &amp; \mathbf{0} \\ \mathbf{T}^T &amp; 1 \end{bmatrix} \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 0 \\ 0.5N &amp; 0.5M &amp; 1 \end{bmatrix}.<br>\]</p><h4 id="Aside"><a href="#Aside" class="headerlink" title="Aside"></a>Aside</h4><p>One technical nuance that we need to consider is whether forward or backward mapping should be used after transformation. Intuitively, forward mapping is the procedure to obtain the transformed images. However, couple of problems can occur.<br><img src="forward_mapping.png" srcset="/img/loading.gif" alt="forward"></p><ol><li>It can result pixels outside the image boundary.</li><li>Complex transforms can map several input pixels to the same output pixel.</li><li>The pixel value of the output pixel cannot be obtained simply. As shown in the diagram, the pixel value of the mapped pixel should be interpolated from nearby pixels. Thus, all the mapped pixels have to be computed first.</li></ol><p>Backward mapping, on the other side, find the corresponding point of the mapped pixel on the original image.<br><img src="backward_mapping.png" srcset="/img/loading.gif" alt="backward"><br>Then, the pixel value can be calculated using the nearby pixels on the original image. Therefore, it costs less computationally.</p><p>As we need to interpolate the pixel values, An interpolation mechanism needs selecting. Normally, bilinear interpolation is preferred. OpenCV uses bilinear interpolation as well.</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><h4 id="General-Transformation-Method-Affine-Transformation"><a href="#General-Transformation-Method-Affine-Transformation" class="headerlink" title="General Transformation Method (Affine Transformation)"></a>General Transformation Method (Affine Transformation)</h4><pre><code>dst = cv2.warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]])</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image.<br><strong>dst</strong> -&gt; Destination image.<br><strong>M</strong> -&gt; (compulsory) \((2 \times 3)\) transformation matrix.<br><strong>dsize</strong> -&gt; (compulsory) Destination image size.<br><strong>flages</strong> -&gt; (optional) Combination of interpolation methods (Conduct the last post <em>Resize</em>). <strong>INTER_LINEAR</strong> is used by <em>default</em><br><em>-Addition-</em><br><strong>borderMode</strong> and <strong>borderValue</strong> take care of the border of the destination image. For example, if <strong>borderMode=BORDER_TRANSPARENT</strong>, then the pixels in the destination image corresponding to the “outliers” in the source image are not modified by the function.</p><p><em>Note</em> Unlike what I stated in <strong>Computation</strong>, <strong>M</strong> in this function is a \((2 \times 3)\) matrix as stated in <strong>Mathematics</strong>.</p><h4 id="Rotation-Matrix"><a href="#Rotation-Matrix" class="headerlink" title="Rotation Matrix"></a>Rotation Matrix</h4><p>As mentioned in <strong>Computation</strong>, for rotation and shearing, we need to convert the coordinate system first. In OpenCV, for rotation, this can easily be done using <code>getRotationMatrix2D</code>.</p><pre><code>R = cv2.getRotationMatrix2D(center, angle, scale)</code></pre><p><strong>center</strong> -&gt; (compulsory) Center of the rotation in the source image.<br><strong>angle</strong> -&gt; (compulsory) Rotation angle in <em>degrees</em>. Positive values mean anti-clockwise rotation (the coordinate origin is assumed to be the top-left corner).<br><strong>scale</strong> -&gt; (compulsory) Isotropic scale factor.<br>This will give the rotation matrix \(\mathbf{R}\), which can then be used in <code>warpAffine</code>. As shown in <strong>Computation</strong>, to convert the coordinate system to Cartesian, the <code>center=(cols/2, rows/2)</code>, where <code>cols</code> and <code>rows</code> can be obtained from <code>image.shape</code></p><h4 id="Shearing-Matrix"><a href="#Shearing-Matrix" class="headerlink" title="Shearing Matrix"></a>Shearing Matrix</h4><p>To implement the shearing operation, we have to construct the matrix ourselves with the procedure stated in <strong>Computation</strong>. Notice that truncated and transverse matrix should be constructed to match OpenCV convention. Details in example code.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><pre><code>import numpy as npimport cv2name = image_nameimg = cv2.imread(&#39;./{}.jpg&#39;.format(name), cv2.IMREAD_UNCHANGED)rows, cols, _ = img.shape# TranslationT = np.array(([1, 0, 100], [0, 1, 50]), dtype=np.float32)# RotationR = cv2.getRotationMatrix2D((cols/2, rows/2), 45, 1)# Shearingconvert = np.array(([1, 0, 0], [0, -1, 0], [cols/2, rows/2, 1]), dtype=np.float32)inverse = np.array(([1, 0, 0], [0, -1, 0], [-cols/2, rows/2, 1]), dtype=np.float32)S = np.array(([1, 0, 0], [0.5, 1, 0], [0, 0, 1]), dtype=np.float32)  # unit vector (0, 1) to (0.5, 1), shear to x directionS = inverse@S@convert# To match OpenCV conventionS = np.transpose(S[:, :-1])translated = cv2.warpAffine(img, T, (cols, rows))rotated = cv2.warpAffine(img, R, (cols, rows))sheared = cv2.warpAffine(img, S, (cols, rows))cv2.imshow(&quot;Translated image&quot;, translated)cv2.imshow(&quot;Rotated image&quot;, rotated)cv2.imshow(&quot;Sheared image&quot;, sheared)# Press &#39;s&#39; for saving the imagek = cv2.waitKey(0)if k == 27:         # wait for ESC key to exit    cv2.destroyAllWindows()elif k == ord(&#39;s&#39;): # wait for &#39;s&#39; key to save and exit    cv2.imwrite(&quot;Translated.jpg&quot;,translated)    cv2.imwrite(&quot;Rotated.jpg&quot;,rotated)    cv2.imwrite(&quot;Sheared.jpg&quot;,sheared)    cv2.destroyAllWindows()</code></pre><p>Example results are shown below:<br><strong>The original image</strong> (From last post)<br><img src="flower_Resized_image.jpg" srcset="/img/loading.gif" alt="small"></p><p><strong>Translation</strong><br><img src="flower_Translated.jpg" srcset="/img/loading.gif" alt="t"></p><p><strong>Rotation</strong><br><img src="flower_Rotated.jpg" srcset="/img/loading.gif" alt="r"></p><p><strong>Shearing</strong><br><img src="flower_Sheared.jpg" srcset="/img/loading.gif" alt="s"></p><hr><h1 id="Build-Each-Transformation-Function-From-Scratch"><a href="#Build-Each-Transformation-Function-From-Scratch" class="headerlink" title="Build Each Transformation Function From Scratch"></a>Build Each Transformation Function From Scratch</h1><p>In this section, I will show my implementation of these three transformations. Currently, it only accomplishes the task. Hence, optimisation may be applied in the future.</p><h3 id="Compute-The-Translation-Matrix"><a href="#Compute-The-Translation-Matrix" class="headerlink" title="Compute The Translation Matrix"></a>Compute The Translation Matrix</h3><pre><code>def translate(dx, dy):    t = np.eye(3)    t[2, 0] = dx    t[2, 1] = dy    return t</code></pre><h3 id="Compute-The-Rotation-Matrix"><a href="#Compute-The-Rotation-Matrix" class="headerlink" title="Compute The Rotation Matrix"></a>Compute The Rotation Matrix</h3><pre><code>def rotation(img, theta, direction=&quot;anticlockwise&quot;):    t = np.eye(3)    cols = img.shape[0]    rows = img.shape[1]    convert = np.array(([1, 0, 0], [0, -1, 0], [cols/2, rows/2, 1]), dtype=np.float32)    inverse = np.array(([1, 0, 0], [0, -1, 0], [-cols/2, rows/2, 1]), dtype=np.float32)    if direction==&quot;clockwise&quot;:        theta = theta/180 * np.pi    elif direction==&quot;anticlockwise&quot;:        theta = - theta/180 * np.pi    else:        raise Exception(&quot;Error: diretion should be either anticlockwise or clockwise&quot;)    t[0, 0] = np.cos(theta)    t[0, 1] = np.sin(theta)    t[1, 0] = -np.sin(theta)    t[1, 1] = np.cos(theta)    return inverse@t@convert</code></pre><h3 id="Compute-The-Shearing-matrix"><a href="#Compute-The-Shearing-matrix" class="headerlink" title="Compute The Shearing matrix"></a>Compute The Shearing matrix</h3><pre><code>def shear(img, step, direction=&quot;x&quot;):    t = np.eye(3)    cols = img.shape[0]    rows = img.shape[1]    convert = np.array(([1, 0, 0], [0, -1, 0], [cols/2, rows/2, 1]), dtype=np.float32)    inverse = np.array(([1, 0, 0], [0, -1, 0], [-cols/2, rows/2, 1]), dtype=np.float32)    if direction==&quot;x&quot;:        t[1, 0] = step    elif direction==&quot;y&quot;:        t[0, 1] = step    else:        raise Exception(&quot;Error: diretion should be either x or y&quot;)    return inverse@t@convert</code></pre><h3 id="Create-The-Result-Image"><a href="#Create-The-Result-Image" class="headerlink" title="Create The Result Image"></a>Create The Result Image</h3><pre><code>def affine_transform(img, operation):    dst = np.zeros_like(img)    cols = img.shape[0]    rows = img.shape[1]    for i in range(rows):        for j in range(cols):            after = np.array([i, j, 1])            before = after@np.linalg.inv(operation)            &quot;&quot;&quot;            If the (x, y) in the resulted image is from a point (u, v),            which is not in the original image, then there should be no            pixel value.            &quot;&quot;&quot;            if (before[0] &gt;= 0) &amp; (before[1] &gt;= 0) &amp; (before[0] &lt; rows) &amp; (before[1] &lt; cols):                dst[j, i] = img[int(before[1]), int(before[0])]    return dst</code></pre><p>The results shown as follow.</p><p><strong>manually Created Small Image</strong><br><img src="temp1.jpg" srcset="/img/loading.gif" alt="small"></p><p><strong>After Translation</strong><br><img src="tempt.jpg" srcset="/img/loading.gif" alt="tr"></p><p><strong>After Rotation</strong><br><img src="tempR.jpg" srcset="/img/loading.gif" alt="Ro"></p><p><strong>After Shearing</strong><br><img src="temps.jpg" srcset="/img/loading.gif" alt="Sh"></p><p>The image was created using the same manner in the last post. (<a href="https://billmazengou.github.io/2020/04/19/Computer-Vision-Fundation-Image-Processing1-Digital-Image-Interpolation/" target="_blank" rel="noopener">https://billmazengou.github.io/2020/04/19/Computer-Vision-Fundation-Image-Processing1-Digital-Image-Interpolation/</a>)</p><hr><h1 id="Option-Other-Transformation-Functions-TODO"><a href="#Option-Other-Transformation-Functions-TODO" class="headerlink" title="(Option) Other Transformation Functions (TODO)"></a>(Option) Other Transformation Functions (TODO)</h1><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Reed, N., 2020. Rotations And Infinitesimal Generators – Nathan Reed’S Coding Blog. [online] Reedbeta.com. Available at: <a href="http://reedbeta.com/blog/rotations-and-infinitesimal-generators/" target="_blank" rel="noopener">http://reedbeta.com/blog/rotations-and-infinitesimal-generators/</a> [Accessed 22 April 2020].</li><li>Lohninger, H., 2020. Java Programming Course - Coordinates. [online] Vias.org. Available at: <a href="http://www.vias.org/javacourse/chap04_10.html" target="_blank" rel="noopener">http://www.vias.org/javacourse/chap04_10.html</a> [Accessed 22 April 2020].</li><li>Blog.csdn.net. 2020. 图像变换——向前映射和向后映射_人工智能_薇洛的打火机-CSDN博客. [online] Available at: <a href="https://blog.csdn.net/glorydream2015/article/details/44873703" target="_blank" rel="noopener">https://blog.csdn.net/glorydream2015/article/details/44873703</a> [Accessed 22 April 2020].</li><li>Engr.case.edu. 2020. [online] Available at: <a href="http://engr.case.edu/merat_francis/eecs490f07/lectures/lecture4.pdf" target="_blank" rel="noopener">http://engr.case.edu/merat_francis/eecs490f07/lectures/lecture4.pdf</a> [Accessed 22 April 2020].</li><li>Docs.opencv.org. 2020. Geometric Image Transformations — Opencv 2.4.13.7 Documentation. [online] Available at: <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html?highlight=warpaffine" target="_blank" rel="noopener">https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html?highlight=warpaffine</a> [Accessed 22 April 2020].</li><li>Photo by Viktor Mogilat on Unsplash.</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision Foundation -&gt; Image Processing1: Digital Image Interpolation</title>
    <link href="/2020/04/19/Computer-Vision-Fundation-Image-Processing1-Digital-Image-Interpolation/"/>
    <url>/2020/04/19/Computer-Vision-Fundation-Image-Processing1-Digital-Image-Interpolation/</url>
    
    <content type="html"><![CDATA[<p>To fully understand computer vision, learning image processing is inevitable. One basic operation is to resize the images. When enlarging a small image, the resulted image may have jagged pixel edges. Image interpolation is to add a few pixels and manipulate their value in purpose so that the resulted image looks smoother.</p><p>This post is split into four sections:</p><ol><li>Two Basic Image Interpolation Algorithm</li><li>Practice with OpenCV in Python</li><li>Build <em>Resize</em> Function From Scratch</li><li>(Option) Other Interpolation Algorithm</li></ol><p>OpenCV code: <a href="https://github.com/BillMaZengou/cv_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis</a> -&gt; resize_image.py<br>Re-implementation code: <a href="https://github.com/BillMaZengou/cv_basis/my_opencv" target="_blank" rel="noopener">https://github.com/BillMaZengou/cv_basis/my_opencv</a> -&gt; resize.py</p><hr><h1 id="Two-Basic-Image-Interpolation-Algorithm"><a href="#Two-Basic-Image-Interpolation-Algorithm" class="headerlink" title="Two Basic Image Interpolation Algorithm"></a>Two Basic Image Interpolation Algorithm</h1><h3 id="Nearest-neighbour-interpolation"><a href="#Nearest-neighbour-interpolation" class="headerlink" title="Nearest-neighbour interpolation"></a>Nearest-neighbour interpolation</h3><p>When the image size increases, the extra pixels will use the same value as their nearest neighbour.<br><img src="/images/Nearest1.png" srcset="/img/loading.gif" alt="Nearest"><br>This is the simplest method. However, it is not hard to realise that this approach is problematic as it preserves the same resolution as the small image.</p><h3 id="Bilinear-interpolation"><a href="#Bilinear-interpolation" class="headerlink" title="Bilinear interpolation"></a>Bilinear interpolation</h3><p>Bilinear interpolation tries to estimate the new pixel values using information from their neighbours. Normally, linear relations are assumed in both \(x\) and \(y\) directions, namely bilinear relation. Hopefully, it will give a smoother enlarged image than <strong>Nearest-neighbour interpolation</strong>.</p><p>Typical derivation follows the logic that the enlargement should have bilinear relation and it should work to give a smoother image. Here, I decided to follow the opposite route. By assuming the resulted image is continuous and smooth, we can obtain the <strong>Bilinear interpolation</strong> without assuming bilinear relation at the first place. The full derivation will be shown in <em>Appendix</em>. Two routes are essentially equivalent.<br><img src="/images/Bilinear.png" srcset="/img/loading.gif" alt="Bilinear"><br>The equation is<br>\[<br>  f(x, y) = \frac{(x_2 - x)(y_2-y)}{(x_2 - x_1)(y_2-y_1)}f(Q_{11}) + \frac{(x - x_1)(y_2-y)}{(x_2 - x_1)(y_2-y_1)}f(Q_{21}) + \frac{(x_2 - x)(y-y_1)}{(x_2 - x_1)(y_2-y_1)}f(Q_{12}) + \frac{(x - x_1)(y-y_1)}{(x_2 - x_1)(y_2-y_1)}f(Q_{22}).<br>\]<br>Or in matrix form,<br>$$<br>  f(x, y) = \frac{1}{(x_2 - x_1)(y_2-y_1)} \begin{bmatrix} (x_2-x) &amp; (x-x_1) \end{bmatrix} \begin{bmatrix} f(Q_{11}) &amp; f(Q_{12}) \\ f(Q_{21}) &amp; f(Q_{22}) \end{bmatrix} \begin{bmatrix} (y_2-y) \\ (y-y_1) \end{bmatrix}.<br>$$</p><hr><h1 id="Practice-with-OpenCV-in-Python"><a href="#Practice-with-OpenCV-in-Python" class="headerlink" title="Practice with OpenCV in Python"></a>Practice with OpenCV in Python</h1><h3 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h3><p>Despite the maths looks complicated, the implementation is straightforward using OpenCV and Python.</p><pre><code>dst = cv2.resize(src, dsize[, dst[, fx[, fy[, interpolation]]]])</code></pre><p><strong>src</strong> -&gt; (compulsory) Source image.<br><strong>dst</strong> -&gt; Destination image.<br><strong>dsize</strong> -&gt; (compulsory) Destination image size. If it is \(0\), it is computed as:</p><pre><code>  dsize = Size(round(fx*src.cols), round(fy*src.rows))</code></pre><p><strong>fx</strong> -&gt; (optional) Scale factor along the horizontal axis. When it is \(0\), it is computed as:</p><pre><code>  (double)dsize.width/src.cols</code></pre><p><strong>fy</strong> -&gt; (optional) Scale factor along the vertical axis. When it is \(0\), it is computed as:</p><pre><code>  (double)dsize.height/src.rows</code></pre><p><strong>interpolation</strong> -&gt; (optional) Interpolation method:<br>  <strong>INTER_NEAREST</strong> - a nearest-neighbour interpolation<br>  <strong>INTER_LINEAR</strong> - a bilinear interpolation (used by <em>default</em>)<br>  <em>-Addition-</em><br>  <strong>INTER_AREA</strong> - resampling using pixel area relation. It may be a preferred method for image decimation, as it gives moire’-free results. But when the image is zoomed, it is similar to the <strong>INTER_NEAREST</strong> method.<br>  <strong>INTER_CUBIC</strong> - a bicubic interpolation over 4x4 pixel neighbourhood<br>  <strong>INTER_LANCZOS4</strong> - a Lanczos interpolation over 8x8 pixel neighbourhood</p><p><em>Note</em> that, normally, <strong>INTER_AREA</strong> is used when scaling down the image. Otherwise, <strong>INTER_CUBIC</strong> and <strong>INTER_LINEAR</strong> are good choices. But <strong>INTER_CUBIC</strong> is a bit slow.</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><pre><code>import cv2img = cv2.imread(image_name, cv2.IMREAD_UNCHANGED)  # replace with your image_namescale_percent = 5       # percent of original sizewidth = int(img.shape[1] * scale_percent / 100)height = int(img.shape[0] * scale_percent / 100)dim = (width, height)# scale down the imageresized = cv2.resize(img, dim, interpolation = cv2.INTER_LINEAR)fx = 3fy = 3# scale up the resized imageresized1 = cv2.resize(resized, dsize=None, fx=fx, fy=fy, interpolation = cv2.INTER_NEAREST)resized2 = cv2.resize(resized, dsize=None, fx=fx, fy=fy, interpolation = cv2.INTER_LINEAR)# display the resultscv2.imshow(&quot;Resized image&quot;, resized)cv2.imshow(&quot;INTER_NEAREST image&quot;, resized1)cv2.imshow(&quot;INTER_LINEAR image&quot;, resized2)# Press &#39;s&#39; for saving the imagek = cv2.waitKey(0)if k == 27:         # wait for ESC key to exit    cv2.destroyAllWindows()elif k == ord(&#39;s&#39;): # wait for &#39;s&#39; key to save and exit    cv2.imwrite(&quot;Resized_image.jpg&quot;,resized)    cv2.imwrite(&quot;INTER_NEAREST_image.jpg&quot;,resized1)    cv2.imwrite(&quot;INTER_LINEAR_image.jpg&quot;,resized2)    cv2.destroyAllWindows()</code></pre><p>Example results are shown below:<br><strong>Scale down the original image by 95% using Bilinear interpolation</strong><br><img src="/images/Resized_image.jpg" srcset="/img/loading.gif" alt="small"></p><p><strong>Scale up the resized image by 30% using Nearest-neighbour interpolation</strong><br><img src="/images/INTER_NEAREST_image.jpg" srcset="/img/loading.gif" alt="nearest_image"></p><p><strong>Scale up the resized image by 30% using Bilinear interpolation</strong><br><img src="/images/INTER_LINEAR_image.jpg" srcset="/img/loading.gif" alt="bilinear_image"><br>It is clear that the result of the Bilinear interpolation is much smoother than it of the Nearest-neighbour interpolation.</p><hr><h1 id="Build-Resize-Function-From-Scratch"><a href="#Build-Resize-Function-From-Scratch" class="headerlink" title="Build Resize Function From Scratch"></a>Build <em>Resize</em> Function From Scratch</h1><p>In this section, I will implement <em>Resize</em> function without using OpenCV.</p><h3 id="Nearest-neighbour-interpolation-1"><a href="#Nearest-neighbour-interpolation-1" class="headerlink" title="Nearest-neighbour interpolation"></a>Nearest-neighbour interpolation</h3><p>To simply the problem, currently only focus on greyscale images. More details about  greyscale images, check <strong>Colour Conversion</strong> (<a href="https://billmazengou.github.io/2020/04/24/Computer-Vision-Foundation-Image-Processing3-Colour-Conversion/" target="_blank" rel="noopener">https://billmazengou.github.io/2020/04/24/Computer-Vision-Foundation-Image-Processing3-Colour-Conversion/</a>)</p><pre><code>def scale(img, x_fac, y_fac):    x, y = img.shape    x_new = int(x * x_fac)    y_new = int(y * y_fac)    scaled_img = np.zeros((x_new, y_new))    for i in range(x_new):        for j in range(y_new):            scaled_img[i, j] = img[int(i//x_fac), int(j//y_fac)]    return scaled_img</code></pre><p>The results shown as follow.</p><p><strong>manually Created Small Image</strong><br><img src="/images/temp1.jpg" srcset="/img/loading.gif" alt="small"></p><p><strong>After scaling up by 1.5</strong><br><img src="/images/temp2.jpg" srcset="/img/loading.gif" alt="large"></p><p>The image can be easily created using</p><pre><code># Create an imagesize = 50img_t = np.zeros((size, size))for i in range(size):    if i &gt; size/2:        for j in range(size):            if j &gt; size/2:                img_t[i, j] = 255.0    else:        for j in range(size):            if j &lt; size/2:                img_t[i, j] = 255.0</code></pre><p>You can play with the <strong>size</strong></p><p>Test with other images.<br><strong>Original Greyscale Image</strong><br><img src="/images/temp_g.jpg" srcset="/img/loading.gif" alt="g"></p><p><strong>After scaling up by 1.5</strong><br><img src="/images/temp_c.jpg" srcset="/img/loading.gif" alt="c"></p><h3 id="Bilinear-interpolation-TODO"><a href="#Bilinear-interpolation-TODO" class="headerlink" title="Bilinear interpolation (TODO)"></a>Bilinear interpolation (TODO)</h3><hr><h1 id="Option-Other-Interpolation-Algorithm-TODO"><a href="#Option-Other-Interpolation-Algorithm-TODO" class="headerlink" title="(Option) Other Interpolation Algorithm (TODO)"></a>(Option) Other Interpolation Algorithm (TODO)</h1><hr><h1 id="Appendix-Derivation-of-Bilinear-interpolation"><a href="#Appendix-Derivation-of-Bilinear-interpolation" class="headerlink" title="Appendix - Derivation of Bilinear interpolation"></a>Appendix - Derivation of Bilinear interpolation</h1><p><img src="/images/Bilinear.png" srcset="/img/loading.gif" alt="Bilinear"><br>Assumptions: The image is smooth in any directions; \(Q_{11}\), \(Q_{12}\), \(Q_{21}\), \(Q_{22}\), \(R_1\), \(R_2\) are all close enough to point \(p\), \((x, y)\).</p><p>Follow by the assumptions, we have<br>\[<br>f(R_1) = f(x, y) - \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y-y_1);<br>f(R_2) = f(x, y) + \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y_2-y).<br>\]<br>Then,<br>\[<br>f(Q_{11}) = f(R_1) - \frac{\partial f(R_1)}{\partial x}\Bigr\rvert_{y_1} (x-x_1); f(Q_{21}) = f(R_1) + \frac{\partial f(R_1)}{\partial x}\Bigr\rvert_{y_1} (x_2-x);<br>\]<br>\[<br>f(Q_{12}) = f(R_2) - \frac{\partial f(R_2)}{\partial x}\Bigr\rvert_{y_2} (x-x_1); f(Q_{22}) = f(R_2) + \frac{\partial f(R_2)}{\partial x}\Bigr\rvert_{y_2} (x_2-x).<br>\]<br>Substituting \(f(R_1)\) and \(f(R_2)\) into the equations, we find that<br>\[<br>f(Q_{11}) = f(x, y) - \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y-y_1) - \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_1} (x-x_1) + \frac{\partial^2 f(x, y)}{\partial x \partial y}\Bigr\rvert_{y_1} (x-x_1) (y-y_1),<br>\]<br>the last term equals \(0\) as the function is evaluated at \(y=y_1\). Therefore, we have<br>\[<br>f(Q_{11}) = f(x, y) - \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y-y_1) - \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_1} (x-x_1);<br>\]<br>\[<br>f(Q_{21}) = f(x, y) - \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y-y_1) + \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_1} (x_2-x);<br>\]<br>\[<br>f(Q_{12}) = f(x, y) + \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y_2-y) - \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_2} (x-x_1);<br>\]<br>\[<br>f(Q_{22}) = f(x, y) + \frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y_2-y) + \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_2} (x_2-x).<br>\]</p><p>By rearranging the equations, we can find that<br>\[<br>\frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} (y-y_1) = f(x, y) - f(Q_{11}) - \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_1} (x-x_1) = f(x, y) - f(Q_{21}) + \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_1} (x_2-x);<br>\]<br>\[<br>\frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x}(y_2-y) = f(Q_{12}) - f(x, y) +  \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_2} (x-x_1) = f(Q_{22}) - f(x, y) - \frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_2} (x_2-x).<br>\]<br>Then,<br>\[<br>\frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_1} = \frac{f(Q_{21}) - f(Q_{11})}{x_2-x_1};<br>\]<br>\[<br>\frac{\partial f(x, y)}{\partial x}\Bigr\rvert_{y_2} = \frac{f(Q_{22}) - f(Q_{12})}{x_2-x_1}.<br>\]</p><p>Substituting the above equations back, we can fine \(\frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x}\) as<br>\[<br>\frac{\partial f(x, y)}{\partial y}\Bigr\rvert_{x} = \frac{f(x, y) - f(Q_{11})}{y-y_1} - \frac{x-x_1}{y-y_1}\frac{f(Q_{21}) - f(Q_{11})}{x_2-x_1} = \frac{f(Q_{12}) - f(x, y)}{y_2-y} - \frac{x-x_1}{y_2-y}\frac{f(Q_{22}) - f(Q_{12})}{x_2-x_1}.<br>\]</p><p>Rearrange to get \(f(x, y)\). Finally, we have the <strong>Bilinear relation</strong> that we want.<br>\[<br>f(x, y) = \frac{(x_2 - x)(y_2-y)}{(x_2 - x_1)(y_2-y_1)}f(Q_{11}) + \frac{(x - x_1)(y_2-y)}{(x_2 - x_1)(y_2-y_1)}f(Q_{21}) + \frac{(x_2 - x)(y-y_1)}{(x_2 - x_1)(y_2-y_1)}f(Q_{12}) + \frac{(x - x_1)(y-y_1)}{(x_2 - x_1)(y_2-y_1)}f(Q_{22}).<br>\]</p><hr><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>.</p><hr><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li>Blog.csdn.net. 2020. Opencv框架与图像插值算法_网络_Weixin_39940512的博客-CSDN博客. [online] Available at: <a href="https://blog.csdn.net/weixin_39940512/article/details/105343418" target="_blank" rel="noopener">https://blog.csdn.net/weixin_39940512/article/details/105343418</a> [Accessed 20 April 2020].</li><li>Angel, A., 2020. Nearest Neighbor Interpolation. [online] Imageeprocessing.com. Available at: <a href="https://www.imageeprocessing.com/2017/11/nearest-neighbor-interpolation.html" target="_blank" rel="noopener">https://www.imageeprocessing.com/2017/11/nearest-neighbor-interpolation.html</a> [Accessed 19 April 2020].</li><li>En.wikipedia.org. 2020. Bilinear Interpolation. [online] Available at: <a href="https://en.wikipedia.org/wiki/Bilinear_interpolation" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Bilinear_interpolation</a> [Accessed 20 April 2020].</li><li>Opencv.org.cn. 2020. Geometric Image Transformations — Opencv 2.3.2 Documentation. [online] Available at: <a href="http://www.opencv.org.cn/opencvdoc/2.3.2/html/modules/imgproc/doc/geometric_transformations.html?highlight=resize#cv.Resize" target="_blank" rel="noopener">http://www.opencv.org.cn/opencvdoc/2.3.2/html/modules/imgproc/doc/geometric_transformations.html?highlight=resize#cv.Resize</a> [Accessed 20 April 2020].</li><li>Photo by Viktor Mogilat on Unsplash.</li><li>Opencv-python-tutroals.readthedocs.io. 2020. Getting Started With Images — Opencv-Python Tutorials 1 Documentation. [online] Available at: <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_image_display/py_image_display.html" target="_blank" rel="noopener">https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_image_display/py_image_display.html</a> [Accessed 20 April 2020].</li></ol><hr>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Create A Blog|创建博客</title>
    <link href="/2020/04/13/Create-A-Blog-%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/"/>
    <url>/2020/04/13/Create-A-Blog-%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<p>Today is 2020/04/13, I will be on the plane to China in about 10 hours. I heard about how writing blog would help improve IT skills for a long time, but did not know when and how to start. Now I finally made my mind to create a blog. For thhe processes, I conducted “<a href="https://www.bilibili.com/video/BV1Yb411a7ty?from=search&amp;seid=13126380643235482353&quot;" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Yb411a7ty?from=search&amp;seid=13126380643235482353&quot;</a>. CodeSheep gave a great and detailed introduction to the Hexo blog framework. As I succeed, I decided to write it down which may make your creation easier and faster.</p><p>I used MacOs, so if you encounter any difficulies, google it.</p><p>This article is served as a record for myself. All the credits should be given to CodeSheep. (<a href="https://www.codesheep.cn/" target="_blank" rel="noopener">https://www.codesheep.cn/</a>)</p><h2 id="Step-1-Download-Dependencies"><a href="#Step-1-Download-Dependencies" class="headerlink" title="Step 1: Download Dependencies"></a>Step 1: Download Dependencies</h2><p>nodejs: <a href="https://nodejs.org/en/" target="_blank" rel="noopener">https://nodejs.org/en/</a><br>git: <a href="https://git-scm.com/downloads" target="_blank" rel="noopener">https://git-scm.com/downloads</a></p><p>Open Terminal and switch to root user</p><pre><code>sudo su</code></pre><p>put your password in.</p><p>Use</p><pre><code>node -vnpm -vgit -v</code></pre><p>to confirm you successfully download node.js and Git.</p><h2 id="Addition-Step"><a href="#Addition-Step" class="headerlink" title="Addition Step"></a>Addition Step</h2><p>If you are in China, ‘cnpm’ should be faster than ‘npm’.</p><p>Install cnpm and registry to TaoBao.</p><pre><code>npm install -g cnpm --registry=https://registry.npm.taobao.org</code></pre><p>Then, in the following context, replace ‘npm’ with ‘cnpm’</p><hr><h2 id="Step-2-Download-Hexo-Framework"><a href="#Step-2-Download-Hexo-Framework" class="headerlink" title="Step 2: Download Hexo Framework"></a>Step 2: Download Hexo Framework</h2><pre><code>npm install -g hexo-cli</code></pre><p>After dowaload, you can use</p><pre><code>hexo -v</code></pre><p>to confirm.</p><hr><h2 id="Step-3-Create-a-blog"><a href="#Step-3-Create-a-blog" class="headerlink" title="Step 3: Create a blog"></a>Step 3: Create a blog</h2><p>Use</p><pre><code>mkdir blog</code></pre><p>to create a directory for you to write your blog.</p><p>In that directory, use</p><pre><code>sudo hexo init</code></pre><p>to create the blog.</p><p>Congratulations! Your blog has been created. Then you can use</p><pre><code>hexo s</code></pre><p>to start running the hexo server. By default, you should be able to access your blog locally with “<a href="http://localhost:4000/&quot;" target="_blank" rel="noopener">http://localhost:4000/&quot;</a></p><p>Basic hexo instructions can be found in your blog when you first create it. Here are some.</p><pre><code>hexo n &quot;PUT THE NAME OF YOUR POST HERE&quot;  # to creat a new posthexo clean  # to clean the bloghexo g  # to generate the blog with your posthexo s  # to run your blog locallyhexo d  # to deploy your blog</code></pre><p>The blog posts will be in MarkDown format.</p><hr><h2 id="Step-4-Deploy-your-Blog"><a href="#Step-4-Deploy-your-Blog" class="headerlink" title="Step 4: Deploy your Blog"></a>Step 4: Deploy your Blog</h2><p>To run your blog remotely, you have to deploy your blog. Here, we will deploy it on your github.</p><p>Create a new repository on Github. The Repository name has to be “YourID.github.io”!!</p><p>Use</p><pre><code>npm install --save hexo-deployer-git</code></pre><p>to download a plugin needed.</p><p>After download, modify “_config.yml” file, “Deployment” section to</p><pre><code>deploy:  type: git  repo: https://github.com/YourID/YourID.github.io.git  branch: master  # set the default branch.</code></pre><p>The ‘branch’ is not necessary unless you want to use an alternative branch of your git project.</p><p>Use the instuction</p><pre><code>hexo d</code></pre><p>to deploy the blog.</p><p>It may ask you to fill in your github ID and password. Then use “<a href="https://YourID.github.io&quot;" target="_blank" rel="noopener">https://YourID.github.io&quot;</a>, you should be able to access your blog remotely.</p><hr><h2 id="Step-5-Change-the-Theme"><a href="#Step-5-Change-the-Theme" class="headerlink" title="Step 5: Change the Theme"></a>Step 5: Change the Theme</h2><p>First, you can find a hexo theme that you like, just google it.</p><p>Here I will use mine as an example. I used hexo-theme-fluid. More details are in the github “<a href="https://github.com/fluid-dev/hexo-theme-fluid&quot;" target="_blank" rel="noopener">https://github.com/fluid-dev/hexo-theme-fluid&quot;</a>.</p><p>To use the theme, you should first download it or simply use</p><pre><code>&quot;&quot;&quot;Change the link to your selected one, and the &#39;fluid&#39; to the name you like.Better to use the theme name&quot;&quot;&quot;git clone https://github.com/fluid-dev/hexo-theme-fluid themes/fluid</code></pre><p>modify “_config.yml”, “Extensions” section. Change the theme name to your file name. In my case, it was</p><pre><code>theme: fluid</code></pre><p>Clean, generate and deploy it again. Then, the blog becomes the way you see now.</p><p>That is all. I am glad if you also manage to create one. Later, I will put more stuff on this website. May be my studies, my interests or even diaries. Hopefully, they will be helpful for you.</p><p><em>Thank you! Wish you a great day!</em></p>]]></content>
    
    
    
    <tags>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
