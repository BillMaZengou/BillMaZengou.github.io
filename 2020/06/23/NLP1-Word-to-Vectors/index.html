<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="UCL MRes Virtual Reality; University of Warwick BSc Physics">
  <meta name="author" content="Zengou Ma">
  <meta name="keywords" content="">
  <title>Natural Language Processing1: Word To Vectors - Bill Ma&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css" />

<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_9n5xqdrq0nc.css">



  <link  rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css" />




<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Bill Ma's Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">
              <i class="iconfont icon-home-fill"></i>
              首页</a>
          </li>
        
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">
              <i class="iconfont icon-archive-fill"></i>
              归档</a>
          </li>
        
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">
              <i class="iconfont icon-category-fill"></i>
              分类</a>
          </li>
        
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">
              <i class="iconfont icon-tags-fill"></i>
              标签</a>
          </li>
        
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">
              <i class="iconfont icon-user-fill"></i>
              关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/bg/banner.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
                <div class="mt-3 post-meta">
                  <i class="iconfont icon-date-fill" aria-hidden="true"></i>
                  <time datetime="2020-06-23 23:03">
                    星期二, 六月 23日 2020, 11:03 晚上
                  </time>
                </div>
              

              <div class="mt-1">
                
                  
                  <span class="post-meta mr-2">
                    <i class="iconfont icon-chart"></i>
                    1.2k 字
                  </span>
                

                
                  
                  <span class="post-meta mr-2">
                      <i class="iconfont icon-clock-fill"></i>
                    
                    
                    24
                     分钟
                  </span>
                

                
              </div>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <p>In this series of posts, we will tackle on another direction of AI - Natural Language Processing. Human language is a type of natural language which differs from normal programming language.</p>
<p>This post is split into four sections:</p>
<ol>
<li>The Basic Principles of word2vec</li>
<li>Practice in Python</li>
<li>Compute word2vec From Scratch</li>
<li>Appendix - Mathematical Derivation</li>
</ol>
<p>Source code: <a href="https://github.com/BillMaZengou/nlp_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/nlp_basis</a> -&gt; WordVector.ipynb</p>
<hr>
<h1 id="The-Basic-Principles-of-word2vec"><a href="#The-Basic-Principles-of-word2vec" class="headerlink" title="The Basic Principles of word2vec"></a>The Basic Principles of word2vec</h1><p>To perform NLP, we need to first find a way to vectorise words so that we can put them into our applications.</p>
<h3 id="One-hot-Vector"><a href="#One-hot-Vector" class="headerlink" title="One-hot Vector"></a>One-hot Vector</h3><p>The simplest method is the <strong>one-hot vector</strong>. Each word can be considered as a unit vector. For example,<br>\[<br>  w^{a} =<br>  \begin{bmatrix}<br>  1\\<br>  0\\<br>  0\\<br>  .\\<br>  .\\<br>  .\\<br>  0<br>  \end{bmatrix},<br>  w^{at} =<br>  \begin{bmatrix}<br>  0\\<br>  1\\<br>  0\\<br>  .\\<br>  .\\<br>  .\\<br>  0<br>  \end{bmatrix},<br>  …<br>  w^{zebra} =<br>  \begin{bmatrix}<br>  0\\<br>  0\\<br>  0\\<br>  .\\<br>  .\\<br>  .\\<br>  1<br>  \end{bmatrix}<br>\]<br>However, in NLP, we would normally like to analysis words in the context. Therefore, we need the correlations between each word. For instance, when we search “house”, we expect to find anything related to houses like “home” or “apartment”. On the contrast, we also expect to find a low correlation between words like “house” and “cat”. Those are impossible to do with one-hot vectors because all the unit vectors are orthogonal. I.e.)<br>\[<br>  (w^{house})^T w^{home} = (w^{house})^T w^{cat} = \mathbf{0}<br>\]  </p>
<p>Previously, researchers tried to use WordNet’s list of synonyms to get similarity of the word vectors.</p>
<p><em>Note</em> WordNet is like a large dictionary, which contains information like synonyms, hypernyms, etc.</p>
<p>The problem is that in WordNet, the synonyms are put together without a clear relationship between each word. Take “good” as an example, “proficient” is considered as a synonym of “good”. They represent the same meaning in some certain circumstances. However, they are not interchangeable in general. Therefore, it is well-known to fail badly to use WordNet to get the similarities of words.</p>
<h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>A better approach is to encode similarity into the word vectors. The key idea is to use so-called <strong>distributional semantics</strong>. A word’s meaning is given by the words that frequently appear close-by.</p>
<p>Word vectors, which are sometimes referred as word embeddings or word representations, are a distributed representation.</p>
<p>To obtain the distributed representation of word vectors, <strong>Word2Vec</strong> is a frequently used technique.</p>
<p><strong>The Algorithm</strong></p>
<ol>
<li>Get a large corpus of text.</li>
<li>Every word in a fixed vocabulary is represented by a vector.</li>
<li>Go through each position \(t\) in the text, which has a centre word \(c\) and context (“outside”) words \(o\).</li>
<li>Use the similarity of the word vectors for \(c\) and \(o\) to calculate the probability of \(o\) given \(c\) (i.e. \(P(o|c)\)) or vice versa.</li>
<li>Adjust the word vectors to maximise the probability.<br><img src="word2vec.png" srcset="/img/loading.gif" alt="word"></li>
</ol>
<p><em>Note</em> <strong>Corpus</strong> is used in NLP to represent a bulk</p>
<p>The mathematical derivation can be found in Appendix.</p>
<p>Recall from \(\theta\) that, for each word, we have two vectors. We can average both at the end to obtain a better result of the optimisation.</p>
<p>In this post, we introduced a basic model of Word2Vec. It is called <strong>Skip-grams</strong> or SG for short. There is an alternative choice of the model called <strong>Continuous Bag of Words (CBOW)</strong></p>
<p>For the optimisation, we can use <strong>Stochastic Gradient Descent</strong> which is faster than the normal gradient descent. We may discuss in detail in later posts.</p>
<hr>
<h1 id="Practice-in-Python"><a href="#Practice-in-Python" class="headerlink" title="Practice in Python"></a>Practice in Python</h1><pre><code>import numpy as np
import matplotlib.pyplot as plt
plt.style.use(&quot;ggplot&quot;)

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec

glove_file = datapath(&#39;/Users/billma/Desktop/Interests/nlp_basis/data/glove.6B.100d.txt&#39;)
word2vec_glove_file = get_tmpfile(&quot;glove.6B.100d.txt&quot;)
dimension = glove2word2vec(glove_file, word2vec_glove_file)
print(dimension)

model = KeyedVectors.load_word2vec_format(word2vec_glove_file)
leader = model.most_similar(&#39;obama&#39;)
print(&#39;obama is related to: &#39;)
for i in leader:
    print(i)

anti_fruit = model.most_similar(negative=&#39;banana&#39;)
for i in anti_fruit:
    print(i)

result = model.most_similar(positive=[&#39;woman&#39;,&#39;king&#39;], negative=[&#39;man&#39;])
print(&quot;{}: {:.4f}&quot;.format(*result[0]))</code></pre><p>Example results are shown below:<br><strong>Similar Words To Obama</strong><br><img src="obama.png" srcset="/img/loading.gif" alt="obama"></p>
<p><strong>Dissimilar Words To Banana</strong><br><img src="banana.png" srcset="/img/loading.gif" alt="banana"></p>
<p><strong>Word Composition</strong><br>\[<br>  w_{King} - w_{Man} + w_{Woman} = w_{Queen}<br>\]<br><img src="queen.png" srcset="/img/loading.gif" alt="queen"></p>
<hr>
<h1 id="Compute-word2vec-From-Scratch-TODO"><a href="#Compute-word2vec-From-Scratch-TODO" class="headerlink" title="Compute word2vec From Scratch (TODO)"></a>Compute word2vec From Scratch (TODO)</h1><hr>
<h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><p>For each position \(t = 1,…,T\), predict context words within a window of fixed size \(m\), given centre word \(w_j\), we have a likelihood function such that<br>\[<br>  L(\theta) = \prod_{t=1}^T \prod_{-m \leq j \leq m, j \neq 0} P(w_{t+j}|w_t;\theta)<br>\]</p>
<p>The objective function, or so-called loss or cost function, \(J(\theta)\) is the normalised negative log likelihood,<br>\[<br>  J(\theta) = -\frac{1}{T}log(L(\theta)) = -\frac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m, j \neq 0} log(P(w_{t+j}|w_t;\theta) )<br>\]</p>
<p>Thus, our goal is to minimise the objective function so that the predictive accuracy can be maximised.</p>
<p>The probability of the words are calculated using <strong>softmax</strong> function, which is commonly used in classification. Therefore,<br>\[<br>  P(o|c) = \frac{exp(u_{o}^{T} v_c)}{\sum_{w}exp(u_{w}^{T} v_c)}<br>\]<br>where \(v_c\) and \(u_w\) are the word vectors. \(w\) is the context words.</p>
<p>In the objective function, \(\theta\) can represent all model parameters in one long vector. In our case with \(d\)-dimensional vectors and \(V\)-many words, we have a vector with \(2dV\) elements.<br>\[<br>  \theta =<br>  \begin{bmatrix}<br>  v_{a}\\<br>  .\\<br>  .\\<br>  .\\<br>  v_{zebra}\\<br>  u_{a}\\<br>  .\\<br>  .\\<br>  .\\<br>  u_{zebra}\\<br>  \end{bmatrix}<br>\]<br><em>Note</em> every word has two vectors.</p>
<p>To optimise the objective function, we need to take derivative for both \(v_c\) and \(u_w\).</p>
<h3 id="Optimise-with-respect-to-v-c"><a href="#Optimise-with-respect-to-v-c" class="headerlink" title="Optimise with respect to \(v_c\)"></a>Optimise with respect to \(v_c\)</h3><p>\[<br>  \frac{\partial}{\partial v_c} log(\frac{exp(u_{o}^{T} v_c)}{\sum_{w}exp(u_{w}^{T} v_c)})<br>\]<br>\[<br> =&gt;  \frac{\partial}{\partial v_c} u_{o}^{T} v_c - \frac{\partial}{\partial v_c} log(\sum_{w}exp(u_{w}^{T} v_c))<br>\]<br>\[<br> =&gt; u_{o}^{T} - \sum_x \frac{exp(u_{x}^T v_c)}{\sum_{w}exp(u_{w}^{T} v_c)} u_x<br>\]<br>\[<br> =&gt; u_{o}^{T} - \sum_x P(x|c) u_x<br>\]</p>
<h3 id="Optimise-with-respect-to-u-w"><a href="#Optimise-with-respect-to-u-w" class="headerlink" title="Optimise with respect to \(u_w\)"></a>Optimise with respect to \(u_w\)</h3><p>\[<br>  \frac{\partial}{\partial u_o} log(\frac{exp(u_{o}^{T} v_c)}{\sum_{w}exp(u_{w}^{T} v_c)})<br>\]<br>\[<br> =&gt;  \frac{\partial}{\partial u_o} u_{o}^{T} v_c - \frac{\partial}{\partial u_o} log(\sum_{w}exp(u_{w}^{T} v_c))<br>\]<br>\[<br> =&gt; v_c - \frac{exp(u_{o}^T v_c)}{\sum_{w}exp(u_{w}^{T} v_c)} v_c<br>\]<br>\[<br> =&gt; v_c - P(o|c) v_c<br>\]</p>
<hr>
<h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The creation of this post is inspired by <strong>Datawhale</strong>. The learning path is based on <strong>Stanford University CS224n: Natural Language Processing with Deep Learning</strong>.</p>
<hr>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li>Zh.gluon.ai. 2020. 10.1. 词嵌入（Word2vec） — 《动手学深度学习》 文档. [online] Available at: <a href="http://zh.gluon.ai/chapter_natural-language-processing/word2vec.html" target="_blank" rel="noopener">http://zh.gluon.ai/chapter_natural-language-processing/word2vec.html</a> [Accessed 25 June 2020].</li>
<li>Bilibili.com. 2020. [online] Available at: <a href="https://www.bilibili.com/video/BV1s4411N7fC?t=4725" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1s4411N7fC?t=4725</a> [Accessed 25 June 2020].</li>
<li>En.wikipedia.org. 2020. Natural Language Processing. [online] Available at: <a href="https://en.wikipedia.org/wiki/Natural_language_processing" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Natural_language_processing</a> [Accessed 25 June 2020].</li>
<li>Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S. and Dean, J., 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).</li>
</ol>
<hr>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Computer/">Computer</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2020/06/25/Computer-Vision-Foundation2-LBP-Face-Recognition/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Computer Vision Foundation 2: LBP Face Recognition</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2020/06/22/Computer-Vision-Foundation1-Harris-Corner-Detector/">
                        <span class="hidden-mobile">Computer Vision Foundation 1: Harris Corner Detector</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/main.js" ></script>


  <script  src="/js/lazyload.js" ></script>



  
  <script  src="https://cdn.staticfile.org/tocbot/4.10.0/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '.post-content',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>





  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>






<!-- Plugins -->



  <script  src="https://cdn.staticfile.org/prettify/188.0.0/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').each(function () {
        const pre = $(this);
        if (pre.find('code.mermaid').length > 0) {
          return;
        }
        pre.addClass('prettyprint  linenums');
      });
      prettyPrint();
    })
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "Natural Language Processing1: Word To Vectors&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
              processEscapes: true,
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
          }
      });
      MathJax.Hub.Register.StartupHook("End Jax",function () {
        var BROWSER = MathJax.Hub.Browser;
        var jax = "HTML-CSS";
        if (BROWSER.isMSIE && BROWSER.hasMathPlayer) jax = "NativeMML";
        return MathJax.Hub.setRenderer(jax);
      });
      MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });

    </script>

    <script  src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" ></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

  














<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
