<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="UCL MRes Virtual Reality; University of Warwick BSc Physics">
  <meta name="author" content="Zengou Ma">
  <meta name="keywords" content="">
  <title>Natural Language Processing2: Word Vectors -&gt; Algorithm and Analysis - Bill Ma&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css" />

<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_9n5xqdrq0nc.css">



  <link  rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css" />




<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Bill Ma's Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">
              <i class="iconfont icon-home-fill"></i>
              首页</a>
          </li>
        
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">
              <i class="iconfont icon-archive-fill"></i>
              归档</a>
          </li>
        
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">
              <i class="iconfont icon-category-fill"></i>
              分类</a>
          </li>
        
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">
              <i class="iconfont icon-tags-fill"></i>
              标签</a>
          </li>
        
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">
              <i class="iconfont icon-user-fill"></i>
              关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
                <div class="mt-3 post-meta">
                  <i class="iconfont icon-date-fill" aria-hidden="true"></i>
                  <time datetime="2020-06-27 16:50">
                    星期六, 六月 27日 2020, 4:50 下午
                  </time>
                </div>
              

              <div class="mt-1">
                
                  
                  <span class="post-meta mr-2">
                    <i class="iconfont icon-chart"></i>
                    1.1k 字
                  </span>
                

                
                  
                  <span class="post-meta mr-2">
                      <i class="iconfont icon-clock-fill"></i>
                    
                    
                    21
                     分钟
                  </span>
                

                
              </div>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <p>In <em>NLP1</em>, we discussed how to convert a word into a vector with distributional semantics. Also, we demonstrated using pre-trained results to find the synonyms. However, they are more details to consider. Before diving into the neural networks and build a custom system, we present some issues and Stanford’s solution.</p>
<p>Source code: <a href="https://github.com/BillMaZengou/nlp_basis" target="_blank" rel="noopener">https://github.com/BillMaZengou/nlp_basis</a> -&gt; WordVector.ipynb</p>
<hr />
<h1 id="issues-of-word-vectors"><a class="markdownIt-Anchor" href="#issues-of-word-vectors"></a> Issues of word vectors</h1>
<h2 id="visualisation"><a class="markdownIt-Anchor" href="#visualisation"></a> Visualisation</h2>
<p>Usually, the results of NLP lie on a high dimensional space. (The pre-train result that we used is <code>glove.6B.100d.txt</code>, which is from <a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">https://nlp.stanford.edu/projects/glove/</a>) Therefore, accurately visualising the result is impossible. A plausible way is to project the word vectors onto a 2D surface.</p>
<pre class="highlight"><code class="">def display_pca_scatterplot(model, words=None, sample=0):
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.decomposition import PCA

    if words == None:
        if sample &gt; 0:
            words = np.random.choice(list(model.vocab.keys()), sample)
        else:
            words = [ word for word in model.vocab ]

    word_vectors = np.array([model[w] for w in words])

    twodim = PCA().fit_transform(word_vectors)[:, :2]

    plt.figure(figsize=(6,6))
    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')
    for word, (x,y) in zip(words, twodim):
        plt.text(x+0.05, y+0.05, word)  # put labels
</code></pre>
<p><strong>Projection of The Results</strong><br />
<img src="result.png" srcset="/img/loading.gif" alt="project" /></p>
<p>As you can see, the food and drinks are on the left top corner; countries are on the right top corner; animals are on the left bottom corner; anything related to studies are on the right bottom corner. However, since we forced them to show up on the 2D plane, many distant features may look closer than they suppose to be.</p>
<p>If you inspect the left top corner carefully, tea seems closer to food rather than drinks. If we compute the similarities between the tea and the coffee as well as the tea and pizza, we can see clearly that the <em>tea</em> is a drink.</p>
<p><strong>Real Similarities</strong><br />
<img src="similarity.png" srcset="/img/loading.gif" alt="similarity" /></p>
<h2 id="optimisation-gradient-descent"><a class="markdownIt-Anchor" href="#optimisation-gradient-descent"></a> Optimisation: Gradient Descent</h2>
<p>In the last post, we proposed the cost function, \(J(\theta)\) for Word2vec. Gradient descent is an algorithm to minimise the cost function.</p>
<p>The key idea is to use the negative gradient of \(J(\theta)\) as a direction to take a small step. Iterate the process until we reach the minimum of the cost function. Mathematically,<br />
\[<br />
\theta^{new} = \theta^{old} - \alpha \nabla_{\theta} J(\theta)<br />
\]<br />
where \(\alpha\) is the step size or learning rate. The choice of \(\alpha\) is a trade-off. If \(\alpha\) is small, it may take too many iteration to converge. However, if it is too large, \(\theta\) cannot even converge because of overshooting.</p>
<p>Nevertheless, \(\nabla_{\theta} J(\theta)\) is computationally expensive as the training text sample can contain millions of words.</p>
<p>As the last post mentioned, people normally use <strong>Stochastic Gradient Descent</strong>. Instead of consider all the words in the text, we consider only a portion of the text around the centre word. This portion is called a window. It may work badly for one window of the words. However, as we iterate through all words, it does converge.</p>
<p><em>Note</em> The window is normally one of the power numbers of \(2\), like \(32\) or \(64\). They can match the structure of GPU relatively easier.</p>
<p>With the stochastic gradient, as we only compute the gradient of some certain word vectors, \(\nabla_{\theta} J(\theta)\) is large sparse matrix. Therefore, if we use sparse matrix structure instead of dense matrix, we are able to save space of memories.</p>
<h2 id="probability"><a class="markdownIt-Anchor" href="#probability"></a> Probability</h2>
<p>in the last post, we mentioned to use softmax function to turn the dot product (cosine similarity) of two vectors into a probability. However, to calculate the probability with softmax function, the computational expense is high. Hence, in standard word2vec, the implementation of the skip-gram model uses <strong>negative sampling</strong>.</p>
<p>For negative sampling, we need to train binary logistic regressions for a <em>true pair</em> (centre word and word in its context window) versus several <em>noise pairs</em> (the centre word paired with a random word)</p>
<p>The new cost function is<br />
\[<br />
J(\theta) = \frac{1}{T} \sum_{t=1}^{T} J_t(\theta)<br />
\]<br />
\[<br />
J_t(\theta) = -log(\sigma(\mathbf{u_o^T}\mathbf{v_c})) - \sum_{k=1}<sup>{K}log(\sigma(\mathbf{u_k</sup>T}\mathbf{v_c}))<br />
\]<br />
where we take (K\) negative samples and<br />
\[<br />
\sigma(x) = \frac{1}{1+e^{-x}}<br />
\]</p>
<p>For this cost function, we need to maximise probability that real outside word appears but minimise probability that random words appear around centre word.<br />
\[<br />
P(w) = \frac{U(w)^{\frac{3}{4}}}{Z}<br />
\]<br />
where \(Z\) is for the normalisation and \(U(w)\) is a unigram distribution so that \(P(w_1, w_2) = P(w_1|w_2)P(w_2) \) turns to \(P_{uni}(w_1, w_2) = P(w_1)P(w_2)\). The \(\frac{3}{4}\) is a hyperparameter.</p>
<h2 id="co-occurrence-vector-todo"><a class="markdownIt-Anchor" href="#co-occurrence-vector-todo"></a> Co-occurrence Vector (TODO)</h2>
<p>An alternative approach. It produce a large matrix, but can be reduce the dimension using SVD. More about SVD, we will discuss in later post.</p>
<p>It may compose a vector space such that semantic similarity can be found be inspecting the linear relation between words. E.g.) The vector difference between <em>Swim</em> and <em>Swimmer</em> can be close to the difference between <em>Teach</em> and <em>Teacher</em></p>
<h2 id="hybrid-of-two-approach-todo"><a class="markdownIt-Anchor" href="#hybrid-of-two-approach-todo"></a> Hybrid of Two Approach (TODO)</h2>
<p>Instead of using the co-occurrence probability, use the ratio of the co-occurrence probability can improve the result.</p>
<h2 id="question-about-the-symmetry"><a class="markdownIt-Anchor" href="#question-about-the-symmetry"></a> Question About the Symmetry</h2>
<p>Recall from the last post, we can do word composition with word vectors. For example,<br />
\[<br />
w_{King} - w_{Man} + w_{Woman} = w_{Queen}<br />
\]</p>
<p>However, it seems the rule is not simply a vector addition.<br />
<img src="asymmetry.png" srcset="/img/loading.gif" alt="asymmetry" /></p>
<p>It is clear that<br />
\[<br />
w_{Unscrupulous} - w_{Man} + w_{Woman} = w_{Dishonest}<br />
\]<br />
and<br />
\[<br />
w_{Dishonest} - w_{Woman} + w_{Man} = w_{Inept}<br />
\]<br />
rather than returning to \(w_{Unscrupulous}\).</p>
<p>Currently, we have two hypotheses. One is that occurs because of the bias of the training sample and the default word2vec algorithm does not have debias approach. The other is that the way to calculate the similarity does not simply use the vector addition or something similar due to the multi-dimension.</p>
<p><em>Note</em> the hypotheses need verification.</p>
<hr />
<h1 id="acknowledgement"><a class="markdownIt-Anchor" href="#acknowledgement"></a> Acknowledgement</h1>
<p>The creation of this post is inspired by <strong>Datawhale</strong>. The learning path is based on <strong>Stanford University CS224n: Natural Language Processing with Deep Learning</strong>.</p>
<hr />
<h1 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h1>
<ol>
<li><a href="http://Bilibili.com" target="_blank" rel="noopener">Bilibili.com</a>. 2020. [online] Available at: <a href="https://www.bilibili.com/video/BV1s4411N7fC?t=4725" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1s4411N7fC?t=4725</a> [Accessed 25 June 2020].</li>
<li><a href="http://En.wikipedia.org" target="_blank" rel="noopener">En.wikipedia.org</a>. 2020. Language Model. [online] Available at: <a href="https://en.wikipedia.org/wiki/Language_model" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Language_model</a> [Accessed 27 June 2020].</li>
<li></li>
<li>Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S. and Dean, J., 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119).</li>
</ol>
<hr />

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Computer/">Computer</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2020/06/25/Computer-Vision-Foundation2-LBP-Face-Recognition/">
                        <span class="hidden-mobile">Computer Vision Foundation 2: LBP Face Recognition</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/main.js" ></script>


  <script  src="/js/lazyload.js" ></script>



  
  <script  src="https://cdn.staticfile.org/tocbot/4.10.0/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '.post-content',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>





  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>






<!-- Plugins -->



  <script  src="https://cdn.staticfile.org/prettify/188.0.0/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').each(function () {
        const pre = $(this);
        if (pre.find('code.mermaid').length > 0) {
          return;
        }
        pre.addClass('prettyprint  linenums');
      });
      prettyPrint();
    })
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "Natural Language Processing2: Word Vectors -> Algorithm and Analysis&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
              processEscapes: true,
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
          }
      });
      MathJax.Hub.Register.StartupHook("End Jax",function () {
        var BROWSER = MathJax.Hub.Browser;
        var jax = "HTML-CSS";
        if (BROWSER.isMSIE && BROWSER.hasMathPlayer) jax = "NativeMML";
        return MathJax.Hub.setRenderer(jax);
      });
      MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });

    </script>

    <script  src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" ></script>

  














<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body>
</html>
